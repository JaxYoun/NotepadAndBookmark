【5678012345306762216811237515583433502006】  //thinkPad机器码
正确姿势是用管理员权限打开CMD.EXE
接着输入以下命令：
slmgr /ipk NPPR9-FWDCX-D2C8J-H872K-2YT43
弹出窗口提示：“成功的安装了产品密钥”。


继续输入以下命令：g'i't
slmgr /skms zh.us.to
弹出窗口提示：“密钥管理服务计算机名成功的设置为zh.us.to”。


输入以下命令：
slmgr /ato
按回车键后将弹出窗口提示：“成功的激活了产品”。
至此，Win10正式企业版系统激活成功.
成功激活请回复楼主！

NPPR9-FWDCX-D2C8J-H872K-2YT43 

1.new一个maven Project，不勾选【create a simple project(skip archetype selection)】，【可以使用默认工作空间，也可以浏览选择目录】。
2.目录选择，选择【Maven-archetype-webapp】。
3.group id和artifact id都可以灵活填写，包名会根据group Id 和 artifact Id自动生成，也可以改写或删除暂不填。
4.如果此时点击next报无法解析archetype错误的话说明maven插件的目录没有加载好，需要为maven添加插件（http://repo1.maven.org/maven2/archetype-catalog.xml）。
5.习惯性的，将新建项目的字符集修改为UTF-8,在项目建立之初就射好字符集，能避免很多麻烦.
6.由于未知原因，项目不能显示某些目录，打开项目属性，选择Maven下的Project Facets，【java选择jdk对应版本，动态web模型选择较高版本】(修改动态web模型不成功的话，需要)
7.再在Java Biuld Path选择libraries选项卡，选中默认的jre system libraries[jre7]版本，然后编辑选择适应工作空间的jre版本。

JSON封装与拆封：
String jsonStr = "{'name': 'helloworlda','array':[{'a':'111','b':'222','c':'333'},{'a':'999'}],'address':'111','people':{'name':'happ','sex':'girl'}}";
JSONObject jsonobj=JSONObject.fromObject(jsonStr);//将字符串转化成json对象

String name=jsonobj.getString("name");//获取字符串。
JSONArray array=jsonobj.getJSONArray("array");//获取数组。
JSONObject obj=jsonobj.getJSONObject("people");//获取对象。

1.声明一个数组 

Java代码 
String[] aArray = new String[5];  
String[] bArray = {"a","b","c", "d", "e"};  
String[] cArray = new String[]{"a","b","c","d","e"};  


2.输出一个数组 

Java代码 
int[] intArray = { 1, 2, 3, 4, 5 };  
String intArrayString = Arrays.toString(intArray);  
   
// print directly will print reference value  
System.out.println(intArray);  
// [I@7150bd4d  
   
System.out.println(intArrayString);  
// [1, 2, 3, 4, 5]  


3.从一个数组创建数组列表 


Java代码 
String[] stringArray = { "a", "b", "c", "d", "e" };  
ArrayList<String> arrayList = new ArrayList<String>(Arrays.asList(stringArray));  
System.out.println(arrayList);  
// [a, b, c, d, e]  


4.检查一个数组是否包含某个值 


Java代码 
String[] stringArray = { "a", "b", "c", "d", "e" };  
boolean b = Arrays.asList(stringArray).contains("a");  
System.out.println(b);  
// true  


5.连接两个数组 

Java代码 
int[] intArray = { 1, 2, 3, 4, 5 };  
int[] intArray2 = { 6, 7, 8, 9, 10 };  
// Apache Commons Lang library  
int[] combinedIntArray = ArrayUtils.addAll(intArray, intArray2);  


6.声明一个内联数组（Array inline） 


Java代码 
method(new String[]{"a", "b", "c", "d", "e"});  


7.把提供的数组元素放入一个字符串 


Java代码 
// containing the provided list of elements  
// Apache common lang  
String j = StringUtils.join(new String[] { "a", "b", "c" }, ", ");  
System.out.println(j);  
// a, b, c  


8.将一个数组列表转换为数组 

Java代码 
String[] stringArray = { "a", "b", "c", "d", "e" };  
ArrayList<String> arrayList = new ArrayList<String>(Arrays.asList(stringArray));  
String[] stringArr = new String[arrayList.size()];  
arrayList.toArray(stringArr);  
for (String s : stringArr)  
    System.out.println(s);  


9.将一个数组转换为集（set） 


Java代码 
Set<String> set = new HashSet<String>(Arrays.asList(stringArray));  
System.out.println(set);  
//[d, e, b, c, a]  


10.逆向一个数组 

Java代码 
int[] intArray = { 1, 2, 3, 4, 5 };  
ArrayUtils.reverse(intArray);  
System.out.println(Arrays.toString(intArray));  
//[5, 4, 3, 2, 1]  


11.移除数组中的元素 

Java代码 
int[] intArray = { 1, 2, 3, 4, 5 };  
int[] removed = ArrayUtils.removeElement(intArray, 3);//create a new array  
System.out.println(Arrays.toString(removed));  


12.将整数转换为字节数组 

Java代码 
byte[] bytes = ByteBuffer.allocate(4).putInt(8).array();  
   
for (byte t : bytes) {  
   System.out.format("0x%x ", t);  
}  
工厂方法模式（三种）
补充：{
	1.简单工厂：仅有一个产品接口但有多个产品实现类；无工厂接口仅有一个工厂实现类和一个生产方法，生产方法用中有多个case语句，根据传入参数生产不同产品。（不符合开闭原则）
	2.工厂方法：仅有一个产品接口但有N个产品实现类；仅有一个工厂接口约定生产方法但有N个实现类，每个实现类实现一个生产方法，每个方法只生产一种产品实现类。（扩展后工厂类太多）
	3.抽象工厂：仅有一个产品接口但有N个产品实现类，且各种产品实现的部件间存在差异；仅有一个工厂接口但有N个工厂实现类，
				工厂接口既要约定产品的生产方法，还要约定部件的生产方法，每个工厂实现既要实现产品的生产方法，还要实现部件的生产方法。（满足多部件复杂产品）
	}
一、工厂方法模式（三种实现方式，但存在工厂依赖，限制扩展）：
	一个工厂类，一个产品接口，多个产品类
	1.一个工厂方法，传入不同参数生产出不同产品（即简单工厂，归为工厂方法的一个特例）
	2.每种产品对应一个工厂方法，想得到哪种产品，直接调用相应的方法。
	3.每种产品对应一个静态的工厂方法，用法同上，但是不用实例化工厂对象，直接使用工厂类。
二、抽象工厂模式：
	一个工厂接口，对应产品实现类有多个工厂实现类，一个产品接口，多个产品实现类，
	当需要扩展产品种类时，只需新建一个产品实现类和一个工厂实现类，分别实现上述两个接口，而不用修改现存的所有类。
三、单例模式（两种实现方式）：
	可以降低频繁初始化尤其是大型类初始化造成的系统开销，同时某些业务不许同时存在多个同类实例对象。
	基础是在类中申明一个该类的私有、静态对象作为成员变量，私有化这个类的构造方法，对外提供一个返回上述成员变量的方法。
	1.懒汉模式
四、建造者模式：
	工厂模式针对的是单个产品类的实例化问题，建造者模式则是将各种产品集中管理，建造者提供返回复合型对象的方法，
	复合对象可以是简单产品的各种组合形成的集合对象（比如返回一个同时具有三个产品实例的数组对象），
		建造者模式是在抽象工厂模式的基础上发展而来的。
五、原型模式：
	原型模式是指将一个实例通过复制或克隆的方式得到与原对象一样的新实例，需要先创建一个原型实例，然后clone（）。
六、适配器模式：
	类的适配器模式：目标接口I规定方法i和方法b，源类A实现了方法b；现在要求一个类既提供i方法的实现，又提供a方法，同时具备两种功能。
		解决方法是新建一个适配器类，即继承A类，又实现I接口。
	
'<span title="'+value+'" style="">'+value+'</span>'

基础语法：$(selector).action()
$  定义jQuery
选择器(selector)   查询和查找HTML元素
action()   执行对元素的操作
文档就绪函数：防止文档在完全加载（就绪）之前运行 jQuery 代码。
$(document).ready(function(){
	$("myId").click(function(){
		alert(888);
	});
});

$("#myId"):根据id选定元素
$(".myClass"):根据类名选定元素
$("input"):根据标签名选定元素
$("div,#myId,.myClass"):根据多种依据，混合选定多个元素
$("*"):选定所有DOM元素

基本过滤选择器（10种）
	$("div:first"):选定所有div元素中的第一个div
	$("div:last"):选定所有div元素中的最后一个div
	$("div:not(.myClass)"):选定所有div元素中类名不是myClass的div
	$("div:even"):选定所有div中排序索引是偶数的div
	$("div:odd"):选定所有div中排序索引是奇数的div
	$("div:eq(7)"):选定所有div中排序索引为7的div
	$("div:gt(4)"):选定所有div中排序索引大于4的div
	$("div:lt(4)"):选定所有div中排序索引小于4的div
	$("div:header"):选定所有header标签(包括<h1><h2><h3>...)
	$("div:animated"):选定正在执行动画效果的div

内容过滤选择器(4种):
	$("div:cntains('hello')"):选定包含文本元素‘hello’的div
	$("div:empty"):选定不包含子元素(包括文本元素)的空div
	$("div:has(.myClass)"):选定包含有类名位myClass元素为子元素的div
	$("div:parent"):选定作为父元素(包括文本元素)的div

	
实体类：
	图片：id,name,url,typeId,styleId;
	分类：id,name;
	风格：id,name;
	方案：id,personId,headerTitle,bodyerTitle,footerTitle,headerUrl,bodyerUrl,footerUrl;
	
SELECT t0.url AS headerUrl
FROM t_image t
WHERE t0.fk_type_id = 1
AND t0.fk_style_id = 1
AND t0.fk_position_id = 1;

SELECT t1.url AS headerUrl
FROM t_image t
WHERE t1.fk_type_id = 1
AND t1.fk_style_id = 1
AND t1.fk_position_id = 1;

SELECT t2.url AS headerUrl
FROM t_image t
WHERE t2.fk_type_id = 1
AND t2.fk_style_id = 1
AND t2.fk_position_id = 1;


SELECT
	iv0.headerUrl AS headerUrl,
	iv1.bodyerUrl AS bodyerUrl,
	iv2.footerUrl AS footerUrl
FROM
	(SELECT t0.url AS headerUrl FROM t_image t0 WHERE t0.fk_type_id = 1 AND t0.fk_style_id = 1 AND t0.fk_position_id = 1) iv0,
	(SELECT t1.url AS bodyerUrl FROM t_image t1 WHERE t1.fk_type_id = 1 AND t1.fk_style_id = 1 AND t1.fk_position_id = 2) iv1,
	(SELECT t2.url AS footerUrl FROM t_image t2 WHERE t2.fk_type_id = 1 AND t2.fk_style_id = 1 AND t2.fk_position_id = 3) iv2
同一浏览器多用户登录bug解决思路
问题描述：
		用账户a1在页面p1先登录，在p1的form中录入待提交数据；此时，在同一浏览器打开新页面p2，用账户a1登录随即在p2页注销，
		之后换账户a2在p2登录；此时提交p1页面form中的数据到后台，会出现后台数据的提交者是a2的现象。
解决办法：
		a1登录时将此时session中的a1账户信息存到系统中不会刷新的主页的隐藏标签中，
		同时在系统的所有ajax（包括ext和jquery）操作中给每个请求加包头，并附上隐藏标签中的值，
		后台配置拦截器，拦截所有请求取出添加的报头值与后台session中的账户信息做对比，
		信息一致则放行，不一致则阻断操作。

解决系统同一浏览器多人登录时，


要为指定action绑定拦截器

1、在struts2中有一系列拦系统截器组成的系统默认拦截器栈，
是框架的核心，如果某Action不配置自定义拦截器，默认拦截器栈即使不显式配置也会自动生效，
当Action配置了自定义拦截器后，默认拦截器将失效，
如果要继续使用默认拦截器就必须在自定义拦截器的最后面显式配置默认拦截器。
2、默认拦截器栈和可以与任意多自定义拦截器组成自定义拦截器栈。
3、配置全局拦截器需要用<default-interceptor-ref name="stack"></default-interceptor-ref>，
应用指向单个拦截器或栈。


Struts2拦截器：
1.创建拦截器类：一般通过继承AbstractInterceptor或其子类MethodFilterInterceptor，
	来间接实现interceptor接口的方式（这样init和destroy方法就可以空实现）。
		public class LoginInterceptor extends MethodFilterInterceptor {
			private static final long serialVersionUID = 3513708880894509250L;
			private static Logger log = Logger.getLogger(LoginInterceptor.class);//实例化日志对象
			@Override
			//invocation参数是【被拦截的请求或方法】
			public String doIntercept(ActionInvocation invocation) throws Exception {
				ActionContext context = invocation.getInvocationContext();
				String invocationName = context.getName();
				log.info("当前拦截的action是：" + invocationName);
				Set<String> exclude= this.excludeMethods;
				if (exclude.contains(invocationName)) {
					return invocation.invoke();
				}
				HttpServletRequest  requeset =  ServletActionContext.getRequest();
				HttpSession session = requeset.getSession();
				User user = (User) session.getAttribute(SystemConstant.CURRENT_USER);
				if (user != null) {
					//被拦截的请求或方法在本拦截其中被执行，
					//如果有后续拦截器会继续被拦截，如果没有就直接return到视图
					return invocation.invoke();
				} else {
					String headType = requeset.getHeader("X-Requested-With");
					if(headType !=null && headType.equalsIgnoreCase("XMLHttpRequest")){ 
						try {
							ServletActionContext.getResponse().setStatus(999);
						} catch (Exception e) {
							System.out.println(e.getMessage());
						}
					}
				}
				return Action.LOGIN;
			}

	}
2.配置拦截器：在struts.xml文件下
	<interceptors>
		<!-- 逐个声明拦截器 -->
		<interceptor name="exception" class="com.opensymphony.xwork2.interceptor.ExceptionMappingInterceptor"></interceptor>
		<interceptor name="loginInterceptor" class="com.sckj.gwxt.common.interceptor.LoginInterceptor">
			<param name="excludeMethods">idCardLogin,logout,login,toLogin,userLogin</param>//给本拦截器配置排除列表参数
		</interceptor>
		<!-- 将声明的拦截器放入栈，【单个拦截器和栈属同一级别，从节点写法也可看出】 -->
		<interceptor-stack name="stack">
			<interceptor-ref name="defaultStack"></interceptor-ref>//系统预定义的拦截器栈
			<interceptor-ref name="loginInterceptor"></interceptor-ref>
			<interceptor-ref name="exception"></interceptor-ref>
		</interceptor-stack>
	</interceptors>

	<!-- 设置默认全局拦截器【可以是单个拦截器，也可是是栈】，
	默认拦截器系统强制生效，但可以自定义以替代系统预定义的 -->
	<default-interceptor-ref name="stack"></default-interceptor-ref>


<!-- 配置全局的转发，当其他地方匹配不到时，会转到全局转发中寻找 -->
<global-results>
	<result name="exception" type="chain">toLogin</result>
	<result name ="sesssioIsOverdue">/WEB-INF/pages/login.jsp</result>
	<result name="reboot">/WEB-INF/pages/error/serverReboot.jsp</result>
</global-results>

<!-- 配置全局异常-->
<global-exception-mappings>
	//result="exception"属性表示:当发生异常时转发到exception视图，正好在全局转发中有定义
	<exception-mapping result="exception" exception="java.lang.RuntimeException" />
	<exception-mapping result="exception" exception="com.sckj.wtgl.exception.DaoException" />
	<exception-mapping result="exception" exception="com.sckj.wtgl.exception.ServiceException" />
</global-exception-mappings>

//四项俱全：action名为logout时，会转给指定的class中指定的method进行处理，然后转发到所指定的试图页面
<action name="logout" class="com.sckj.wtgl.action.admin.UserAction" method="logout">
	<result>/WEB-INF/pages/login.jsp</result>
</action>

//有class无方法：action名为toIndex时，调用指定class的execute方法处理，直接转发到指定试图页面
<action name="logout" class="com.sckj.wtgl.action.admin.UserAction">
	<result>/WEB-INF/pages/login.jsp</result>
</action>

//只具备必要的两项：action名为toIndex时，不调用任何处理方法，直接转发到指定试图页面
<action name="toIndex">
	<result>/WEB-INF/pages/index.jsp</result>
</action>


GROUP_CONCAT()函数可以拼接查询结果集的同一字段（其中：如果不写separator 关键字默认用逗号分隔，如果写上就会以其后的‘+’作为连接符）：
SELECT GROUP_CONCAT(ta.RESPONSE_PERSON_NAME separator '+') from t_accident_response_person ta WHERE ta.FK_ACCIDENT_ID = 1;
使用扩展：
SELECT
	t.PK_ID,
	vi.`NAMES`
FROM
	t_accident t
INNER JOIN (
	SELECT
		GROUP_CONCAT(ta.RESPONSE_PERSON_NAME) AS NAMES,
		ta.FK_ACCIDENT_ID AS accidentId
	FROM
		t_accident_response_person ta
	WHERE
		ta.FK_ACCIDENT_ID = 1
) vi ON t.PK_ID = vi.accidentId
WHERE
	t.PK_ID = 1;

MySQL 触发器简单实例
~~语法~~
CREATETRIGGER <触发器名称>  --触发器必须有名字，最多64个字符，可能后面会附有分隔符.它和MySQL中其他对象的命名方式基本相象.
{ BEFORE | AFTER }  --触发器有执行的时间设置：可以设置为事件发生前或后。
{ INSERT | UPDATE | DELETE }  --同样也能设定触发的事件：它们可以在执行insert、update或delete的过程中触发。
ON <表名称>  --触发器是属于某一个表的:当在这个表上执行插入、 更新或删除操作的时候就导致触发器的激活. 我们不能给同一张表的同一个事件安排两个触发器。
FOR EACH ROW  --触发器的执行间隔：FOR EACH ROW子句通知触发器 每隔一行执行一次动作，而不是对整个表执行一次。
<触发器SQL语句>  --触发器包含所要触发的SQL语句：这里的语句可以是任何合法的语句， 包括复合语句，但是这里的语句受的限制和函数的一样。
--你必须拥有相当大的权限才能创建触发器（CREATE TRIGGER），如果你已经是Root用户，那么就足够了。这跟SQL的标准有所不同。

example1:
创建表tab1

1
2
3
4
DROP TABLE IF EXISTS tab1;
CREATE TABLE tab1(
    tab1_id varchar(11)
);
创建表tab2

1
2
3
4
DROP TABLE IF EXISTS tab2;
CREATE TABLE tab2(
    tab2_id varchar(11)
);

创建触发器:t_afterinsert_on_tab1

作用：增加tab1表记录后自动将记录增加到tab2表中

1
2
3
4
5
6
7
DROP TRIGGER IF EXISTS t_afterinsert_on_tab1;
CREATE TRIGGER t_afterinsert_on_tab1 
AFTER INSERT ON tab1
FOR EACH ROW
BEGIN
     insert into tab2(tab2_id) values(new.tab1_id);
END;
测试一下

1
INSERT INTO tab1(tab1_id) values('0001');
看看结果

1
2
SELECT * FROM tab1;
SELECT * FROM tab2;
example2:

创建触发器:t_afterdelete_on_tab1

作用：删除tab1表记录后自动将tab2表中对应的记录删去

1
2
3
4
5
6
7
DROP TRIGGER IF EXISTS t_afterdelete_on_tab1;
CREATE TRIGGER t_afterdelete_on_tab1
AFTER DELETE ON tab1
FOR EACH ROW
BEGIN
      delete from tab2 where tab2_id=old.tab1_id;
END;
测试一下

1
DELETE FROM tab1 WHERE tab1_id='0001';
看看结果

1
2
SELECT * FROM tab1;
SELECT * FROM tab2;

1.MySQL中格式化日期或者时间，只需根据所需精度做适当的增减：SELECT DATE_FORMAT(d.submitdate,'%y/%m/%d:%h:%m:%s') as fmtdate from t_document d;
2.select * from t_user u where u.age > 30 order by name limit 4 offset 1;第一种：以第1条为起点，读取四条【同时注意】 limit语句应写在order by之后
select * from t_user u where u.age > 30 order by name limit 4 1;第二种：这时则恰恰相

另一个MySql一个语句从同一个表查询多个count的方法：SELECT
	*
FROM
	(
		SELECT
			COUNT(1) AS qq


一、工厂方法模式（三种实现方式，但存在工厂依赖，限制扩展）：
	一个工厂类，一个产品接口，多个产品类
	1.一个工厂方法，传入不同参数生产出不同产品
	2.每种产品对应一个工厂方法，想得到哪种产品，直接调用相应的方法。
	3.每种产品对应一个静态的工厂方法，用法同上，但是不用实例化工厂对象，直接使用工厂类。
二、抽象工厂模式：
	一个工厂接口，对应产品实现类有多个工厂实现类，一个产品接口，多个产品实现类，
	当需要扩展产品种类时，只需新建一个产品实现类和一个工厂实现类，分别实现上述两个接口，而不用修改现存的所有类。
三、单例模式（两种实现方式）：
	可以降低频繁初始化尤其是大型类初始化造成的系统开销，同时某些业务不许同时存在多个同类实例对象。
	基础是在类中申明一个该类的私有、静态对象作为成员变量，私有化这个类的构造方法，对外提供一个返回上述成员变量的方法。
	1.懒汉模式
四、建造者模式：
	工厂模式针对的是单个产品类的实例化问题，建造者模式则是将各种产品集中管理，建造者提供返回复合型对象的方法，
	复合对象可以是简单产品的各种组合形成的集合对象（比如返回一个同事具有三个产品实例的数组对象），建造者模式是在抽象工厂模式的基础上发展而来的。
五、原型模式：
	原型模式是指将一个实例通过复制或克隆的方式得到与原对象一样的新实例，需要先创建一个原型实例，然后clone（）。
	
	String jsonStr = "{'name':'kkk','age':12}";
	JsonObject jobj = getJSONObject

	
	java中JSON封装与拆封：
String jsonStr = "{'name': 'helloworlda','array':[{'a':'111','b':'222','c':'333'},{'a':'999'}],'address':'111','people':{'name':'happ','sex':'girl'}}";
JSONObject jsonobj=JSONObject.fromObject(jsonStr);//将字符串转化成json对象

String name=jsonobj.getString("name");//获取字符串。
JSONArray array=jsonobj.getJSONArray("array");//获取数组。
JSONObject obj=jsonobj.getJSONObject("people");//获取对象。

语法
stringObject.match(regexp)

返回值
返回匹配结果的数组。该数组的内容依赖于 regexp 是否具有全局标志 g。

如果 regexp 没有标志 g，那么 match() 方法就只能在 stringObject 中执行一次匹配。如果没有找到任何匹配的文本，
 match() 将返回 null。否则，它将返回一个数组，其中存放了与它找到的匹配文本有关的信息。该数组的第 0 个元素存放的是匹配文本，
 而其余的元素存放的是与正则表达式的子表达式匹配的文本。除了这些常规的数组元素之外，返回的数组还含有两个对象属性。
 index 属性声明的是匹配文本的起始字符在 stringObject 中的位置，input 属性声明的是对 stringObject 的引用。
如果 regexp 具有标志 g，则 match() 方法将执行全局检索，找到 stringObject 中的所有匹配子字符串。
若没有找到任何匹配的子串，则返回 null。如果找到了一个或多个匹配子串，则返回一个数组。不过全局匹配返回的数组的内容与前者大不相同，
它的数组元素中存放的是 stringObject 中所有的匹配子串，而且也没有 index 属性或 input 属性。

例子：

<script type="text/javascript">var str="1 plus 2 equal 3"document.write(str.match(/\d+/g))</script>

输出：
1,2,3



MySql一个语句从同一个表查询多个count：
SELECT
	temp1.CO AS KK,
	temp2.CO AS FF
FROM
	(
		SELECT
			COUNT(1) AS CO
			
			另一个MySql一个语句从同一个表查询多个字段的count的方法：
SELECT * FROM((SELECT COUNT(1) AS qq from t),select count())

SELECT v2.* FROM (SELECT v1.* FROM (SELECT t FROM t_person t WHERE t.id > 100) v1 WHERE rownum <= 40) v2 WHERE rownum >=  20;

Stack<String> s = new Stack<String>();  //实例化栈
s.push("1");  //压栈（往栈中添加元素）
s.push("2");
s.push("3");
s.push("4");
s.push("5");
s.push("6");
boolean isEmpty = s.isEmpty();  //判断栈是否为空栈
boolean cAll = s.containsAll(list));  //是否包含整个集合
boolean cOne = s.contains("5");  //是否包含单个元素
String str1 = s.peek();  //取出栈顶值（不删除）
String str2 = s.pop();  //取出栈顶值（随之删除栈顶值）




package com.sckj.wtgl.action;  //测试类所在包
import java.net.URL;  //url包
import org.codehaus.xfire.client.Client;  //xfire插件包
public class ClientTest {
	public static void main(String[] args) throws Exception {
		URL url = new URL("http://localhost:8080/WTGL/service/checkProblem?wsdl");  //实例化url
		Client client = new Client(url);  //实例化客户端
		Object[] result = client.invoke("getMsg", new Object[]{"good", "bye", "KKK"});  //调用远程函数
		System.out.println(result[0]);  //打印结果集中的第0个
	}
}

线性结构：每个元素至多一个前驱一个后继，有头有尾，头没有前驱，尾没有后继。
树形结构：每个元素除了根以外，只有一个前驱，后继的个数没有限制，0个到多个都可以。
图形结构：每个元素的前驱和后继个数没有任何限制。

线性结构（逻辑结构）：
1.一般线性结构实现，顺序表（数组）、链表
2.受限线性结构实现，队列（先进先出），栈（后进先出）

1.java语言
	语言基础
	设计模式
	算法设计
2.数据库

3.struts2
4.springMVc
5.spring
6.hibernate
7.mybatis

8.html
9.css
10.javascript
11.jquery


共享线程对象的情况下：成员变量与局部变量；
	如果是成员变量，那么多个线程对同一个对象的成员变量的操作是彼此影响的，也就是说一个线程对成员变量的值改变后，后面的线程读的是改变后的值；
	如果是局部变量，那么每个线程会有一个独立的局部变量拷贝，线程间不共用该变量，互不影响。
不共享对象的情况下：自然互不影响。


1.数据类型表示越界问题(加1后越界就成了负数)：存在数i使i > i + 1;
2.存在不满足 i>j || i <= j的数i和j;比如Double.NaN或Float.NaN，任意两个这样的数做大小比较都是false。
3.集合关系图。
4.流关系图。
5.class HelloA {  
    public HelloA(){  
        System.out.println("HelloA");  
    }   
    { System.out.println("I'm A class");}  
    static { System.out.println("static A");}  
}  
public class HelloB extends HelloA {  
    public HelloB() {  
        System.out.println("HelloB");  
    }   
    { System.out.println("I'm B class");}  
    static { System.out.println("static B");}  
    public static void main(String[] args){   
　　　　 new HelloB();   
　　 }  
}
答案：  
static A
static B  
I'm A class  
HelloA  
I'm B class  
HelloB  
解析：说实话我觉得这题很好，考查静态语句块、构造语句块（就是只有大括号的那块）以及构造函数的执行顺序。
对象的初始化顺序：（1）类加载之后，按从上到下（从父类到子类）执行被static修饰的语句；
					（2）当static语句执行完之后,再执行main方法；
					（3）如果有语句new了自身的对象，将从上到下执行构造代码块、构造器（两者可以说绑定在一起）
6.考察多个catch语句块的执行顺序。当用多个catch语句时，catch语句块在次序上有先后之分。
从最前面的catch语句块依次先后进行异常类型匹配，这样如果父异常在子异常类之前，那么首先匹配的将是父异常类，
子异常类将不会获得匹配的机会，也即子异常类型所在的catch语句块将是不可到达的语句。
所以，一般将父类异常类即Exception老大放在catch语句块的最后一个。

7.public class foo{
    public static void main (String[] args){
        String s;
        System.out.println("s=" + s);
		System.out.println(100 + s);
    }
}
运行后才发现Java中所有定义的基本类型或对象都必须初始化才能输出值，否则会编译错误（基本类型自带默认值）。
当s被初始化为null时，就会编译通过，并且输出值为null，同时经过加法运算输出值为s=null和100null；
java中，一切对象、基本数据类型或null与String对象连接后，都会是String。
8.当switch-case语句中的case语句后没有break时，当某一个case满足条件，其后的一系列case将被击穿运行，所以case后一定要跟上break。
7.（1）abstract关键字只能修饰类和方法，不能修饰字段。  
	（2）抽象类不能被实例化（无法使用new关键字创建对象实例），只能被继承。  
	（3）抽象类可以包含属性，方法，构造方法，初始化块，内部类，枚举类，和普通类一样，普通方法一定要实现，
		变量可以初始化或不初始化但不能初始化后在抽象类中重新赋值或操作该变量（只能在子类中改变该变量）。  
	（4）抽象类中的抽象方法（加了abstract关键字的方法）不能实现。  
	（5）含有抽象方法的类必须定义成抽象类。
   
扩展：抽象类和接口的区别，做个总结吧：  
（1）接口是公开的，里面不能有私有的方法或变量，是用于让别人使用的，而抽象类是可以有私有方法或私有变量的。  
（2）abstract class 在 Java 语言中表示的是一种继承关系，一个类只能使用一次继承关系。但是，一个类却可以实现多个interface，
	实现多重继承。接口还有标识（里面没有任何方法，如Remote接口）和数据共享（里面的变量全是常量）的作用。  
（3）在abstract class 中可以有自己的数据成员，也可以有非abstarct的成员方法，而在interface中，
	只能够有静态的不能被修改的数据成员（也就是必须是 static final的，不过在 interface中一般不定义数据成员），
	所有的成员方法默认都是 public abstract 类型的。
（4）abstract class和interface所反映出的设计理念不同。其实abstract class表示的是"is-a"关系，interface表示的是"has-a"关系。  
（5）实现接口的一定要实现接口里定义的所有方法，而实现抽象类可以有选择地重写需要用到的方法，一般的应用里，最顶级的是接口，
	然后是抽象类实现接口，最后才到具体类实现。抽象类中可以有非抽象方法。接口中则不能有实现方法。  
（6）接口中定义的变量默认是public static final 型，且必须给其初值，所以实现类中不能重新定义，
	也不能改变其值。抽象类中的变量默认是 friendly 型，其值可以在子类中重新定义，也可以在子类中重新赋值。

	
HashMap和HashTable都是Map的实现类，其中HashMap是非线程安全的，但更高效，而HashTable是线程安全的。
StringBuilder和StringBuffer都是比String更灵活字符变量操作类，优点是操作在一个对象上，StringBuilder是非线程安全，但更高效，而StringBuffer是线程安全的。
ArrayList和Vector功能极为相似，ArrayList是非线程安全的，但更高效，而Vector是线程安全的。

equals方法是在两个同类型对象上使用：
首先，如果两对象的equals方法为true，其hashCode一定相同，但如果两个对象的equals方法为false，其hashCode不一定。
相对的，如果两个对象的hashCode相同，其equals方法不一定为true，而两个对象的hashCode不相同，其equals方法一定为false。

初学者可以这样理解，hashCode方法实际上返回的就是对象存储的物理地址（实际可能并不是）。
这样一来，当无序集合要添加新的元素时，先调用这个元素的hashCode方法，就一下子能定位到它应该放置的物理位置上。
如果这个位置上没有元素，它就可以直接存储在这个位置上，不用再进行任何比较了；
如果这个位置上已经有元素了，就调用它的equals方法与新元素进行比较，相同的话就不存了，不相同就散列其它的地址（链式地址）。
所以这里存在一个冲突解决的问题。这样一来实际调用equals方法的次数就大大降低了，几乎只需要一两次。

1.进程是一个运行中的应用程序，具有独立的内存空间，线程是一个进程中的一条执行路径，一个进程至少包含一个线程，同进程下的多个线程可以共享内存。
2.new、start、
3.sleep方法，进入休眠状态，让出cpu，但不释放资源对象锁。
4.join等待关联线程执行完才执行。
5.用户线程和守护线程（后台），

当需要处理批量任务时可以用线程池。
线程池顶级接口是Executor，其子接口是ExecutorService，定义了execute(Reunnable r);方法，传入线程实现对象即可自动调用线程的执行方法（可以不必实例化）。
Executors是主要的线程池工厂类，有许多静态方法可以用来创建线程池：
	1.newSingleThreadExecutor();创建一个单线程的线程池（即容量为1），如果池中的唯一线程由于异常而死亡了，池会创建一个新的同类线程来代替他。
	池中只有一个线程在工作，池中的所有任务由一个线程串行执行，池保证任务的执行顺序按照提交顺序排队，依次进行（上一个完成之后才轮到下一个）。
	2.newFixedThreadExecutor();创建一个指定容量的线程池，每次提交一个任务就创建一个线程，直到到达池的容量，如果有线程因异常而死亡，
	池会创建新的线程来充满池；池中所有线程同时工作（宏观上），当池容量小于任务数时（线程数就会小于任务数），任务会排队，有限的线程(空闲者优先)会加班执行。
	3.newCachedThreadPool();创建一个可缓存的池，池的最大容量完全依赖于OS或JVM所能支持的线程数量；当池的容量大于任务所需线程数，
	OS或JVM会自动回收掉一部分空闲（60秒不执行任务）线程，当已有线程不够用时，又会自动创建线程来补充。
	4.newScheduleThreadPool(int coreSize);创建不限制池，参数表示基本容量，不论是否空闲都会保持基本容量而不回收。
	
Lambda表达式（lambda expressions）
匿名类型最大的问题就在于其冗余的语法。有人戏称匿名类型导致了“高度问题”（height problem）：
比如前面ActionListener的例子里的五行代码中仅有一行在做实际工作。

lambda表达式是【匿名方法】，它提供了轻量级的语法，从而解决了匿名内部类带来的“高度问题”。

下面是一些lambda表达式：
(int x, int y) -> x + y
() -> 42
(String s) -> { System.out.println(s); }
第一个lambda表达式接收x和y这两个整形参数并返回它们的和；
第二个lambda表达式不接收参数，返回整数'42'；
第三个lambda表达式接收一个字符串并把它打印到控制台，不返回值。

lambda表达式的语法：参数列表、箭头符号"->"和函数体组成。函数体既可以是一个表达式，也可以是一个语句块：

表达式：表达式会被执行然后【返回执行结果】。
语句块：语句块中的语句会被依次执行，就像方法中的语句一样
	return语句会把控制权交给匿名方法的调用者
	break和continue只能在循环中使用

如果函数体有返回值，那么函数体内部的每一条路径都必须返回值
表达式函数体适合小型lambda表达式，它消除了return关键字，使得语法更加简洁。

lambda表达式也会经常出现在嵌套环境中，比如说作为方法的参数。为了使lambda表达式在这些场景下尽可能简洁，我们去除了不必要的分隔符。
不过在某些情况下我们也可以把它分为多行，然后用括号包起来，就像其它普通表达式一样。

下面是一些出现在语句中的lambda表达式：

FileFilter java = (File f) -> f.getName().endsWith("*.java");//等号后面实际是一个匿名函数，函数的运行结果被返回并赋给左边的变量。
String user = doPrivileged(() -> System.getProperty("user.name"));
new Thread(() -> {
  connectToService();
  sendNotification();
}).start();


public class zhiyinshu{
	public static void main(String[] args){
		System.out.print("该数为:");
		Scanner input=new Scanner(System.in);
		int i=input.nextInt();
		int l=2;
		System.out.print(i+"=");
		while(i>1){
			while(i%l==0){
				i=i/l;
				if(i==1)
				System.out.print(l);
				else
				System.out.print(l+"*");
			}
			l=l+1;
		}
	}
}

Method[] methods = userCla.getMethods();  
for(int i = 0; i < methods.length; i++){  
   Method method = methods[i];  
   if(method.getName().startsWith("get")){  
	  System.out.print("methodName:"+method.getName()+"\t");  
	  System.out.println("value:"+method.invoke(bean));//得到get 方法的值  
   }  
}


JVM之类加载器
一、ClassLoader；用于将类的class文件加载到内存。
	JVM预定义类加载器：根类加载器（c++实现开发者无法获得该类Bootstrap）、
						扩展类加载器（java实现Extension）、
						系统类加载器（java实现App/System）。
	用户自定义加载器：java.lang.ClassLoader的子类，用户可自定义加载方式、时机等。
	加载器不需要等到某个类被主动使用时才加载该类，JVM规范允许类加载器在预料到某个类将被使用时预先加载该类，
	如果在加载过程中遇到class文件丢失或错误（如类的编译版本不兼容），
	加载器必须在程序首次主动使用该类时才报LinkageError错，如果这个类一直没有被主动使用，加载器一直不会报错。

二、java虚拟机与程序的生命周期：
	——System.exit(int n);//当n为0表示正常的受控的退出，否则表示非正常退出。
	——程序执行结束。
	——程序在运行中遇到异常，而没有catch的JVM进程终止。
	——操作系统终止了。

三、ClassLoader工作过程
——加载：查找并加载类的二进制数据（class文件加载到内存）。
class文件的加载方式：从本地系统加载、通过网络下载（URLClassLoader）、从zip和jar等压缩文件提取class文件、
					从专有数据库中提取、将源文件动态编译成class文件。
	1.字节码被加载到内存后，将其存放到运行时数据区的方法区中，然后在堆中创建一个对应的
		java.lang.Class对象用来封装类在方法区中的数据结构并且向开发者提供了访问方法区内数据结构的接口
		（可以从这个对象通过反射访问所有与该类有关的东西）这个对象是加载过程的最终产物（每一个Class对象都包含一个
		加载【该类】的加载器对象的引用null表示用的是Bootstrap）。
——连接：将已加载到内存中的二进制数据根据相互间的引用关系整合到JVM的运行时环境中。
	1）验证：确保加载的class文件的正确性（避免非法class文件）。
		1.文件结构检查：确保文件结构符合java类文件的固定格式。
		2.语义检查：确保类本身符合java的语法，如final类无子类，final方法无覆写。
		3.字节码检查：确保字节码流能被JVM安全的执行，字节码流代表java方法（实例方法和静态方法），
		是一系列称为操作码的单字节指令组成的指令序列，每个操作码后跟一个或多个操作数。字节码检查会检查每个操作码是否合法，
		即其后是否跟了合法的操作数。
		4.二进制兼容检查：确保相互引用的类之间的关系一致。
	2）准备：为类的静态变量分配内存，并为其分配系统默认值（按变量类型来）。
	3）解析：把类中的符号引用转换为直接引用（如car.run(10),调用的是引用对象car的run方法，
	这就是符号引用，解析过程就是将这种符号引用替换为指向Car类的run方法在方法区内位置的指针）。
		
——初始化：为类的静态变量赋予正确的初始值（如果【已】有显式赋值就初始化为显式值，如果没有就保持系统默认值），执行静态块。

四、java程序对类的使用方式：
——主动使用（6种）：创建累的实例、读写类或接口的静态变量、调用类的静态方法、反射使用类、
			初始化一个类的子类时主动使用父类、虚拟机启动时被标明为启动类（包含main方法）的类。
——被动使用：
五）所有java虚拟机实现必须在每个类或接口被java程序“首次主动使用”时才初始化他们。


中央仓库：远程服务器
本地仓库：本机
代理仓库：本地私服

1.maven项目构建过程中先查询本地仓库、再查询代理仓库（如果有代理仓库）最后才到中央仓库查找。
2.可以把多数项目都会用到的公共jar包放在一个parent里面，其他项目都来继承他，一方面可以减少重复jar包配置，更重要的是可以统一控制公司的所有jar包的版本以达成技术的统一。


关系型数据库水平拆分和垂直拆分
（以mysql为例，mysql能承受的数量级在百万静态数据可以到千万）
 
简单购物系统涉及如下三表：
1.产品表（数据量10w，稳定）
2.订单表（数据量200w，且有增长趋势）
3.用户表 （数据量100w，且有增长趋势）
 
一、垂直拆分：
1.能解决问题：表与表之间的io竞争
2.不能解决问题：单表中数据量增长出现的压力
3.方法：把产品表和用户表放到一个server上订单表单独放到一个server上
 
二、水平拆分：
1.能解决问题：单表中数据量增长出现的压力
2.不能解决问题：表与表之间的io争夺 
3.方法：用户表通过性别拆分为男用户表和女用户，表订单表通过已完成和完成中拆分为已完成订单和未完成订单，
产品表和未完成订单放一个server上，已完成订单表和男用户表放一个server上，女用户表放一个server上(女的爱购物)




Redis
1.内存数据库，避免缓慢的磁盘IO，所有数据存在内存中读写速度快。
2.提供数据备份机制，全量备份RDB、增量备份AOF。
3.操作原子性，因为Redis是单进程、单线程的（由于Redis的处理速度远远超过目前网络带宽，
	所以不需要多进程、多线程，同时多进程、多线程机制增加了系统复杂性，会导致数据互斥）。
4.事件驱动机制，低cpu占用率。
5.支持订阅/发布模型。
6.支持key过期，适用做缓存。
Redis适用场景：
1.获取最新的N条数据。
2.排行榜应用，如获取Top 500条数据。
3.需要精确控制过期时间的应用。
4.计数器应用。
5.Unique操作，如获取某段时间下的所有数据并排除重复值。
6.实时系统，反垃圾系统。
7.Pub/Sub构建实时消息系统（发布/订阅系统）。
8.构建队列系统（用链表可实现队列和栈）。
9.作为缓存。

				Redis	MogoDB	MySql
数据库概念		有		有		有
表/字段的概念	无		集合	有
行/列的概念		无		无		有


Redis中一旦一个key或者field的属性被删除或值为null，即表示不再具有这个key或field。
1.数据类型多样，灵活对应不同大小的对象【所有数据类型都是针对value的】。

【String数据类型】Redis中最简单的数据类型，一个key对应一个value，Sring类型是二进制安全的，
在Redis中String类型可以用来存储任何数据，如多媒体数据、序列化对象等。
方法{
	set key value;//返回OK(设置key对应的值为string类型的value，如果存在旧键值对，则覆盖旧值)
	setnx key value;//返回成功操作的条数(设置key对应的值为string类型的value，如果key已存在则不覆盖旧值，同时返回0；如果不存在则键值对新增成功) 
	setex key 10(秒) value;//(设置key对应的值为string类型的value，并指定键值对的有效期，过期返回null) 
	setrange key offset gmail.com;//从起点参数开始覆盖起点及其后子串对应位置上的字符，如果同时保留覆盖范围外的全部字符，返回替换后的串总长
	mset key1 value1 key2 value2 key3 value3;//【事务属性】一次设置多个key的值，返回OK表示全部成功，返回0表示0条成功。
	msetnx key1 value1 key2 value2 key3 value3;//【事务属性】一次设置多个key的值，返回OK/1表示全部成功，返回0表示0条成功，但是不会覆盖已存在的key。
	incr key；//使key的值递增一次，若key值未设置，则设置原值为0，并增1，返回1。
	incrby key n；//使key的值递增阶值n，n可以为负，若key值未设置，则设置原值为0，并增加阶值后返回。
	deci key；//使key的值递减一次，若key值未设置，则设置原值为0，并减1，返回-1。
	deciby key n；//使key的值递减阶值n，n可以为负，若key值未设置，则设置原值为0，并减去阶值后返回。
	append key；//给key的值追加串，返回追加后的长度。
	strlen key；//查看key的值的串长。
	get key;//获取key的值，如果未设置则返回null。
	getset key value；//返回旧值，并设置新值。
	getrange key1 offset end；//闭区间去子串。
	mget key1 key2 key3;//批量取值，若有key没有设置值则返回null。
}
【Hash数据类型】是一个String类型的field和value的映射hash表，他的添加、删除操作都是0(1)(平均)。hash特别适合用于存储对象，
相较于将对象的每个字段存储成单个的string类型，将对象存储在hash类型中会占用较少的内存，并且可以更方便的存取整个对象。
方法{
	hset hashTabName:objectID fieldName value;//设置hash表的id为objectID的对象，field字段为指定值，若field不存在则先创建（hset user:001 age 44;//表示设置user表中001对象的age值为44）。
	hget hashTabName:objectID fieldName；//获取hash表的id为objectID的对象，指定field字段值，若指定field不存值在则返回null（hget user:001 age;//表示获取user表中001对象的age值）。
	hsetnx hashTabName:objectID fieldName value;//返回1，表示设置成功，0表示设置失败，设置hash表的，field字段为指定值，若field原值不存在（hsetnx user:001 age 44;//表示设置user表中001对象的age值为44如果无原值）。
	hmset hashTabName:objectID fieldName1 value1 fieldName2 value2；//【事务属性】批量设置同一个对象的多个字段，成功返回ok，失败返回0。
	hmget hashTabName:objectID fieldName1 value1 fieldName2 value2；
	hincrby hashTabName:objectID fieldName n；//增加阶值。
	hexists hashTabName:objectID fieldName;//判断指定对象是否具有某个字段，若具有返回1，不具有返回0。
	hlen hashTabName:objectID;//返回指定对象具有的字段的个数。
	hdel hashTabName:objectID field；//删除指定对象的指定字段值。
	hkeys hashTabName:objectID；//返回指定对象的所有字段名。
	hvals hashTabName:objectID；//返回指定对象的所有字段值。
	hgetall hashTabName:objectID；//获取指定对象的所有【键值对】。
	}
【List】是一种每个元素都是string类型的双向链表结构，主要功能是push、pop、获取特定范围内所有值等，
操作中的key理解为链表的名字，可以通过push、pop从链表头或尾添加或删除元素，这样list既可以作为栈也可以作为队列使用。
方法{
	lpush listName value;//栈实现，从头部压入元素，0在头部，成功返回压入后list总长度，失败返回-1。
	rpush listName value;//队列实现，从尾部压入元素，0在头部，成功返回压入后list总长度，失败返回-1。
	lpop listName；//从list的头部弹出一个元素，返回弹出元素。
	rpop listName；//从list的尾部弹出一个元素，返回弹出元素。
	lrange listName offset end;//从头取值（即0在头部），批量取出list闭区间所有元素（lrange listName 0 -1；表示取出整个list所有元素）。
	linsert listName before element0 elementToInsert；//在元素间插入元素（before、after都是以0在头部数起，头的方向为前），放回list长度。
	lset listName index newValue;//将指定位置的值覆盖为新值，成功返回OK。
	lrem listName num value；//从list中删除num个值为value的元素，返回被删除元素个数（0-n）。
	ltrim listName offset end；//保留指定下标闭区间内的数据，删除其两端的所有元素（ltrim list1 1 -1；表示只保留第0位同时删除其他所有元素）。
	rpoplpush listName1 listName2；//从第一个list的尾部弹出一个元素，并将第一个list剩下部分从头部压入第二个list，返回弹出元素。
	lindex listName n；//返回指定索引位置上的元素。
	llen listName；//返回list长度。
}
【Set】是集合，是string类型的无序集合，set是通过hash table实现的，添加、删除和查找的复杂度都是0（1），
对集合可以求并、求交，求差（这些操作可以实现sns中的共有好友推荐和blog的tag功能）。
方法{
	sadd setName eleValue；//向集合中添加元素,返回值为添加成功的条数（0-n），重复添加返回0。
	smebers setName;//返回集合中的所有元素。
	srem setName eleValue;//删除集合中指定元素，成功返回被删除条数，失败返回0。
	spop  setName；//从集合中【随机】弹出一个元素，返回被弹出元素值。
	sdiff setKey0 setKey1;//以集合0为驱动，与集合1比较，返回两个集合的差（最终返回的是驱动集中未被匹配到的所有元素，差集一定是驱动集的全字迹，差集于非驱动及无交集）。
	sdiffstore setKey2 setKey0 setKey1;//求集0与集1的差集并将差集存入集2，返回值为集2的长度。
	sinter setKey0 setKey1;//求集0与集1的交集，返回交集。
	sinterstore setKey2 setKey0 setKey1;//求集0与集1的交集并将交集存入集2，返回值为集2的长度。
	sunion setKey0 setKey1;//求集0与集1的并集，返回并集。
	sunionstore setKey2 setKey0 setKey1;//求集0与集1的并集并将并集存入集2，返回值为集2的长度。
	smove setKey0 setKey1 eleValue；//从集0中的指定元素移动到集1。
	scard setKey;//返回集合的长度。
	sismember setKey eleValue;//判断指定元素是否是集合成员，是返回1，不是返回0。
	srandmember setKey;//随即返回一个集合元素，但不删除元素。
}
【Sorted Set】(zset)有序集合，是set类型的升级版，是在set基础上增加了顺序属性，顺序属性在添加、修改元素时可以指定，
每次指定后，zset会自动按新的值调整顺序，可以理解为具有两个字段的sql表，一个字段为valu，一个字段为sort，
而key可以理解为zset的名字。
方法{
	zadd zsetKey n eleValue；//向zset元素中添加元素，并指定序号，返回值为新增条数，如果存在重复旧值而序号不同则更新序号此时返回0，但如果序号相同值不同则返回1，亦即可以多值同序。
	zrange zsetKey 0 -1 withscores;//批量输出zset元素，并附带序号，按序号升序输出。
	zrem zsetKey value；//删除有序集合中的元素，返回删除条数。
	zincrby zsetKey sortNum；//如果集合中存在旧元素则其顺序号增加阶值，如果不存在则向集合中添加元素并将其顺序号设为阶值。
	zrank zsetKey eleValue；//返回z集合中指定元素的排名（index值，而不是顺序值，但这里的index是先按score升序排列的，然后取index）。
	zrevrank zsetKey eleValue；//返回z集合中指定元素的排名（index值，而不是顺序值，但这里的index是先按score降序排列的，然后取index）。
	zrevrange zsetKey eleValue；//批量输出zset元素，并附带序号，按序号降序输出。
	zrangebyscore zsetKey 0 -1 withscores;//按序号升序返回闭区间内所有元素。
	zcount zsetKey offset end withscores；//返回指定闭区间内的元素个数。
	zcard zsetKey；//返回z集合的长度。
	zremrangebyrank zsetKey offset end；//删除z集合中排名在指定索引区间的元素，返回作用条数。
	zremrangebyscore zsetKey offset end；//删除z集合中顺序号在指定索引区间的元素，返回作用条数。
}
key-value相关命令{
	keys *;//返回库中所有的key集。
	keys *str；//返回库中所有以指定串结尾的key集。
	keys str*；//返回库中所有以指定串开头的key集。
	exists key；//返回key的条数（0/1）。
	del key；//删除key，返回被删除key的条数。
	expire key seconds；//设置某key的过期时间（秒），成功返回作用条数。
	ttl key；//返回key的剩余存活时间（秒），返回负数或0表示已过期，-1表示没有过期限制。
	move key destDB；//将key转移到目标库，成功返回移动条数。
	persist key；//取消key的过期时间，。
	randomkey；//随机返回库中的一个key。
	rename key newKey；//重命名key，成功返回OK。
	type key；//返回key的数据类型。
}
服务器相关命令{
	select 0；//表示进入0库，redis共16个数据库（名字分别是0-15），默认进入的是0库，若选择超出范围则报无效数据库索引错。
	ping；//测试链接是否存活，成功返回pong。
	echo anyStr；//返回anyStr。
	exit/quit；//退出连接。
	dbsize；//返回当前库的key数量。
	info；//返回服务器软硬件、客户端、启用库的概要信息。
	config get option；//返回指定选项的配置信息。
	flushdb；//清除当前库中的所有key。
	flushall；//清除所有库对象中的所有key，不论当前库的位置。
}

Redis高级实用特性
1.安全性
配置
{
	Redis在一台好服务器上每秒能进行高达150k次的密码尝试，
	为保证安全客户端连接后进行任何操作都需要非常强壮的密码验证。
	# requirepass foobared//在服务器配置文件的这个位置下启用密码验证功能。
	requirepass someStr；//设置密码，重启服务，进入客户端后任何操作都需要验证。
	redis-cli -a myPassword；//在进入客户端时输入密码，无返回值。
	auth myPassword；//进入客户端后输入密码，成功返回OK。
}
2.主从复制
过程：slave与master建立连接，发送sync同步命令，master接到同步请求后，master主进程会处理外来的读写并缓存，同时会另启
一个后台进程将其数据库写入一个文件，然后将此文件发送给slave，slave将此文件写入自己的磁盘，下次slave
启动时就读本地磁盘上的文件，将文件中的库映射到其内存达到数据库主从同步的目的。
{
	1.通过主从复制可以允许多个slave server拥有与master server相同的数据库副本。一个master可以拥有多个slave，
	所有slave都与master相连。
	2.slave之间还可以互联，作为为master故障后的继承者。
	3.主从复制过程中不会锁定master，master可以继续处理业务数据。
	4.可提高系统伸缩性。
	只要完成了连接与验证，只要在主服务器上有的库和key从机也就有了，查看角色及相关配置，只需info命令即可。
	# slaveof <masterip> <masterport>//在从机此位置填写master ip地址和端口
	# masterauth <master-password>//在从机的此位置配连接密码。
}
3.事务处理
Redis只能保证一个client发起的同一事务中的多个命令顺序的执行，事务过程中不会插入其他client的命令。
当一个client在一次连接中发出multi命令时，这个连接就会进入事务上下文，该连接的其他后续命令不会立即执行，
而是进入等待队列，当执行exec命令时，redis会批量顺序的执行等待队列的所有命令【但队列中的部分操作失败，整个事务不会回滚】。
命令{
	multi；//进入事务上下文，打开事务队列。
	exec；//批量顺序的执行等待队列的所有操作。
	discard；//清空事务队列，退出事务上下文，达到事务回滚效果。
}
乐观锁{
	大多乐观锁是基于数据版本的记录机制实现的，即为数据增加一个版本标识。在基于数据库表的版本解决方案中，
	一般是通过为数据库表添加一个version字段，读数据时将此版本号一同读出，更新该数据时，数据库对此版本号加1，
	之后，将提交的版本号与数据库表对应记录的当前版本号进行对比，如果提交的数据版本号大于库中的当前版本号，
	则予以更新，否则视为过期数据不做更新处理。
	watch key；//监控指定key，当exec时如果受监控的key在调用watch后发生了变化，则这个事务失败。
	unwatch；//取消连接中的所有监控。
	也可以多次调用watch监控多个key，就可以对指定的key加乐观锁了。被监控的key在整个连接中都有效，事务也一样。
	一旦连接断开，监控和事务都会被一并清除。exec、discard、unwatch都会清除连接中的所有监控。
}
4.持久化机制
{
	snapshotting；//将内存数据以快照的方式写入磁盘中的二进制文件中，默认文件名为dump.rdb，可以配置
	自动化做快照持久化，如可以配置redis在n秒内如果有超过m个key被添加、修改或删除就自动执行一次快照持久化。
	是默认持久化方式，。
	{
		
	}
	append-only file;//将操作动作保存到磁盘中的文件里，由于OS会在内核中缓存write做的修改，所有可能不会立即将修改写入磁盘，这样的话aof的持久化也可能导致数据丢失。
	可以通过配置文件告诉redis我们想要通过fsync函数强制OS写入修改数据到磁盘的时机。
	{
		#appendonly yes//启用aof。
		# appendfsync always//收到写入操作就立即持久化到磁盘，最慢，但是能保证数据完全持久化。
		#appendfsync everysec//每隔一秒持久化一次，在性能和数据完整性两方面做了均衡。
		# appendfsync no//完全依赖OS控制持久化频率，性能最高，但是不能保证数据完整性。
	}
}
5.发布订阅消息
发布订阅（pub/sub）是一种消息通信模式，主要目的是接触发布者和订阅者间的耦合，redis作为一个pub/sub的server，
在订阅者和发布者间起到了消息路由的功能，订阅者可以通过subscribe和psubscribe命令向redis server订阅自己想要的
消息类型，redis中的消息类型称为channel，当发布者用publish命令向redis server发布特定类型的消息后，订阅
该消息类型的全部client都会接收到此消息。
{
	
}
6.虚拟内存
Redis的虚拟内存与OS的不同，但是思路和目的是相同的，就是把暂时不经常访问的数据从内存交换到磁盘，腾出内存
空间，用于经常访问的数据的存储，对于redis这样的内存数据库，内存总是不够用的，除了可以将数据分割到多个
redis server上外，另一个有效方式就是把闲置数据交换到磁盘。
{
	vm-enabled yes//启用虚拟内存。
	vm-swap-file diskPath//交换出来的value保存的路径。
	vm-max-memery 1000000//redis使用的内存上限。
	vm-page-size 32//虚拟内存每个页面大小32byte。
	vm-pages 134217728//虚拟内存最多是用多少个页面。
	vm-max-threads 4//用于执行value对象换入磁盘的工作线程数量。
}
/**************************Linu安装Redis*****************************/
1.到Redis官网下载redis-3.2.7.tar.gz（也可以在线下载安装源码到当前位置：wget http://download.redis.io/releases/redis-3.2.7.tar.gz）
2.tar xzf redis-3.2.7.tar.gz  //解压源码
3.cd redis-3.2.7  //到解压后目录
4.make  //编译C源码，此时需要系统安装gcc编译器，否则会报gcc: Command not found错
如果继续报zmalloc.h:50:31: fatal error: jemalloc/jemalloc.h: No such file or directory，
执行make MALLOC=libc，然后make test看看有没有错，无错执行make install即可。
5.启动Redis服务器：./redis-server &  //在src目录下操作，添加【&】表示服务后台运行
6.启动Redis客户端：./redis-cli  //在src目录下操作(客户端是一个前台程序可以做测试)
7.关闭服务器：在客户端下执行：shutdown命令  //允许用Redis客户端关闭其服务器
8.退出客户端：exit
/****************************************************************************/
允许远程访问
1.修改Redis.conf文件，将bind 127.0.0.1行注释掉
2.重启Redis服务器
3.用redis-cli登陆，执行config get requirepass查看是否需要密码，如果结果是【1) "requirepass" 2) ""】表示可免密码访问
4.config set requirepass myPassword，返回【OK】表示成功
5.执行keys *命令看看能不能取出键列表，如果提示【(error) NOAUTH Authentication required.】则表示需要密码
6.执行认证命令输入密码auth myPassword，在执行keys *
/****************************************************************************/
简单的单线程Redis操作（测试前Redis服务器必须设置访问密码）
	Jedis jedis = new Jedis("192.168.88.131", 6379);  //新建Redis连接实例
	jedis.auth("kkk");  //为客户端设置密码
	System.err.println(jedis.ping());  //测试连通性，正常输出：PONG
	System.err.println(jedis.set("hello", "world"));  //测试存放键值对，正常输出：OK
	System.err.println(jedis.get("hello"));  //测试根据键取值，正常输出：world
/****************************************************************************/
 *
 * 在不同的线程中使用相同的Jedis实例会发生奇怪的错误。但是创建太多的实现也不好因为这意味着会建立很多sokcet连接， 
 * 也会导致奇怪的错误发生。单一Jedis实例不是线程安全的。为了避免这些问题，可以使用JedisPool, 
 * JedisPool是一个线程安全的网络连接池。可以用JedisPool创建一些可靠Jedis实例，可以从池中拿到Jedis的实例。 
 * 这种方式可以解决那些问题并且会实现高效的性能 
 * 
/****************************************************************************/
1.引入泛型可以获得编译时的类型安全和降低运行时抛出类型转换异常的风险（只要编译时不出问题，执行就不会出问题）。
2.所谓泛型：就是【变量类型】的参数化。
public class Gener<T>{
	private T foo;
	getFoo(){}
	setFoo(){}
	
	public static void main(String[] args){
		Gener<Integer> i = new Gener<Integer>();
		Gener<String> s = new Gener<String>();
		
		i.setFoo(new Integer(111));//存入时就会类型检查，如果不对就会编译错误，避免执行时报错
		s.setFoo(new String("kkkkkk"));
		
		i.getFoo();
		s.getFoo();
	}
}


程序结构{
	1.Go程序通过package组织程序，只有main包里才能包含main函数。
	2.一个可执行程序有且只有一个main包，main包里有且只有一个main函数。
	3.导入包如果多余会编译出错，导入包可以设置别名。
	4.Go语言中所有函数、变量、类型、结构、接口、变量、常量的访问控制都是由索引名指定的，首字母大写表示public、
	小写表示private，在Go语言中并不存在public和private关键字，两个概念的控制范围是都是包，即public表示所有包都可以访问而private表示仅限本包访问。
}

数组操作：
搜索：
{
	1.遍历查找，逐个匹配元素的值，如果相等就返回下标、如果没有匹配的就返回-1。[逻辑简单，但效率低]
	2.二分查找，先将数组排序，找出排序后的数组的中间位置数（中位数），之后看查询目标在中位数的哪边，
		同时忽略另一边，再将剩下一边去中位数，如此递归直到找到搜索完毕。
}





















lj_yjx      yjx123
https://211.149.197.146/svn/lj_park




// 棰滆壊#FF00FF鏍煎紡杞负Array(255,0,255)
function color2rgb(color){
	var r = parseInt(color.substr(1, 2), 16);
	var g = parseInt(color.substr(3, 2), 16);
	var b = parseInt(color.substr(5, 2), 16);
	return new Array(r, g, b);
}
// 棰滆壊Array(255,0,255)鏍煎紡杞负#FF00FF
function rgb2color(rgb){
	var s = "#";
	for (var i = 0; i < 3; i++)
	{
		var c = Math.round(rgb[i]).toString(16);
		if (c.length == 1)
		c = '0' + c;
		s += c;
	}
	return s.toUpperCase();
}
// 灏嗙敾甯冭浆鍖栦负鍦板潃
function convertCanvasToImage(canvas) {
  var image = canvas.toDataURL("image/png");
  return image;
}

$(document).ready(function(){
	// 棰滆壊閫夋嫨妗嗚仈鍔?
	$("#main-form").on("change",".color",function() {
		$(this).parent().parent().find("input").val($(this).val());
	});

	var color_num = 2;
	$("#add_color").click(function(){
		if(color_num<6){
			color_num++;
			$("#main-form").append('<div class="form-group"><label for="color-'+color_num+'">閫夋嫨涓�涓鑹诧紙'+color_num+'锛?</label><div class="col-xs-10"><input type="text" class="form-control input-lg color" id="c-'+color_num+'" value="#000000"></div><div class="col-xs-2"><input type="color" class="form-control input-lg color-in color" id="color-'+color_num+'"></div><div class="clearfix"></div></div>');
			if(color_num>=6){
				$("#add_color").attr('disabled',"true");
			}
			if(color_num>=3){
				$("#remove_color").removeAttr("disabled");
			}
		}
	});
	$("#remove_color").click(function(){
		if(color_num>2){
			color_num--;
			$("#main-form .form-group:last-child").remove();
			if(color_num<6){
				$("#add_color").removeAttr("disabled");
			}
			if(color_num<=2){
				$("#remove_color").attr('disabled',"true");
			}
		}
	});


	$("#make-card").click(function() {
		var canvas = document.getElementById('myCanvas');
		if (canvas.getContext) {
			var ctx = canvas.getContext('2d');
		}
		var rgb = new Array();
		for (var i = 0; i < color_num; i++) {
			color = $("#c-"+(i+1)).val();
			rgb[i] = color2rgb(color);
		};

		var cardcolor = new Array();
		var card_num = $("#card-num").val();
		ctx.fillStyle = $("#c-1").val();
		ctx.fillRect(0, 0, 1920, 300);
		for (var i = 0; i < (color_num-1); i++) {
			var rgb_bh = new Array();
			rgb_bh[0] = (rgb[i][0]-rgb[i+1][0])/(1020/(color_num-1));
			rgb_bh[1] = (rgb[i][1]-rgb[i+1][1])/(1020/(color_num-1));
			rgb_bh[2] = (rgb[i][2]-rgb[i+1][2])/(1020/(color_num-1));
			for (var n = 0; n < (1020/(color_num-1)); n++) {
				var rgb_sc = new Array();
				rgb_sc[0] = rgb[i][0]-n*rgb_bh[0];
				rgb_sc[1] = rgb[i][1]-n*rgb_bh[1];
				rgb_sc[2] = rgb[i][2]-n*rgb_bh[2];
				var color_sc = rgb2color(rgb_sc);
				ctx.fillStyle = color_sc;
				ctx.fillRect(0, i*(1020/(color_num-1))+n+300, 1920, 1);
				for (var k = 1; k < card_num; k++) {
					if((i*(1020/(color_num-1))+n)==(1020*k/(card_num-1))){
						cardcolor[k-1]=color_sc;
					}
				};
			};
		};

		ctx.fillStyle = $("#c-"+color_num).val();
		ctx.fillRect(0, 1320, 1920, 600);

		ctx.fillStyle = "rgba(0,0,0,0.2)";
		ctx.fillRect(0, 0, 1920, 1920);

		ctx.shadowOffsetX = 40; // 璁剧疆姘村钩浣嶇Щ
		ctx.shadowOffsetY = 40; // 璁剧疆鍨傜洿浣嶇Щ
		ctx.shadowBlur = 160; // 璁剧疆妯＄硦搴?
		ctx.shadowColor = "rgba(0,0,0,0.2)"; // 璁剧疆闃村奖棰滆壊
		ctx.fillStyle = "#333333"; 
		ctx.fillRect(450,350,1020,1220);

		ctx.fillStyle = $("#c-1").val();
		ctx.fillRect(450, 350, 1020, 320);
		// 璁剧疆瀛椾綋
		ctx.font = "30px Consolas";
		// 璁剧疆瀵归綈鏂瑰紡
		ctx.textAlign = "left";
		// 璁剧疆濉厖棰滆壊
		ctx.fillStyle = "rgba(255,255,255,0.8)";
		// 璁剧疆瀛椾綋鍐呭锛屼互鍙婂湪鐢诲竷涓婄殑浣嶇疆
		ctx.fillText($("#c-1").val(), 470, 645);
		for (var i = 0; i < (card_num-2); i++) {
			ctx.fillStyle = cardcolor[i];
			ctx.fillRect(450, 670+i*80, 1020, 80);
			ctx.fillStyle = "rgba(255,255,255,0.8)";
			ctx.fillText(cardcolor[i], 470, 725+i*80);
		};
		ctx.fillStyle = $("#c-"+color_num).val();
		ctx.fillRect(450, 670+(card_num-2)*80, 1020, 900-(card_num-2)*80);
		ctx.fillStyle = "rgba(255,255,255,0.8)";
		ctx.fillText($("#c-"+color_num).val(), 470, 725+(card_num-2)*80);

		$("#color-card .panel-body").html('<img src="'+convertCanvasToImage(canvas)+'" width=100%>');
		$("#color-card").show();
	});
});


<%@include file="/public/common/front/head.jsp"%>
<%@include file="/public/common/front/foot.jsp"%>

ORDER BY t.ACTIVITY_RELETIME DESC

<%
	String url = request.getScheme() + "://";//获取请求协议类型
	url += request.getHeader("host");//获取服务器地址
	url += request.getRequestURI();//获取请求名
	if (request.getQueryString() != null) {
		url += "?" + request.getQueryString();
	}
	session.setAttribute("myUrl", url);//将地址放到session
%>
    
function submitData(){
$.ajax({
		url:"${pageContext.request.contextPath}/services/service/LjService.checkServiceItem.json",
		type:"post",
		contentType: "application/json; charset=utf-8",  //发送数据到服务器时所使用的内容类型。默认是："application/x-www-form-urlencoded"。
		dataType: "json",  //期望的服务器响应的数据类型。
		data:{
			"id":"${param.id}",
			"state":$(".check li input[name='state']:checked").val(),
			"stateText":$("#checkText").val()
		},
		success:function(data){
			var result=data.responseData.result;
			if(result=="1"){
				 ACore.initWin("审核成功");
				window.location.href="${pageContext.request.contextPath}/public/LjService/service/LjService_checker_index.jsp";
			}
			else{
				 ACore.initWin("审核失败");
			}
		}
	})
}

function redirect_publish(){
	var destUrl="${pageContext.request.contextPath}/public/lease/service/Lease_add.jsp";
	window.location.href="${pageContext.request.contextPath}/public/common/uc/index.jsp?url=" + destUrl;
}

深蓝按钮
{
	float: left;
	border: none;
	border-radius: 5px;
	width: 160px;
	cursor: pointer;
}style="padding:6px 30px;background:#2E89EF;border-radius:5px;color:#fff;"

浅蓝色按钮
{
	display: inline-block;
	background-color: #83b7e9;
	color: #FFF!important;
	text-align: center;
	line-height: 24px;
	border-radius: 10px;
	font-size: 13px;
	padding: 3px 8px;
}

<%
	String url = request.getScheme() + "://";//获取请求协议类型
	url += request.getHeader("host");//获取服务器地址
	url += request.getRequestURI();//获取请求名
	if (request.getQueryString() != null) {
		url += "?" + request.getQueryString();
	}
	session.setAttribute("myUrl", url);//将地址放到session
%>

jsp中写java脚本：
<%
String path = request.getContextPath();//项目及页面上下文，即应用名（/MyBiz）
String basePath = request.getScheme()+"://"+request.getServerName()+":"+request.getServerPort()+path+"/";  //完整的服务器及页面地址（http://localhost:8080/MyBiz/）
%>
<base href="<%=basePath%>">

前台获取项目上下文路径：
<c:set var="ctx" value="${pageContext.request.contextPath}"></c:set>

<filter>
<filter-name>jfinal</filter-name>
<filter-class>com.jfinal.core.JFinalFilter</filter-class>
<init-param>
<param-name>configClass</param-name>
<param-value>cn.myapp.config.MyAppConfig</param-value>
</init-param>
</filter>
<filter-mapping>
<filter-name>jfinal</filter-name>
<url-pattern>/*</url-pattern>
</filter-mapping>

<form action="${basePath}/sayHello" method="post">
请输入您的名字：
<input type="text" name="userName" />
<input type="submit" value="确定"/>
</form>


<!-- 配置权限管理器 -->  
    <bean id="securityManager" class="org.apache.shiro.web.mgt.DefaultWebSecurityManager">    
        <!-- ref对应我们写的realm  MyShiro -->  
        <property name="realm" ref="myShiro"/>    
        <!-- 使用下面配置的缓存管理器 -->  
        <property name="cacheManager" ref="cacheManager"/>    
    </bean>  
      
    <!-- 配置shiro的过滤器工厂类，id- shiroFilter要和我们在web.xml中配置的过滤器一致 -->  
    <bean id="shiroFilter" class="org.apache.shiro.spring.web.ShiroFilterFactoryBean">   
        <!-- 调用我们配置的权限管理器 -->   
        <property name="securityManager" ref="securityManager"/>   
        <!-- 配置我们的登录请求地址 -->   
        <property name="loginUrl" value="/login"/>    
        <!-- 配置我们在登录页登录成功后的跳转地址，如果你访问的是非/login地址，则跳到您访问的地址 -->  
        <property name="successUrl" value="/user"/>    
        <!-- 如果您请求的资源不再您的权限范围，则跳转到/403请求地址 -->  
        <property name="unauthorizedUrl" value="/403"/>    
        <!-- 权限配置 -->  
        <property name="filterChainDefinitions">    
            <value>    
                <!-- anon表示此地址不需要任何权限即可访问 -->  
                /static/**=anon  
                <!-- perms[user:query]表示访问此连接需要权限为user:query的用户 -->  
                /user=perms[user:query]  
                <!-- roles[manager]表示访问此连接需要用户的角色为manager -->  
                /user/add=roles[manager]  
                /user/del/**=roles[admin]  
                /user/edit/**=roles[manager]  
                <!--所有的请求(除去配置的静态资源请求或请求地址为anon的请求)都要通过登录验证,如果未登录则跳到/login-->    
                /** = authc  
            </value>    
        </property>    
    </bean>  
      
      
    <bean id="cacheManager" class="org.apache.shiro.cache.MemoryConstrainedCacheManager" />    
    <bean id="lifecycleBeanPostProcessor" class="org.apache.shiro.spring.LifecycleBeanPostProcessor" />   
	

$(".list_").append("<div class='data_list' style='text-align:center'>没有找到相关数据!</div>");

<div style="width:100%;padding-left:40%;padding-top:10px;">
	<button class="goback" onclick="history.go(-1)" style="padding:6px 30px;background:#2E89EF;border-radius:5px;color:#fff;">返回</button>
</div>

.goback {
	border: 1px solid rgba(0, 0, 0, 0.3);
	border-radius: 5px;
	height: 30px;
	margin-top: 2.5px;
	width: 150px;
	cursor:pointer;
}




<c:if test="${fn:length(command.responseData.data) == 0}">
	<tr >
		<th colspan="6">没有找到相关数据!</th>
	</tr>
</c:if>

/*Enter键提交-----start*/
function isBrowserIE(){
	var result = false;
	var browser = navigator.appName;
	if(browser == "Microsoft Internet Explorer"){
		result = true;
	}
	return result;
}

function keyDown(e){
	var keycode = 0;
	if(isBrowserIE()){
		keycode = event.keyCode;
	}else{
		keycode = e.which;
	}
	if(keycode == 13 ){
		submitLoginForm();
	}
}

document.onkeydown = keyDown;  //此为原生版的事件绑定可能【不稳定

$(document).ready(function(){  //此为jQuery版的事件绑定【稳定
  $(document).keydown(function(e){ 
	  keyDown(e);
  });
});
/*Enter键提交-----end*/



ini配置文件各配置集含义
[main]
authc.loginUrl=/login	#定义用户认证失败时的跳转页面#
roles.unauthorizedUrl=/unauthorized	 	#定义角色认证失败时的跳转页面#
perms.unauthorizedUrl=/unauthorized		#定义权限认证失败时的跳转页面#

[users]
zhang=123,admin		#定义用户=密码,角色#
wang=123		#定义用户=密码#

[roles]
admin=user:*,menu:*		#定义角色=权限,权限#

[urls]
/login=authc	#定义某url=访问控制属性(需要何种认证)#
/unauthorized=anon
/static/**=anon
/authenticated=authc
/role=authc,roles[admin]
/permission=authc,perms["user:create"]

SELECT
	*,
	pi.PARK_NAME AS PARK_NAME
FROM
	LJ_FACILITATOR_USER fu
	INNER JOIN LJ_COMPANY_USER cu ON pi.ID = cu.COMPANY_PARK_ID


if(StringUtils.isNotEmpty(disOrEnable) && "1".equals(disOrEnable)) {
			User user = userManager.selectUserByForgeignId(id);
			if("-1".equals(personalState)){
				user.setIsEnabled("0");
			} else if("0".equals(personalState)){
				user.setIsEnabled("1");
			}
			userManager.updateUser(user);
		}
		
<%@ taglib uri="http://java.sun.com/jsp/jstl/functions" prefix="fn"%>

<c:if test="${(command.responseData.data) == null || fn:length(command.responseData.data) == 0}">
	<tr>
		<td colspan="4">没有找到相关数据!</td>
	</tr>
</c:if>

<c:if test="${(command.responseData.data) != null && fn:length(command.responseData.data) > 0}">
	<%@include file="/public/common/pagination.jsp"%>
</c:if>

$(".list_").append("<tr><td colspan="4">没有找到相关数据!</td></tr>");
$("#pager").hide();

'<tr height="40px"><td colspan="5" style="font-size: 15px;">没有找到相关数据!</td></tr>'

$(".list_ #noDataTr").remove();
id="noDataTr"

document.getElementById("put1").innerHTML="<span name='v' class='fkyz_ing'>用户名称不能为空！</span>";

$.ajax({
	url : "${ctx}/demo/sayHi1.do",
	type : 'post',
	async : true,
	data : {'id' : 1},
	dataType : 'json',
	success : function(data){
		for(var it in data){
			alert(data[it].name);
		}
	},
	error : function(XMLHttpRequest, textStatus, errorThrown){
		alert(XMLHttpRequest.status);
		alert(XMLHttpRequest.readyState);
		alert(textStatus);
	}
});

[{"id":1,"name":"杨建雄","password":"123456"},{"id":2,"name":"罗敏","password":"111111"},{"id":3,"name":"小罗","password":"222222"}]======

$("#projectList").change(function(){
	var cityName = $("#projectList").find("option:selected").attr("city_name");
	$("#projectList1 option").each(function () {
		//var value = $(this).attr("city_name");
		if($(this).attr("city_name") == cityName){
			$(this).attr("selected", true);
		}
	});
});


********************Linux运行日志**************************
linux日志在/var/log目录下，但内容各有不同
1./var/log/syslog：它和/etc/log/messages日志文件不同，它只记录警告信息，常常是系统出问题的信息。
2./var/log/messages：包括整体系统信息，其中也包含系统启动期间的日志。此外，还包括mail，cron，daemon，kern和auth等内容。
3./var/log/user.log：记录所有等级用户信息的日志。
4./var/log/auth.log：包含系统授权信息，包括用户登录和使用的权限机制等。
5./var/log/daemon.log：包含各种系统后台守护进程日志信息。
6./var/log/kern.log：包含内核产生的日志，有助于在定制内核时解决问题。

ls -lh  //列出当前目录下所有文件（包括子目录，文件体积带【单位K】）。
find -type f -size +100M  //查找当前目录下大与100M的文件。
find . -name "localtime*"  //查找当前目录下，文件名以localtime开头的。
********************Linux常用命令**************************
useradd youn  //创建名为"youn"的用户
passwd youn  //为用户"youn"设置密码（如果不标明用户名，默认修改的是root的密码）

su userName  //表示切换到指定用户，如果userName为空则表示切换到root，exit表示退出到原用户。

df -h  //查看系统磁盘使用情况

vi /etc/sysconfig/network-scripts/ifcfg-eno16777736  //DHCP获取ip，只修改ONBOOT=yes，此选项为yes表示网络服务启动时自动获取ip。
service network restart  //重启网络服务，使配置生效
ip add  //查看系统当前ip

文件、目录命名规则（大小写敏感）：
1.除了/（代表根分区），其他所有字符都可以用来为文件、目录命名（比windows更灵活）。
2.尽量不要使用空格、退格、tab、@、#、&、(、)、-等，以避免歧义。
3.文件名不必遵从windows下的8.3规则，linux下的文件名可以很长，可以没有后缀名。
4.避免使用.符号作为普通文件名的第一个字符（以.号开头的文件是隐藏文件）。

Linux压缩软件打的压缩包Windows都可以解压，反过来就不一定了，所以尽量使用【唯一】通用的.zip。
1)gzip my.txt //只能压缩文件，且不保留源文件，生成my.txt.gz压缩包。
	gzip -d my.txt.gz //解压
	gunzip my.txt.gz //解压

2)zip //保留源文件，可以压缩目录
	-r //压缩目录选项
	zip my.txt.zip my.txt //压缩文件
	zip -r myDir.zip myDir //压缩目录
	unzip my.txt.zip //解压缩
	
3)bzip2 //是gzip的升级版，压缩比高，适合大文件
	-k //可选，保留源文件
	bzip2 -k my.txt //生成my.txt.bz2
	bunzip2 my.txt.bz2 //解压
	
4)tar //将目录打包成单个文件，以便压缩，myDir.tar
	opt：-c //必选，表示创建一个tar文件
		-C //指定解压后目的路径
		-x //必选，【解tar包为目录】
		-v //可选，表示显示打tar包的过程
		-z //可选，打tar包的同时打一个.gz压缩包，最终生成.tar.gz，或者解压。
		-f //指定打包后的文件名
	tar -cvf newDir.tar  //仅仅解tar包
	tar -zcvf newDir.tar.gz oldDir //指定的压缩包名在前（可以不带后缀，但是尽量带后缀），源目录在后，
	tar -zxvf newDir.tar.gz -C myDir //【解压并同时解包】
	zcat my.gzip > my01.txt //将压缩包里的文件内容重定向到目标文档中
	tar -zxvf scala-2.12.0.tgz  //解压.tgz文件到当前文件夹
5)nodejs.tar.xz文件的解压
	先xz -d nodejs.tar.xz  //解压成tar文件
	再解包这个tar文件
	
write //给登录到同一系统的其他用户发送消息，只有在线发送
	write otherUser //发起会话，然后输入消息，Enter发送，Ctrl + d结束会话
	wall //以广播方式发送消息或文件给所有人

ping //向指定主机发送icmp数据包测试连通性(丢包率等)
	-c //设置ping包的次数
	-s //设置ping包的大小

vi命令：
	1、命令模式下输入【/字符串】，例如【/Section】。
	2、如果查找下一个，按【n】即可。

ssh-keygen -t rsa -C "ssh_key_name" //生成密钥对，-C及后面参数指密钥备注,注意：需要连续三次Enter
scp ~/.ssh/id_rsa.pub  root@192.168.17.113:/root/ //【失效了】将公钥用远程root身份(需要输密码)，以ssh方式复制给远程主机的指定目录
ssh-copy-id -i .ssh/id_rsa.pub youn@slave02 //有效，i选项表示要拷的文件名【ssh-copy-id -i id_rsa.pub 10.0.40.65】root用户也可以
ssh youn@slave01 //【在主机免密码访问从机，验证是否配置成功】
scp my.txt youn@slave01:/home/youn //将文件安全复制到远程机指定目录下(考虑权限)

echo hello >> my.txt //将显示内容hello重定向到my.txt文件

firewall-cmd --zone=public --add-port=8080/tcp --permanent //开放指定端口
firewall-cmd --query-port=8080/tcp //查询指定端口是否开放

ip addr show //
service network restart //

yum -y update  //升级所有包同时也升级软件和系统内核
yum -y upgrade  //只升级所有包，不升级软件和系统内核

yum list installed | grep tomcat
yum list | grep tomcat
yum -vy install tomcat.x86_64

yum -vy install lrzsz //安装lrzsz插件，xshell下文件直接上传、下载到本地
sz  //上传（针对当前linux）
rz  //下载（针对当前linux）
./a.sh //当前工作目录执行shell脚本，执行权限必需
sh hello.sh //当前工作目录执行shell脚本，执行权限非必需

mkdir -p parent/son //p选项表示同时创建父目录和子目录
rm fileName.war
rm -fr dirName

which top //显示命令的路径及别名
whereis top //显示命令路径及使用手册
whatis top //显示命令的简要用途
man top //显示命令对应的使用手册，最详细，有范例
man service.cfg //显示标准配置文件使用手册
top --help //显示命令的选项及参数

echo @LANG //显示系统当前语言
locale //显示系统安装的语言列表

find my.txt //在文件系统中查找文件或目录，磁盘扫描，低效但实时性强
locate my.txt //在文件db中查找文件或目录，数据库查询，高效，实时性略差
grep tcp my.txt //在文件中查找内容，起到筛选行的作用

selinux是增强linux系统的内核级安全组件：
setenforce 0/1 //临时关闭/开启selinux
getenforce //查看selinux是否开启(permissive: 关闭，enforcing:开启)
修改/etc/selinux/config 下的 SELINUX=disabled 重启后永久生效

ps -ef | grep tomcat  //查看tomcat应用的运行状态，若只返回1个进程号，表示该服务未运行
ps -x | grep tomcat  //查看当前用户下tomcat应用的运行状态，若只返回1个进程号，表示该服务未运行

df -hl  //查看全局磁盘空间
du -bs dir_name  //查看指定目录空间

netstat -lntp | grep 22
/********************************************/
Centos7 静态ip配置
1.本机cmd》ipconfig查看ipv4地址、子网掩码、默认网关。
	10.0.13.33
	255.255.252.0
	10.0.12.1
2.WMwear中编辑虚拟机实体的网络连接方式为“桥接”（不勾选“复制物理网络连接状态”）【可以完成配置文件编辑后再修改】。
3.WMwear中编辑虚拟机实体，选择高级，查看虚拟机实体MAC地址。
	00:0C:29:41:3A:6B
4.编辑网络配置文件，添加下列参数，vi /etc/sysconfig/network-scripts/ifcfg-eno16777736
	BOOTPROTO=static  #ip获取协议为静态获取
	ONBOOT=yes  #开机自动加载ip配置
	HWADDR=00:0C:29:41:3A:6B  #虚拟机实体的MAC地址，来自第二步
	
	PADDR=10.0.13.44  #配置ip，在第三步已经设置ip处于192.168.10.xxx这个范围，我就随便设为150了，只要不和网关相同均可
	NETMASK=255.255.255.0  #第三步子网掩码
	GATEWAY=10.0.12.1  #这里的网关地址就是第三步获取到的那个网关地址
5.systemctl restart network  #由于在系统操作时，用的是NAT连接模式，所以需要关机》桥接模式》启动》查看系统ip
/********************************************/
1.定位慢查询语句：
	【1】日志法：
	打开 my.ini,找到"[mysqld]"在其下面添加
　　long_query_time = 2 //设置慢查询阈值（单位：秒）
　　log-slow-queries = D:/mysql/logs/slow.log //设置日志输出路径
	
	【2】profile法（session级别）：
	SHOW variables LIKE '%profil%'; //查看数据库的分析功能配置情况
	SET profiling = 1; //打开profile功能（打开此功能，查询语句的profile就被存储在一张表里，可以查看）
	SHOW PROFILES; //查看所有（默认最新15条）历史分析记录的时间开销
	SHOW PROFILE; //查看上一个查询的时间开销明细
	SHOW PROFILE FOR QUERY 429(语句id); //查看指定id查询语句的时间开销明细
	SHOW PROFILE cpu/memory/block io FOR QUERY 429(语句id); //查看指定id查询语句的cpu或内存或io开销（用","替代"/",可以同时查看几种的开销）
	
	SHOW INDEX FROM t_model;  //查看某张表上的所有索引
	SHOW PLUGINS;  //查看当前数据库支持哪些插件
	当子查询速度慢时，可用JOIN来改写一下该查询来进行优化。
	
2.用EXPLAIN命令分析查询语句，内部执行查询的步骤、具体参数和索引的使用情况，如果有不足，可以添加索引。
	如：EXPLAIN SELECT * FROM t_person_info t WHERE t.pk_ID > 100 AND t.FK_UNIT_ID > 100;
	
	【如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定length】
	CREATE INDEX index_name ON my_table(user_name(length)); //普通索引
	CREATE UNIQUE INDEX index_name ON my_table(user_name(length)); //唯一索引
	DROP INDEX index_name ON my_table; //删除一个索引
	
	<fmt:formatDate value="${entity.birthday}" pattern="yyyy-MM-dd"/> //jstl fmt标签
	YANG@)08liwan
	
MySql关系型数据库：
基本语句
预定义函数
视图、索引
函数
存储过程


MySql视图是从从一个或多个基表的全部或部分查询字段的虚拟包装，可以通过视图间接对基表进行增删改查操作。有如下好处：
1、安全性：一般是这样做的:创建一个视图，定义好该视图所操作的数据，
	之后将用户权限与视图绑定，这样的方式是使用到了一个特性：grant语句可以针对视图进行授予权限。
2、查询性能提高。
3、有灵活性的功能需求后，需要改动表的结构而导致工作量比较大，那么可以使用虚拟表的形式达到少修改的效果，
	这是在实际开发中比较有用的。
4、复杂的查询需求，可以进行问题分解，然后将创建多个视图获取数据。将视图联合起来就能得到需要的结果了。


摩尔定律对于单核cpu已经失效，所以催生了多核cpu，摩尔定律还适用！
并行计算要在多核cpu上才能体现出优势，主要用在图形处理和服务端编程！

同步方法调用：需等待被调用方法执行完成并返回，后续代码才能继续执行，后续代码的等待时间太长。
异步方法调用：无需等待被调用方法执行完成，很快返回，立即执行后续代码；此时先前被调用方法还未执行完成，会另启一个线程，在别处默默执行。

并发：两个进程或线程同时执行，前提必须是多核cpu。
并行：时分多路复用，为不同进程或线程分配时间片（针对单个核心）。

临界区：公共资源或数据，可能被多个线程访问或操作；如果每次只允许单个线程操作，
		一旦被某个线程占用，其他线程需等待。
		
阻塞：线程在操作系统层面被挂起，导致性能不高，但是简单。
非阻塞：允许多个线程同时进入临界区。

死锁：线程之间相互阻塞，导致都不能执行，但它是一个静态的问题，发生死锁的多个线程会释放资源不占用cpu。
活锁：类似同侧避让，持有资源，但它是一个动态的问题更难排查，发生死锁的多个线程不会释放资源并占用cpu。

饥饿：优先级低的线程一直不能获得资源，低优先级线程会被饿死。

并发级别：
阻塞式（悲观）：一个线程进入临界区后，其他线程必须等待。
无障碍式（乐观）：非阻塞，线程可以自由进入临界区，无数据竞争时，有竞争时回滚数据。
无锁式：前提是无障碍，保证每次竞争至少一个线程能胜出走出临界区。
无等待：无锁式，保证每个线程都能顺利离开临界区，不会产生饥饿。

并行的两个定律：
阿姆达尔定律（指导cpu个数和串行化适当比例）：定义了串行系统并行化后的加速比值得计算公式和理论上限；加速比 = 加速前的时耗 / 加速后的时耗
古斯塔森定律：如果并行化足够大，性能提升和cpu个数接近线性关系。

/********************************************/
/*Enter键提交-----start*/
function CheckBrowserIsIE(){
	var result = false;
	var browser = navigator.appName;
	if(browser == "Microsoft Internet Explorer"){
		result = true;
	}
	return result;
}

function keyDown(e){
	var keycode = 0;
	if(CheckBrowserIsIE()){ //兼容ie浏览器
		keycode = event.keyCode;
	}else{
		keycode = e.which;
	}
	if (keycode == 13 ){ //判断如果是Enter键
		submitLoginForm();
	}
}

document.onkeydown = keyDown; //注册键盘事件
/*Enter键提交-----end*/
/********************************************/****
js定时调用函数（周期性/非周期性）
<script type="text/javascript">
	function showTime(){
		$("#date_span").html(moment(new Date()).format("YYYY-MM-DD hh:mm:ss"));
	}
	$().ready(function(){
		setInterval("showTime()", 10); //根据所设周期1000毫秒，反复执行回调函数
		setTimeout("showTime()", 10); //根据所设周期1000毫秒，页面加载后10毫秒，执行回调函数
	});
</script>

public class signTon{
	private static final SignTon signTon;
	private SignTon(){};
	
	public synchronize SignTon getInstance(){
		if(signTon == null){
			signTon = new SingTon();
		}
		return signTon;
	}
}
/********************************************/**********
1、新建一个JavaWeb项目
2、建前端页面
title：<=31字
keyword：用逗号分隔<=76字
description：<=80字

要想从你网页图片中获取流量，需要添加img标签的alt属性，供搜索引擎抓取；
img标签的尺寸，最好写在行内，因为浏览器先加载dom元素，后加载css文件，可以避免加载
css样式前，浏览器计算原始图片尺寸的性能消耗（更快）。

3、后台业务逻辑
4、打通前后台
5、测试（公测：目标用户参与测试，内测：开发团队内部测试）
6、试运行（修改bug）
7、上线（正式运营，根据用户和市场反馈不断迭代，改版）
8、维护
/********************************************/*******************
大数据中心ftp共享目录：\\192.168.122.6\software
用户名：software
密码：123456?

项目框架：http://192.168.121.44:8080/troy-portal/troy_demo_frame!initIndex.action

<>是标准SQL语法.可以移植到其他任何平台.
!=是非标准SQL语法.可移植性差.
但都是表示不等于的意思.
应用中【尽量用<>】

Oracle变量绑定：是指在sql语句的条件中使用变量而不是常量。
比如shared pool里有如下两条sql语句：
	select * from tab1 where col1=1;
	select * from tab1 where col1=2;
对oracle数据库来说，这是两条完全不同的SQL，对这两条语句都需要进行hard parse。
因为oracle会根据sql语句的文本去计算每个字符在内存里的hash值，因此虽然上述两条SQL只有一个字符不一样，
oracle根据hash算法在内存中得到的hash地址就不一样，所以oracle就会认为这是两条完全不同的语句。
而如果将上述SQL改写成select * from tab1 where col1=:var1;,然后通过对变量var1的赋值去查询，
那么oracle对这条语句第一次会进行hard parse，以后就只进行soft parse。
假设某条语句被重复执行了几十万次，那么使用bind var带来的好处是巨大的。
一个应用程序如果bind var使用不充分，那么几乎一定会伴随着严重的性能问题。
/********************************************/
绑定变量是相对文本变量来讲的,所谓文本变量是指在SQL直接书写查询条件，这样的SQL在不同条件下需要反复解析,
绑定变量是指使用变量来代替直接书写条件，查询bind value在运行时传递，然后绑定执行。
优点是减少硬解析,降低CPU的争用,节省shared_pool ;缺点是不能使用histogram,sql优化比较困难
/********************************************/

sqoop：关系型数据库和列式数据库间的数据转换工具。
flume：日志数据提取工具。
yarn：通用的计算资源管理和调度工具。
solr：一个基于lucene的全文搜索服务工具。
oozie：工作流管理引擎，合并、串联多个算法单元为一个实用处理流程。
storm：实时流数据处理框架。
spark：内存计算框架。
hue：做大数据可视化。
impala：做大数据查询，比hive快。
pentaho：商业智能套件。

****************************************
HDFS(M/S架构){
	NameNode{
		1)管理hdfs集群中的fs的nameSpace，如某个nameSpace的开启-关闭、重命名等。
		2)管理fileBlock和nameSpace的映射关系，保证对fileBlock有效访问。
		3)管理dataNode和fileBlock的状态，随时监听其健康状态，在某个单元出现问题后提供及时的备份映射。
		4)主备部署，可以解决单点故障。
		5)主备组部署，解决内存受限问题，每个NN管理部分存储nameSpace。
	}
	DataNode：{
		1)负责对本节点上的数据的管理和访问，作为数据结点的服务进程与文件系统客户端打交道。
		2)向Namenode结点报告状态，每个Datanode结点会周期性地向Namenode发送心跳信号和文件块状态报告。
		3)执行数据的流水线复制，当文件系统客户端从Namenode服务器进程获取到要进行复制的数据块列表后，完成文件块及其块副本的流水线复制。
	}
}

MapReduce:
1)MapReduce是一种编程模型，用于大规模数据集的并行运算框架。
	借助MapRduce框架能够使不会分布式并行编程的程序员将自己的程序运行在分布式系统上。
	JobTracker
	TaskTracker

HBASE(M/S架构分布式列式数据库){
	HMaster-->master：负责全新的安装，集群的启动与关闭为Region server分配region
						负责region server的负载均衡发现失效的region server并重新分配其上的region
						恢复region entao

	Region Server-->slave：Region server维护Master分配给它的region，处理对这些region的IO请求
							Region server负责切分在运行过程中变得过大的region。

}

Hive：
	可以被认为是一种数据仓库，包括数据的存储以及查询。可以将结构化的数据文件映射为一张数据库表，
	并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。可以被认为是一种数据仓库，
	包括数据的存储以及查询。可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，
	可以将sql语句转换为MapReduce任务进行运行。 Hive建立在Hadoop的其它组成部分之上，
	依赖于HDFS进行数据保存，依赖于MapReduce完成查询操作。


YARN(M/S架构)专管整个集群的资源管理和调度，可以整合管理Hadoop、Spark、Storm集群
1)ResourceManager-->Master：主要负责整个集群的资源管理和调度
2)NodeManager-->Slave：主要负责单个节点的资源管理和调度

APPM(负责单个应用程序的管理)
****************************************

raid5：磁盘阵列技术，访问速度和空间利用率在raid0和raid1之间。

equals方法：
对象的默认equals方法（继承自Object类）底层使用【双等号】对变量的引用进行比较，
只有当引用指向同一地址才返回true。一般情况下，我们在重写类的equals方法时也需要重写其hashCode方法，
以保证【相等（equals方法）的对象必须具有相同的hashCode】，反之亦然。
1.反身性：一个对象一定equals于他自身，obj.equals(obj)恒等于true。
2.对称性：如果A对象equals对象B，对象B也一定equals对象A，反之亦然。
3.传递性：如果A对象equals对象B，B对象equals对象C，那么A对象一定equals对象C，反之亦然。
4.一致性：在不改变两个对象的前提下，经过N此equals比较的结果都是相同的。
5.任何非空对象与null的equals结果都是false。

hashCode方法：
1.在同一次java应用的执行过程中，多次调用一个对象的hashCode方法的返回值是相同的（调用前后对象未发生改变）。
2.如果两个对象equals比较返回true，则这两个对象必须具有相同的hashCode返回值。
3.如果两个对象equals比较返回false，则这两个对象可以具有相同的hashCode返回值，具有不同的hashCode时性能更高。
4.对Object类来说，不同的Object对象的hashCode值是不同的，因为Object类的hashCode的值是对象的地址的十六进制表示。
5.继承自Object类的hashCode方法的返回值和Object类的方式相同。

toString方法：
1.继承自Object类的toString方法返回的是【类名@hashCode】

向hashSet中添加对象时，对象的hashCode方法会被调用，将对象的hash code值作为参数，调用，
判断集合中已有对象是否和新来的对象具有相同的hash code，
如果不同则直接添加，如果相同：调用equals方法比较两个对象，返回false则添加到链上，如果返回true则放弃添加。

1.调用key对象的hashCode()方法，得到key的hash_code。
2.调用一个系统哈希函数hash(hash_code)，计算出key的key对应的hash变量。
3.调用indexFor(hash, arrayLength)数组中准备添加entry的位置i。
4.如果位置i上已经存在entry对象e，遍历e对应的链，取出链上entry元素的hash属性(存放时就根据系统hash
	函数计算出来并作为final值存储了的)，与新key计算得到的hash变量比较，如果hash相同，且，
	新key与遍历元素key双等或equals（表示重复了），就放弃添加整个过程return 重复的旧value，
	否则遍历下一个链元素，如果整个遍历过程中没有遇到上述重复，用后来居上的方式，将新entry放道链首。

Entry类有四个成员变量:{
	final K key;
	V value;
	Entry<K, V> next;
	final int hash；
}

MySql中：
SELECT NOW() //返回当前时间
SELECT LENGTH("中国"); //字节长度
SELECT LENGTH("zg");
SELECT CHARACTER_LENGTH("中国"); //字符长度
SELECT CHAR_LENGTH("中国"); //字符长度


数据采集-->数据准备(ETL)-->数分析

以创建一个虚拟机(server)为例，结合下图简述下keystone在openstack的访问流程：
	1)用户Alice通过自己的户名和密码向keystone申请【临时令牌】，keystone认证用户名和密码后，返回token0。
	2)Alice通过token0向keystone查询他所拥有的租户列表，keystone验证token0成功后，返回Alice有权限的所有Tenant列表。
	3)Alice选择某个租户，通过用户名、密码和指定租户申请【正式令牌】，keystone验证用户名、密码和tenant后，
		返回正式token1。（其实1、2步仅仅是为了查询tenant，如果已经知道tenant，可以忽略1、2步直接获取正式令牌）。
	4)Alice通过token1向keystone发送创建server的请求，keystone验证token1(包括该令牌是否有效，
		是否有权限创建虚拟机等)成功后，再把用户请求下发到下游的nova，然后创建虚拟机。
		
CDH完全分布式安装
CDH是Cloudera公司对apache原生Hadoop封装后的商业发行版，
生态圈组件集成比原生的更方便(原生态的很麻烦)，也提供技术支持。
1.前期环境准备
	1)一台虚拟机，
	2)两个软件包(jdk、CDH版Hadoop)，
	3)设置静态IP，
	4)在hosts文件注册集群主机及IP{
		vi /etc/hosts文件
		追加如下配置：
		192.168.88.128 vm0
		192.168.88.129 vm1
		192.168.88.130 vm2
		重启
	}
	5)关闭防火墙：{
		systemctl stop firewalld //关闭目前运行的防火墙
		systemctl disable firewalld //禁用防火墙
		setenforce 0 //关闭selinux，(selinux可以保证集群内节点的隔离性)
		修改/etc/selinux/config 下的 SELINUX=disabled 重启后永久生效
	}
	6)关闭selinks，
	7)生成ssh密钥对。
	
2.安装JDK
	export JAVA_HOME=~/bigData/jdk1.8
	export JRE_HOME=~/bigData/jdk1.8/jre
	export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib
	export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
	source .bashrc //刷新系统环境变量配置项，否则不会生效
3.Hadoop主节点一台
	1)解压Hadoop包
	2)新建dfs/name和dfs/data及temp目录
	3)进入etc/hadoop目录修改如下六个配置文件{
		【slaves】文件，注释掉localhost，直接将两个从节点主机名(slave01、slave02)写上。
		【hadoop-env.sh】文件，追加export JAVA_HOME=/home/youn/bigData/jdk环境变量。
		【core-site.xml】文件，配置HDFS的NameNode地址和临时文件存放路径
			<configuration>
				<!-- 指定HDFS的NameNode通信地址 -->
				<property>
					<name>fs.defaultFS</name>
					<value>hdfs://master01:9000</value>
				</property>
				<!-- 指定指定Hadoop运行时产生的临时文件存放路径 -->
				<property>
					<name>hadoop.tmp.dir</name>
					<value>/home/youn/bigData/hadoop2.6-cdh5.8/temp</value>
				</property>
			</configuration>
		【hdfs-site.xml】文件，
			<configuration>
				<!-- 指定HDFS的NameNode目录 -->
				<property>
					<name>fs.namenode.name.dir</name>
					<value>/home/youn/bigData/hadoop/dfs/name</value>
				</property>
				<!-- 指定HDFS的DataNode目录 -->
				<property>
					<name>dfs.datanode.dir</name>
					<value>/home/youn/bigData/hadoop/dfs/data</value>
				</property>
				<!-- 设置hdfs副本数量 -->
				<property>
					<name>dfs.replication</name>
					<value>1</value>
				</property>
			</configuration>
		【mapred-site.xml】文件（从同名模板文件中copy而来）
			<configuration>
				<!-- 通知框架MR使用YARN资源管理器 -->
				<property>
					<name>mapreduce.framework.name</name>
					<value>yarn</value>
				</property>
			</configuration>
		【yarn-site.xml】文件
			<configuration>
				<!-- 配置reducer的数据获取方式为mapreduce_shuffle -->
				<property>
					<name>yarn.nodemanager.aux-services</name>
					<value>mapreduce_shuffle</value>
				</property>
				<!-- 配置资源管理器主机名 -->
				<property>
					<name>yarn.resourcemanager.hostname</name>
					<value>master01</value>
				</property>
			</configuration>
	}
4.Hadoop从节点两台
	需要与master相同的jdk和hadoop配置，最好克隆，然后修改静态IP。
	测试好主节点自身免密钥、主节点到从节点的面密码访问。
	
5.启动Hadoop分布式集群（虚拟备用主节点，如果启动过程中涉及输入系统密码，统一用youn的）
	./bin/hdfs namenode -format //首次启动需要在主节点格式化nameNode，另一种shell命令写法(sh hdfs namenode -format)
	./sbin/start-dfs.sh //启动hadoop
	主节点jps //查看hadoop进程是否启动，（查看跟java有关的进程，需配置JAVA_HOME
		namenode //HDFS主节点进程
		secondarynamenode //HDFS备份主节点进程
		resourcemanager //yarn主节点进程
	
	从节点jps
		datanode //HDFS从节点进程
		nodemanager //yarn从节点进程
	
	登录主节点http://192.168.248.128:8088访问hadoop集群管理主界面
	登录主节点http://192.168.248.128:50070访问hadoop的HDFS管理界面
	./bin/hadoop fs -mkdir /myDir //在HDFS根目录下新建个人目录
	./bin/hadoop fs -put helloWorld.txt /myDir //将文件放到HDFS目录下
	
	从节点文件系统测试！
	no route to host!
	
	运行自带WordCount装机示例：{//到jar包所在目录，D:\bigData\hadoop2.6-cdh5.8\share\hadoop\mapreduce1
		hadoop jar hadoop-mapreduce-examples-2.2.0.jar wordcount /test/words.txt /test/result
		hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.8.0.jar wordcount /test/pglicense.txt /test/pglicensecount
	}
************************************
静态IP虚拟机克隆后只需修改衍生机的ip地址，其他不需要改。
************************************
MapReduce
逻辑过程：
物理过程：
************************************
HBase：
1.HBase:是一个开源分布式列式数据库。
2.BigTable:任何二维关系表都可以拆分成如{pk_id, key, value}的列式表。
3.hbase会不断碎片整理，会丢弃过时和被逻辑删除的数据，由于记录具有时间戳，所以特别适合按时间查询。
4.行键，rowKey，可以重复，表示不同版本，通过时间戳区分。
5.hbase里没法查询非行键字段，若要查询需要特别设计。
6.一个表有多少列族需要预先定好，但是某个列族中具体有哪些列不需要定义。
7.修改和删除某个行都是通过insert语句插入新版本来实现的。
8.hbase支持两种数据回收机制，可以指定版本数阈值超过了多少条就被丢弃，也可以设定过期时间。
9.行键->列族->限定符来唯一定位一个元素。
10.所有元素的列值都是字节码这种二进制形式存储的，没有数据类型概念，需要取出在应用中转换。
11.一个cell是rowKey相同的一组数据。
************************************
Mybatis打印SQL
<configuration>
	<settings>
        <!-- 配置SQL打印格式 -->
        <setting name="logImpl" value="STDOUT_LOGGING" />
    </settings>
</configuration>

<!-- 批量删除学生城市关系表，无嵌套foreach -->
<delete id="deleteStudentAndCityById" parameterType="java.util.HashMap">
	DELETE t
	FROM
		t_student_and_city AS t
	WHERE
		t.fk_teacher_id = #{teacherId}
	AND t.fk_city_id IN
	<foreach collection="cityIdList" index="index" item="it" open="(" separator="," close=")">
		#{it}
	</foreach>
</delete>

<!-- 批量添加学生城市关系表，嵌套foreach，一对多循环-->
<insert id="addStudentAndCity" parameterType="java.util.HashMap">
	INSERT INTO t_student_and_city (
		fk_city_id,
		fk_student_id,
		fk_teacher_id
	)
	VALUES
	<foreach collection="cityIdList" index="index" item="city" open="" separator="," close="">
		<foreach collection="studentIdList" index="index" item="student" open="" separator="," close="">
			(${city}, ${student}, ${teacherId})
		</foreach>
	</foreach>
</insert>

****************jqgrid********************
拼接函数调用
return "<a href='javascript:void(0);' onclick='deleteRecord(\"#table_list_1\", " + record.id + ");'>删除</a>";

var selectedId = $("#table_list_1").jqGrid("getGridParam", "selrow");  //获取单个选中行id
var selectedIdArr = $("#table_list_1").jqGrid('getGridParam','selarrrow'); //获取所有选中行id
var allIdArr = $("#table_list_1").jqGrid('getDataIDs');  //获取所有行id
var selectRecord = $("#table_list_1").jqGrid('getRowData', myId);  //根据指定行id获取单行记录

************************************
grant all privileges on *.* to root@"%" identified by ".";
************************************

.bash_history
.bash_logout
.bash_profile
.bashrc

***************************************
我们常用的在a标签中有点击事件：
1. a href="javascript:js_method();"
	这是我们平台上常用的方法，但是这种方法在传递this等参数的时候很容易出问题，
	而且javascript:协议作为a的href属性的时候不仅会导致不必要的触发window.onbeforeunload事件，
	在IE里面更会使gif动画图片停止播放。W3C标准不推荐在href里面执行javascript语句
2. a href="javascript:void(0);" onclick="js_method()"
	这种方法是很多网站最常用的方法，也是【最周全】的方法，onclick方法负责执行js函数，
	而void是一个操作符，void(0)返回undefined，地址不发生跳转。
	而且这种方法不会像第一种方法一样直接将js方法暴露在浏览器的状态栏。
3.a href="javascript:;" onclick="js_method()"
	这种方法跟跟2种类似，区别只是执行了一条空的js代码。
4.a href="#" onclick="js_method()"
	这种方法也是网上很常见的代码，#是标签内置的一个方法，代表【top】的作用。
	所以用这种方法点击后网页后返回到页面的最顶端。
5.a href="#" onclick="js_method();return false;"
	这种方法点击执行了js函数后return false，页面不发生跳转，执行后还是在页面的当前位置。
	
我看了下taobao的主页，他们采用的是第2种方法，而alibaba的主页是采用的第1种方法，
和我们的区别是每个href里的javascript方法都用try、catch包围。
综上，在a中调用js函数最适当的方法推荐使用：
a href="javascript:void(0);" onclick="js_method()"
a href="javascript:;" onclick="js_method()"
a href="#" onclick="js_method();return false;"

【注意】：js的方法中，字符String类型参数要加单引号。

get传参时，拼接部分不能带任何空格。

*****************js对象操作***************************
var nodes = {};  //声明空的对象
function zNodeOnCheck(event, treeId, treeNode) {
	var nodeNames = [];
	if(treeNode.checked){
		nodes[treeNode.id] = treeNode.name;  //以Hash方式给对象添加键值对属性
	}else{
		delete nodes[treeNode.id];  //删除对象的指定属性（键值对）
	}
	$.each(nodes, function(key, value){  //遍历对象属性键值对
		nodeNames.push(value);
	});
	$("#org").val(nodeNames.toString());
};
/********************************************/
原生js数组遍历与删除
//var mycars=new Array("Saab","Volvo","BMW")  //初始化
//var arr = new Array(3);  //初始化

var arr = new Array();  //初始化
arr.push(1);
arr.push(2);
arr[2] = 3;

for(var i in arr){
	if(arr[i] == 2){
		arr.splice(i, 1);
	}
}
/********************************************/
var nparms = $('#job_form').serializeArray();  //序列化form表单所有参数为数组形式
/********************************************/
script标签的type和language属性的区别：
两者的作用都一样。
早期浏览器不支持type属性，只能用language="javascript"。
同时写上两者是为了兼容老版本浏览器。
/********************************************/
js中的中文乱码解决方案：
在html页面的script标签中添加charset="gbk"属性。
/********************************************/
在github账号下为项目创建远程仓库
在项目同级目录执行下列命令：
	git remote add origin https://github.com/JaxYoun/FontServer.git	
	git init  //创建本地git仓库
	git status //查看当前仓库托管状态（哪些被托管）
	git add ./demo/  //将工作区中的demo工程给Git托管
	git commit -m "提交注释"  //将工作区提交到本地仓库
	git remote add origin https://github.com/JaxYoun/FontServer.git  //将本地仓库与远程仓库同步
	git push -u origin master  //推送到远程仓库
-------------------------------------------------------------------
echo "# NotepadAndBookmark" >> README.md
git init
git add README.md
git commit -m "first commit"
git remote add origin git@github.com:JaxYoun/NotepadAndBookmark.git
git push -u origin master
-------------------------------------------------------------------
git remote add origin git@github.com:JaxYoun/NotepadAndBookmark.git
git push -u origin master
-------------------------------------------------------------------
git commit -m "yang first commit!"  //将内容提交到本地仓库本地
git push -u origin master //向新仓库首次提交主分支，其中“master”是分支名

D:\myTest>git clone http://10.0.0.171/cloud/cloud.git  //从远程仓库克隆整个项目到本地仓库目录

git branch yjx  //新建非跟踪分支yjx（不与远程分支同步）

D:\myTest\cloud>git branch [branchName] --track origin/alpha  //新建与远程alpha分支对应的本地跟踪分支alpha（会同步）

D:\myTest\cloud>git checkout --track origin/alpha  //新建与远程alpha分支对应的本地跟踪分支alpha（会同步）并切换

git branch	//查看当前操作的分支
git checkout yjx	//切换到yjx分支
git push origin newBranch //将新分支发布到远程仓库

git status	//查看当前分支上的文件状态
git add .	//将修改添加到stage
git commit -m 'xxxxx'	//将修改提交到当前分支对应的本地分支上

git checkout alpha	//切换到本地alpha分支
git branch	
git merge yjx	//将本地yjx分支，合并到本地alpha分支

git status

git fetch origin alpha	//拉取远端alpha分支（FETCH_HEAD）
git merge FETCH_HEAD	//将远端alpha分支合并到本地alpha

//解决冲突

git add .
git commit -m 'yyyyy'

git status
git push origin alpha
/********************************************/
git remote add origin git@github.com:JaxYoun/ActiveMqDemo.git  //将项目添加到远程仓库
/********************************************/
从GitHub克隆项目到本地
1.到GitHub远程仓库打开对应项目，点击"clone or download"按钮，选择通信协议HTTPS或者SSH【一个不能clone，切换试试】
2.复制框中的链接https://github.com/JaxYoun/spring-boot-redis-session-demo.git
3.在本地目录中【不需要新建与项目同名的目录】打开GitBash，输入git clone https://github.com/JaxYoun/spring-boot-redis-session-demo.git
4.等待下载完成
/********************************************/
<%@taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%>
<%@taglib prefix="fn" uri="http://java.sun.com/jsp/jstl/functions" %>
<%
	String path = request.getContextPath();
	String basePath = request.getScheme()+"://"+request.getServerName()+":"+request.getServerPort()+path+"/";
%>
<html>
<head>
<base href="<%=basePath%>">

/********************************************/
	
1、遍历数组(有附加参数)
	$.each(Array, function(p1, p2){
		this;  //这里的this指向每次遍历中Array的当前元素
		p1; p2;  //访问附加参数
	}, ['参数1', '参数2']);
	
2、遍历数组(没有附加参数)
	$.each(Array, function(i, value) {
		this;  //this指向当前元素
		i;  //i表示Array当前下标
		value;  //value表示Array当前元素
	});
	
3、遍历对象(有附加参数)
	$.each(Object, function(p1, p2) {
		this;  //这里的this指向每次遍历中Object的当前属性值
		p1; p2;  //访问附加参数
	}, ['参数1', '参数2']);
	
4、遍历对象(没有附加参数)
	$.each(Object, function(name, value) {
		this;  //this指向当前属性的值
		name;  //name表示Object当前属性的名称
		value;  //value表示Object当前属性的值
	});
	
5、例子：
	var arr = [ "one", "two", "three", "four"]; 
		$.each(arr, function(){ 
		alert(this); 
	}); 
	//上面这个each输出的结果分别为：one,two,three,four 
	 
	var arr1 = [[1, 4, 3], [4, 6, 6], [7, 20, 9]] 
		$.each(arr1, function(i, item){ 
		alert(item[0]); 
	}); 
	//其实arr1为一个二维数组，item相当于取每一个一维数组， 
	//item[0]相对于取每一个一维数组里的第一个值 
	//所以上面这个each输出分别为：1 4 7 
	 
	var obj = { one:1, two:2, three:3, four:4}; 
		$.each(obj, function(key, val) { 
		alert(obj[key]); 
	}); 
	//这个each就有更厉害了，能循环每一个属性 
	//输出结果为：1 2 3 4
/********************************************/
安装完go环境后需要配置系统环境变量：
GOROOT C:\ProgranmeFiles\Go\  //此为go在系统中的的安装路径
GOPATH F:\golang\goDemo  //此为开发go程序的项目主目录goDemo
go env //查看go环境的安装情况
/********************************************/
项目目录结构
src目录：放置源文件如：test.go（其报名为action）
pkg目录：放置包文件如：action.a（编译自text.go文件）
bin目录：
/********************************************/
<c:choose>
	<c:when test="${hasPermission == 'name'}">
		<button>操作</button>
	</c:when>
	<c:otherwise>
		<span style="color:red">当前用户没有操作权限！</span>
	</c:otherwise>
</c:choose>
/********************************************/
DOM元素的固有属性使用prop()方法，自定义属性使用attr()方法。
/********************************************/
function timeOut(selector){
	$(selector).click();
}
$().ready(function(){
	$("myId").click(function(){
		alert($(this).parent().parent().hasClass("lili"));
	});
	var selector = "myId";
	setTimeout("timeOut('"+selector+"')", 4000);
});
/********************************************/
jquery-validate自定义ajax异步提交
$("#addCategorySubmmitBtn").click(function() {  //form中的提交按钮必须submit类型
	$("#add_form").validate({  //validate函数是自定义的，是自定义验证规则的实现体
		submitHandler:function(e){  //通过验证的回调提交函数，其中e代表被验证的【表单元素本身】
			addCategorySubmmit(e);  //要调用自定义提交，必须将e传递到自定义函数
		},
		invalidHandler: function(){  //未通过验证的回调函数
			return false;
		}
	});
});
/********************************************/
<role rolename="admin-gui"/>  //声明超级用户角色
<role rolename="manager-gui"/>  //声明管理员角色
<user username="youn" password="kkk" roles="admin-gui,manager-gui"/>  //声明用户名、密码并为之分配两个角色
/********************************************/
formatter:'date',
formatoptions:{
	srcformat: 'Y-m-d H:i:s',
	newformat: 'Y-m-d H:i:s'
}
/********************************************/
<link rel="stylesheet" href="plugins/BootstrapMenu/css/toastr.css">
<script src="plugins/BootstrapMenu/js/toastr.js"></script>
<script src="plugins/BootstrapMenu/js/toastr-opt.js"></script>
/********************************************/
MySql自定义函数和过程：
1.函数返回值是必选项，用关键字“RETURNS”在函数名后声明，函数体内必须返回某个变量。
2.存储过程的输出值是可选项，如果有，则在参数列表中声明，在过程体内为之赋值即可。
3.调用方式有区别。
/********************************************/
MySql自定义函数写法
delimiter $$  //临时定义“$$”为语句结束符

DROP FUNCTION IF EXISTS f_test_fun;  //先删除库中同名函数
CREATE FUNCTION f_test_fun (in_id INT, in_age INT)
RETURNS INT  -- //*特别注意是【RETURNS】
BEGIN
	DECLARE re INT DEFAULT 0;
	SELECT COUNT(*) INTO re FROM t_user t WHERE t.id >= in_id AND t.age >= in_age;
RETURN re;
END $$  //函数体的结束标志，必须用定义好的“$$”

delimiter ;  //将之前临时定义的语句结束符“$$”，【还原】为系统预定义的“;”符

SELECT f_test_fun(1, 1);  //函数调用
SELECT f_test_fun(1, 1) FROM DUAL;  //同上
/********************************************/
函数的声明定义
delimiter $$
CREATE FUNCTION `getChildList`(rootId varchar(100))   
RETURNS varchar(2000)  
BEGIN   
DECLARE str varchar(2000);  
DECLARE cid varchar(100);   
SET str = '$';   
SET cid = rootId;   
WHILE cid is not null DO   
    SET str = concat(str, ',', cid);   
    SELECT group_concat(id) INTO cid FROM t_yang_test where FIND_IN_SET(parent_id, cid) > 0;   
END WHILE;   
RETURN str;   
END $$
delimiter ;

//生成的函数结果，根据父节点id查询所有其所有子节点（包括父节点本身）
BEGIN
	DECLARE sTemp VARCHAR(1000);
	DECLARE sTempChd VARCHAR(1000);

	SET sTemp = '$';
	SET sTempChd = cast(prentId as CHAR);

	WHILE sTempChd is not null DO
		SET sTemp = concat(sTemp,',',sTempChd);
		SELECT group_concat(id) INTO sTempChd FROM  t_tree_node where FIND_IN_SET(pid, sTempChd) > 0;
	END WHILE;

	RETURN sTemp;
END
/********************************************/
【调用】自定义SQL函数
SELECT
	*
FROM
	t_tree_node t
WHERE
	FIND_IN_SET(
		t.id,
		queryCategoryByAncestorId (2)
	);
/********************************************/
MySQL创建存储过程的时候发现有这么四个数据存取限制的参数，
一些特征提供子程序使用数据的内在信息。
1.CONTAINS SQL：表示子程序不包含读或写数据的语句。
2.NO SQL：表示子程序不包含SQL语句。
3.READS SQL DATA：表示子程序包含读数据的语句，但不包含写数据的语句。
4.MODIFIES SQL DATA：表示子程序包含写数据的语句。

注意：如果这些特征没有明确给定，默认的是CONTAINS SQL，但是“这些特征值目前只提供给服务器，
并没有根据这些特征值来约束过程实际的使用数据情况”，创建存储过程后改变这些个参数，
都不影响执行，也不影响创建。
/********************************************/
delimiter $$  //临时定义“$$”为语句结束符
DROP PROCEDURE IF EXISTS p_procedure_test;  //先删除同名存储过程
CREATE PROCEDURE p_procedure_test (IN in_id INT, OUT out_count INT) READS SQL DATA  //新建存储过程
BEGIN  //过程体开始
	SELECT
		COUNT(*) INTO out_count
	FROM
		t_user t
	WHERE
		t.id >= in_id ; 
END $$  //过程体结束
delimiter ;  //将之前临时定义的语句结束符“$$”，还原为系统预定义的“;”符

CALL P_procedure_test (1, @out_count);  //调用存储过程，@out_count表示将过程的输出作为变量，供后续语句取值
SELECT @out_count;  //此语句用以去输出值对应的变量
SELECT @out_count FROM DUAL;  //同上
/********************************************/
循环生成记录
DELIMITER $$

DROP PROCEDURE IF EXISTS p_generater;
CREATE PROCEDURE p_generater(IN in_count INT)
BEGIN
	DECLARE i INT;
	SELECT IF (ISNULL(MAX(t.id)), 1, MAX(t.id) + 1) INTO i FROM t_user t;
	SET in_count = i + in_count;
	WHILE i < in_count DO
		INSERT INTO t_user (id, name, password, sex, age, birthday) VALUES (i, concat(i), concat(i), concat(i), i, NOW());
		SET i = i + 1;
	END WHILE;
END $$

DELIMITER ;

TRUNCATE t_user;

CALL p_generater(1000);
/********************************************/
hover到元素上鼠标变手型
<style type="text/css">
	.readOnlyTextField {
		cursor: pointer;
	}
</style>
/********************************************/
$('#myform')[0].reset();  //重置form表单(还原而非清空,如select的默认值会得到保留)
/********************************************/
//清空型
$(":input","#myform").not(":button, :submit, :reset, :hidden").val("").removeAttr("checked").removeAttr("selected); 
/********************************************/
var salesChartCanvas = $("#salesChart")[0].getContext("2d");  //jquery获取画布context
/********************************************/
var salesChartCanvas = document.getElementById("salesChart").getContext("2d");  //原生js获取画布context
/********************************************/
HttpClient httpclient = new DefaultHttpClient();
HttpPost httpPost = new HttpPost("http://localhost:8080/OA/OA.html");

StringEntity stringEntity = new StringEntity(bodys,"UTF-8");
httpPost.setEntity(stringEntity);

HttpResponse response = httpclient.execute(httpPost);
HttpEntity httpEntity = response.getEntity();
if (httpEntity != null) {
	InputStream instream = httpEntity.getContent();
	int l;
	byte[] tmp = new byte[2048];
	while ((l = instream.read(tmp)) != -1){}
}
/********************************************/
isEmpty(String str);  //判断某字符串是否为【空】，为空的标准是 str == null或str.length() == 0。
isBlank(String str);  //判断某字符串是否为【空】或由空白符(whitespace)构成，比isEmpty更严格。
/********************************************/
MySql批量插入多条数据有二种方法,一种是写多条insert 语句用";"号分割每条sql【低效】；另一种是insert本身的多个value【高效】；

1.写多条insert用";"分割
这个很简单,;号是mysql执行sql的结束符,写多个insert用;号割就是让mysql执行多次而已.比如:
insert into table (field1,field2,field3) value ('a',"b","c");
insert into table (field1,field2,field3) value ('a',"b","c");
insert into table (field1,field2,field3) value ('a',"b","c");
insert into table (field1,field2,field3) value ('a',"b","c");
insert into table (field1,field2,field3) value ('a',"b","c");

2.在insert中写多个value
INSERT INTO table (field1,field2,field3) VALUES ('a',"b","c"), ('a',"b","c"),('a',"b","c");
/********************************************/
var dbTree = $.fn.zTree.init($("#cityTree"), setting, zNodeArray);
dbTree.expandAll(true);  //加载完毕就展开整棵树的所有节点
var nodes = dbTree.getNodes();  //根据条件初始化check状态
$(nodes).each(function(i, e){
	dbTree.checkNode(e, true, true);
});
/********************************************/
用ajax方式加载zTree能够初始展开树有效
$.ajax({
	"url":"${ctx}/demo/getTreeNode.do",
	"type":"post",
	"async": true,
	"success":function(data){
		var zNodes = JSON.parse(data);
		var treeObj = $.fn.zTree.init($("#cityTree"), setting, zNodes);
		treeObj.expandAll(true);
	}
});
/********************************************/
控制zTree部分节点显示样式
var setting = {
	view: {
		fontCss: setFontCss
	}
}

function setFontCss(treeId, treeNode) {
	return treeNode.status == 1 ? {color:"green"} : {color:"red"};
};
/********************************************/
public Query getProductDetail(Integer productId,String hql) {
	Query query = getSession().createQuery(hql);
	query.setInteger(0, productId);
	return query;
}

public SureOrder getSureOrderVO(Integer productId) {
	String hql = "select pd.id as prodid,pd.cuxiaoPrice as cuxiaoPrice,pd.types as types,pd.price as price,pd.newprice as newprice,p.nodeId,p.imgurl as imgurl,p.types as types,p.factId as factId,p.qianYueId as qianyueId,p.guiGe as guiGe,p.name as name,p.pinPai as pinPai from Product pd,Prod p where p.id=pd.prodId and pd.id =?";
	Query query = productDao.getProductDetail(productId,hql);
	//主要起作用的就是下面的这个方法:SureOrder是我要封装的VO类.这样hibernate在执行hql查询的时候,会直接将结果封装为SureOrder对象.
	List list = query.setResultTransformer(Transformers.aliasToBean(SureOrder.class)).list();
	if(!list.isEmpty()){
		return (SureOrder) list.get(0);
	}
	return null;
}
/********************************************/
String sql = "SELECT cps.id , cps.home_cid , ct1.name as h_name , cps.visit_cid , ct2.name as v_name, cps.pk_date , cps.win "
              + "FROM career_pk_schedule cps , career_team ct1 , career_team ct2 "
              + "WHERE cps.home_cid = ct1.id AND cps.visit_cid = ct2.id "
              + "AND cps.`status` = 0 AND cps.pk_date = " + time ;
Session session = HibernateUtil.currentSession();

Query sqlQuery = session.createSQLQuery(sql);
sqlQuery.addScalar("h_name");
sqlQuery.addScalar("v_name");
sqlQuery.setResultTransformer(Transformers.aliasToBean(VoCareerTodaySchedule.class));

List<VoCareerTodaySchedule> list = sqlQuery.list();
/********************************************/
我的实现【转换为实体类】
public List<ModelCategory> getModelCategoryTree() {
	String sql = "SELECT t.id as id, t.name as name, t.is_leaf as isLeaf, t.description as description,t.parent_id as parentId FROM t_model_category t";
	//可以实现SQL查询结果到POJO的映射，但是不能实现string到布尔值的转换，如下sql不能执行
	//String sql = "SELECT t.id as id, t.name as name, IF(t.is_leaf=1, false, true) as isParent, t.description as description,t.parent_id as parentId FROM t_model_category t";
	SQLQuery sqlQuery = this.getSession().createSQLQuery(sql);
	List<ModelCategory> list = sqlQuery.setResultTransformer(Transformers.aliasToBean(ModelCategory.class)).list();
	list.add(new ModelCategory("1", "模型分类", "0", "根节点", 0, "false"));
	for(ModelCategory it : list){
		if(it.getIsLeaf() == 1){
			it.setIsParent(false);
		}else{
			it.setIsParent(true);
		}
	}
	return list;
}
我的实现【转换为Map】
List<Map<String, Object>> userList = listSqlQuery.setResultTransformer(Transformers.ALIAS_TO_ENTITY_MAP).list();
/********************************************/
但对于iterator的remove()方法，也有需要我们注意的地方：
1、每调用一次iterator.next()方法，只能调用【一次】remove()方法。
2、调用remove()方法前，【必须】调用过一次next()方法。
/********************************************/
public class CollectionIterationRemove {

	public static void main(String[] args) {
		originFor();
		forEach();
		iteration();
		//iterationMultRemove();
		iterationContaintRemove();
	}

	//允许遍历修改，但不允许遍历删除
	public static void originFor(){
		List<String> list = new ArrayList<String>();
		list.add("11");
		list.add("11");
		list.add("22");
		list.add("22");
		list.add("33");
		list.add("44");
		for(int i = 0; i < list.size(); i++){
			if("11".equals(list.get(i)) || "22".equals(list.get(i))){
				//list.remove(i);
				list.set(i, "99");
			}
		}
		System.err.println("originFor》"+list.toString());
	}
	
	//不允许遍历修改、删除
	public static void forEach(){
		List<String> list = new ArrayList<String>();
		list.add("11");
		list.add("11");
		list.add("22");
		list.add("22");
		list.add("33");
		list.add("44");
		for(String it : list){
			if("11".equals(it) || "22".equals(it)){
				//list.remove(it);
				it = "99";
			}
		}
		System.err.println("forEach》"+list.toString());
	}
	
	//允许遍历删除，但不允许遍历修改【前题是每次删除前必须重新执行hasNext()】
	public static void iteration(){
		List<String> list = new ArrayList<String>();
		list.add("11");
		list.add("11");
		list.add("22");
		list.add("22");
		list.add("33");
		list.add("44");
		list.add("22");
		
		Iterator<String> it = list.iterator();
		while(it.hasNext()){
			String str = it.next();
			if("11".equals(str) || "22".equals(str)){
				//str = "99";
				it.remove();
			}
		}
		System.err.println("iteration》"+list.toString());
	}
	
	//允许遍历删除，但不允许遍历修改【前题是每次删除前必须重新执行hasNext()】
	public static void iterationMultRemove(){
		String[] strArr = {"11", "22", "11"};  //内层循环中出现重复值，导致同一个外层元素被重复“删除”，出现异常
		List<String> list0 = new ArrayList<String>();
		list0.add("11");
		list0.add("11");
		list0.add("22");
		list0.add("22");
		list0.add("33");
		
		List<String> list = new ArrayList<String>();
		list.add("11");
		list.add("11");
		list.add("22");
		list.add("22");
		list.add("33");
		list.add("44");
		list.add("22");
		
		Iterator<String> it = list.iterator();
		while(it.hasNext()){
			String str = it.next();
			for(String temp : strArr){
				if(temp.equals(str)){
					it.remove();
				}
			}
		}
		System.err.println("iterationMultRemove》"+list.toString());
	}
	
	//允许遍历删除，但不允许遍历修改【前题是每次删除前必须重新执行hasNext()】
	public static void iterationContaintRemove(){
		List<String> list0 = new ArrayList<String>();  //内层循环中出现重复值，导致同一个外层元素被重复“删除”，出现异常
		list0.add("11");
		list0.add("11");
		list0.add("22");
		list0.add("22");
		list0.add("33");
		
		List<String> list = new ArrayList<String>();
		list.add("11");
		list.add("11");
		list.add("22");
		list.add("22");
		list.add("33");
		list.add("44");
		list.add("22");
		
		Iterator<String> it = list.iterator();
		while(it.hasNext()){
			String str = it.next();
			if(list0.contains(str)){
				it.remove();
			}
		}
		System.err.println("iterationContaintRemove》"+list.toString());
	}
}
/********************************************/
show variables like 'character\_set\_%';
show variables like 'collation_%';
set collation_server=utf8_general_ci;
/********************************************/
ps -ef | grep keystone
/********************************************/
1.if(exp_1, exp_2, exp_3);类似三目运算符的作用，exp_2, exp_3都不为null的前提下：如果exp_1为true则返回exp_2。
2.ifNull(exp_1, exp_2);如果exp_1为null则返回exp_2，如果exp_1不为null则返回exp_1。
/********************************************/
msyql> show global variables like '%timeout%';  //查看mysql server超时时间
msyql> set global wait_timeout=10;  //设置mysql server超时时间（以秒为单位），mysql就会自动断开连接，此时需要重启tomcat
msyql> set global interactive_timeout=10;  //设置活动超时（以秒为单位）这两个值最好一致
<property name="maxIdleTime">60</property>  //最大空闲时间,60秒内未使用则连接被丢弃，默认为0表示永不丢弃，最好小于MySQL的上述两属性
/********************************************/
高效for循环，java、js均适用
1.list.size()函数只执行一次，
2.相比于将将int leng = list.size()，变量放在for外的写法更紧凑优雅
3.比写在for外缩小了变量leng的作用域
for(int j = 0, leng = list.size(); j < size; j++){
	//TODO
}
/********************************************/
var str = '1250'; 
alert( Number(str) );  //得到1250
alert(parseInt(str));  //得到1250
 
var str1 = '00100'; 
alert( Number(str1) );  //得到100
alert(parseInt(str1));  //得到64
/********************************************/
public String getStringSetForSql(String[] stringArr){
	for(int i = 0, leng = arr.length; i < leng; i++){
		StringBuilder stringBuilder = new StringBuilder();
		stringBuilder.append("'" + (String) arr[i] + "'");
		if(i < leng - 1){
			stringBuilder.append(",");
		}
	}
	return stringBuilder.toString();
}
/********************************************/
var treeObj = $.fn.zTree.getZTreeObj("tree");  //获取树对象，其中“tree”是id号，注意：不带“#”
var node = treeObj.getNodes();  //获取树对象的根节点
var nodes = treeObj.transformToArray(node);  //获取树对象的所有节点的数组
/********************************************/
//回填选中测试
function checkNodeAccordingArr(){
	var idArr = [4, 5, 6, 7];
	var tempTree = $.fn.zTree.getZTreeObj("cityTree");
	var rootNode = tempTree.getNodes();
	var nodeArr = tempTree.transformToArray(rootNode);
	$(nodeArr).each(function(i, e){
		if(idArr.indexOf(e.id) >= 0){
			e.checked = true;  //初始化设置勾选
			zNodeOnCheck(e);  //强行调用回掉函数
			tempTree.updateNode(e);  //必须刷新节点，不然勾勾不正常
		}
	});
}

function beforeRightClick(treeId, treeNode){
	var tempTree = $.fn.zTree.getZTreeObj(treeId);
	tempTree.selectNode(treeNode);
	rightSelectNode = treeNode;
	closeWin();
}
/********************************************/
为tomcat配置文件server.xml中的http连接器添加URIEncoding="UTF-8"属性
<Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" URIEncoding="UTF-8"/>
/********************************************/
项目编码过滤器，配置在项目的web.xml文件中
<!-- 编码过滤器配置 -->
<filter>
	<filter-name>encodingFilter</filter-name>
	<filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>
	<init-param>
		<param-name>encoding</param-name>
		<param-value>UTF-8</param-value>
	</init-param>
	<init-param>
		<param-name>forceEncoding</param-name>
		<param-value>true</param-value>
	</init-param>
	<async-supported>true</async-supported>
</filter>
<filter-mapping>
	<filter-name>encodingFilter</filter-name>
	<url-pattern>/*</url-pattern>
</filter-mapping>
/********************************************/
1.在Javascript中，如果一个对象不再被引用，那么这个对象就会被GC回收。
	如果两个对象互相引用，而不再被第3者所引用，那么这两个互相引用的对象也会被回收。
	因为函数a被b引用，b又被a外的c引用，这就是为什么函数a执行后不会被回收的原因。
2.函数内部声明变量的时候，一定要使用var命令。如果不用的话，你实际上声明了一个全局变量！
3.闭包可以用在许多地方。它的最大用处有两个，一个是前面提到的可以读取函数内部的变量，另一个就是让这些变量的值始终保持在内存中。
/********************************************/
LongWritable Text
/********************************************/
1.分析业务逻辑。
2.确定输入输出数据类型。
3.自定义一个Mapper的子类<inKey1, inValue1, outKey1, outValue1>，并重写map方法，在map方法中实现业务逻辑，将新的key-value输出。
4.自定义一个Reducer的子类<inKey2, inValue2, outKey2, outValue2>，并重写reduce方法，在map方法中实现业务逻辑，将新的key-value输出。
5.在一个Entrance类的main方法中，将自定义的Mapper和Reducer通过Job对象组装起来。
/********************************************/
3-4
在MapReduce中，由于所有的数据都会在集群中通过网络传输，所以，数据类型必须可序列化。
考虑到效率因素，没有直接使用java的序列化机制，而是自己实现了一套序列化机制Writable。
【自定义bean必须实现Writable接口，在序列化函数和反序列化函数中，bean的成员变量顺序和数据类型必须保持高度一致】
【重写bean的toString方法】
1.紧凑：高效的使用存储空间，只存储数据值，不涉及继承、实现等类型间的关系。
2.快速：由于存储的紧凑所以减小了数据量，同时省去了一些额外开销。
3.可扩展：可透明的读取老格式的数据，除了支持hadoop的实现机制还支持google的序列化实现。
4.互操作：支持多种语言交互。
/********************************************/
MapReduce中的每一次context.write();操作都会将内容写入磁盘。
/********************************************/
hadoop-common  //基础依赖（所有必填）
hadoop-hdfs  //hdfs依赖（选填）
hadoop-mapreduce-client-core  //hadoopMapreduce依赖（mr必填）
/********************************************/
keys/Content Assist  //命令不全快捷键
/********************************************/
float f1 = 2.4F;
int i1 = (int) f1;
System.out.println(i1);  //2，因为强转操作先拷贝了源，真正被类型转换的是拷贝，然后将强转结果赋给新变量
System.out.println(f1);  //2.4
/********************************************/
Random random = new Random();  //未指定salt，默认以系统毫秒为salt
Random random = new Random(55);  //指定salt
/********************************************/
var str = '["11","22","33"]';
var ids = $.parseJSON(str);
alert(ids);
if(ids.indexOf("22") >= 0){
	alert("ee");
}
/********************************************/
Tomcat7性能调优
tomcat-user.xml
服务器信息
JVM
ajp-bio-8009（与其他啊平常服务器通信的协议）

ajp暂不改
1.改变运行模式
Bio(阻塞式的，低效)、
Nio（异步处理、连接通道、注册器代理数据准备、数据缓冲池）、
Apr（7及win7以上的系统）三种运行模式，默认是Bio，为了兼顾到jre3的老系统。
打开server.xml
将协议由http1.1改为Nio模式
2.启用线程池执行器（默认未启用）
打开server.xml，<Service name="Catalina"> 解禁执行器
3.配置连接器（非常重要）
allowTrace：允许head/trace请求
enableLookups：允许DNS
maxPostSize：限定POST请求最大字节长度
port：tcp端口号，为0则测试环境用
protocol：
/********************************************/
SET FOREIGN_KEY_CHECKS=0;  //暂停MySql外键约束
/********************************************/
SELECT PASSWORD('mypsw');  //41位
SELECT MD5("MySql");  //32位
SELECT SHA("MySql");  //40位
SELECT UUID();  //36位
SELECT REPLACE(UUID(),"-","");  //32位

SELECT LENGTH(PASSWORD("MySql"));
SELECT LENGTH(MD5("MySql"));
SELECT LENGTH(SHA("MySql"));

SELECT * FROM USERS WHERE NAME="JOHN" AND PASSWORD=MD5(‘MYPASS’);  //应用
SELECT REPLACE(UUID(),'-','') AS id;  //应用
/********************************************/
在linux下安装二进制版mysql
安装环境：系统是 centos6.5（均为root操作）
1、下载
下载地址：http://dev.mysql.com/downloads/mysql/5.6.html#downloads
下载版本：我这里选择的5.6.33，通用版，linux下64位
也可以直接复制64位的下载地址，通过命令下载：wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz

2、解压
tar -zxvf mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz  #解压
cp -r mysql-5.6.33-linux-glibc2.5-x86_64 /usr/local/mysql  #复制解压后的mysql目录

3、添加用户组和用户
groupadd mysql  #添加用户组
useradd -g mysql mysql  #添加用户mysql 到用户组mysql

4、安装
cd /usr/local/mysql/  #进入安装目录
mkdir ./data/mysql  #新建数据存储路径
chown -R mysql:mysql ./  #将安装目录所有者改为mysql用户（在local下操作）

./scripts/mysql_install_db --user=mysql --datadir=/usr/local/mysql/data/mysql  #初始化数据库（在安装包目录下操作）
#注意：在初始化时如果报如下错误：
FATAL ERROR: please install the following Perl modules before executing ./scripts/mysql_install_db:
Data::Dumper。表示系统缺Data::Dumper组件，用yum -vy install autoconf可以解决。

cp support-files/mysql.server /etc/init.d/mysqld  //安装包下
chmod 755 /etc/init.d/mysqld
cp support-files/my-default.cnf /etc/my.cnf
 
#修改启动脚本
vi /etc/init.d/mysqld
 
#修改项：
basedir=/usr/local/mysql/mysql-5.6.34  //安装包路径
datadir=/usr/local/mysql/data/mysql  //数据存储路径
 
#启动服务
service mysqld start
 
#测试连接
./mysql/bin/mysql -uroot
 
#加入环境变量，编辑 /etc/profile，这样可以在任何地方用mysql命令了
export PATH=$PATH:/usr/local/mysql/mysql-5.6.34/bin
source /etc/profile



#启动mysql
service mysqld start
#关闭mysql
service mysqld stop
#查看运行状态
service mysqld status

5、错误

5.1 sqlyog连接时，报1130错误，是由于没有给远程连接的用户权限问题
解决1:更改 ‘mysql’数据库‘user’表‘host’项，从‘localhost’改成‘%’。
use mysql;
select 'host' from user where user='root'; 
update user set host = '%' where user ='root';
flush privileges; 
解决2：直接授权
GRANT ALL PRIVILEGES ON *.* TO ‘root’@'%’ IDENTIFIED BY ‘youpassword’ WITH GRANT OPTION;

5.2 安装时的一些错误
-bash: ./scripts/mysql_install_db: /usr/bin/perl: bad interpreter: 没有那个文件或目录
解决： yum -y install perl perl-devel
Installing MySQL system tables..../bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory
解决：yum -y install libaio-devel

6、其他

6.1 配置环境变量
vi + /etc/profile
export PATH=....:/usr/local/mysql/bin
/********************************************/
//常量枚举类申明
public enum ConstantEnum {
	STATUS_DELETED("00"), STATUS_NOT_DELETED("01");
	private String value;
	
	private ConstantEnum(String value) {  //带参构造器
		this.value = value;
	}
	public String getValue() {  //值得getter方法
		return value;
	}
}

System.err.println(ConstantEnum.STATUS_DELETED.getValue());  //应用
/*********************Activemq部署***********************/
下载linux版本的二进制包activemq.tar.gz，解压activemq.tar.gz，然后到bin目录（也可以到更下层linux-x86-64目录）启动mq服务
[youn@master01 bin]$ nohup ./activemq start &  //以后台进程方式启动mq
[youn@master01 bin]$ ./activemq stop  //关闭mq
[youn@master01 bin]$ ps -ef | grep activemq  //查看mq运行状态
http://localhost:8161  //进入Activemq管理页面，查看服务是否正常启动
/********************************************/
1.下载Windows版本的二进制包apache-activemq-5.14.4-bin.zip。
2.解压，双击activemq_home\bin\win64\activemq.bat启动mq服务。
3.用http://127.0.0.1:8161/访问mq服务器【一定要用回环地址，不能用localhost主机名】。
/********************************************/
开发：
1.新建java项目
2.加入activemq-all-5.13.2.jar包
3.创建一个生产者类，
	private static final String USER_NAME = 
	public static void main(String[] args){
		ConnectionFactory connectionFactory;  //连接工厂
		Connection connection;  //连接对象
		Session session;  //接受或发送消息的线程
		Destination destination;  //消息发送目的地
		MessageProducer messageProducer;  //消息生产者
	}
4.先订阅，后发布
/********************************************/
//Windows系统垃圾清理bat脚本
@echo off
echo Launching garbage cleanup progress now, waite a few minutes please...
del /f /s /q %systemdrive%\*.tmp
del /f /s /q %systemdrive%\*._mp
del /f /s /q %systemdrive%\*.log
del /f /s /q %systemdrive%\*.gid
del /f /s /q %systemdrive%\*.chk
del /f /s /q %systemdrive%\*.old
del /f /s /q %systemdrive%\recycled\*.*
del /f /s /q %windir%\*.bak
del /f /s /q %windir%\prefetch\*.*
rd /s /q %windir%\temp & md %windir%\temp
del /f /q %userprofile%\cookies\*.*
del /f /q %userprofile%\recent\*.*
del /f /s /q "%userprofile%\Local Settings\Temporary Internet Files\*.*"
del /f /s /q "%userprofile%\Local Settings\Temp\*.*"
del /f /s /q "%userprofile%\recent\*.*"
echo Garbage cleanuping DONE now, press any key to continue!
echo. & pause
/********************************************/
1.新版Eclipse已集成maven插件，所以无需另行下载安装插件。（ maven=> update project）
2.新版maven必须运行在jdk的jre上上，所以Eclipse的Java->installedJre环境路径需是jdk的路径，之后设置项目的jre为工作空间默认jre。
3.用run as->maven clean清空旧的编译class文件。
4.用run as->maven build->gole中填写package选项，即可重新编译。
5.maven ju he only for management multmoduals.   what for?????package
6.extens for version and dependence.
/********************************************/
1.Maven build 等同于Maven命令：mvn package  //表示对Maven工程进行打包。
2.Maven clean等同于Maven命令：mvn clean  //表示删除maven工程的target目录下的内容。
3.Maven install等同于命令：mvn install  //表示将jar包发布到本地maven仓库。
mvn [-X] clean compile package  //在项目根目录下执行

【注意】此时使用原生maven命令，引用的是默认配置setting.xml，所以一定要在这个文件中修改仓库地址等配置。
/********************************************/
<![CDATA[<=]]>
/********************************************/
生产环境Tomcat配置
自动部署：false。
资源压缩：true。
管理tomcat管理设置（要么修改密码、要么删除管理应用）。
修改Catalina的java选项，一般4G，BASE_CATALINA。
log4j必须配置，或者用更高效的slf4j。
/********************************************/
清理tomcat缓存；删除work目录里面的项目文件夹。
/********************************************/
清理cdh环境日志：/var/log/[Oozie\Hive\Hbase]  //直接到操作系统上remove
/********************************************/
单机版spark安装配置
1.依次将jdk、scala、spark的压缩包解压。
2.配置三组（包括jdk和jre）环境变量，并更新环境变量。
3.分别用java -version、scala -version查看jdk和scala是否配置成功。
4.运行spark目录/bin/spark-shell查看spark是否配置成功。
5.访问http://192.168.248.128:4040/查看spark运行状态。
/********************************************/
WHERE 1 = 1
<if test="modelname != null and modelname != ''">
	AND t.modelName LIKE CONCAT("%", :modelname, "%")
</if>
<if test="startDate != null and startDate != ''">
	AND t.createDate >= CONCAT(:startDate, " 00:00:00")
</if>
<if test="endDate != null and endDate != ''">
	AND t.createDate <![CDATA[<=]]> CONCAT(:endDate, " 23:59:59")
</if>
/********************************************/
由于Windows系统会自动给不同的tomcat分配各自的Catalina环境变量，
所以只需修改conf目录下的server.xml中的【关闭监听、http、AJP】4个端口就可以了
<Server port="8006" shutdown="SHUTDOWN">  //关闭操作监听端口加1
<Connector connectionTimeout="20000" port="8081" protocol="HTTP/1.1" redirectPort="8444"/>  //http访问端口、https转发端口各加1
<Connector port="8010" protocol="AJP/1.3" redirectPort="8444"/>  //AJP连接端口、https转发端口各加1

/********************************************/
	http访问端（默认为8080）:8081 
	Shutdown远程停服务端(默认为8085):8006 
	修改AJP端口（默认为8009端口）8010
	默认端口8443：8444

在Linux中，除了要修改server.xml文件中的三个端口，
还需在各自用户的bashrc分别配置各tomcat的环境变量及catalina变量（保证各tomcat对应的变量名不重复）
【更好的做法是将：公用环境变量、各个tomcat环境变量配到/etc/profile文件开头部分】
CLASSPATH=".:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar"
export TOMCAT_HOME=~/tomcat9
export TOMCAT_HOME
CATALINA_BASE=~/tomcat9
CATALINA_HOME=~/tomcat9
export CATALINA_BASE CATALINA_HOME

export TOMCAT_0_HOME=~/tomcat9_0
export TOMCAT_0_HOME
CATALINA_0_BASE=~/tomcat9_0
CATALINA_0_HOME=~/tomcat9_0
export CATALINA_0_BASE CATALINA_0_HOME

#tomcat use by basic
CATALINA_BASE_3=/home/modelserver/apache-tomcat-basic
CATALINA_HOME_3=/home/modelserver/apache-tomcat-basic
TOMCAT_HOME_3=/home/modelserver/apache-tomcat-basic
export CATALINA_BASE_3 CATALINA_HOME_3 TOMCAT_HOME_3

将CATALINA_HOME和CATALINA_BASE变量分别配置到各自的：
shutdown.sh  //必须配
startup.sh  //必须配
catalina.sh  //必须配
{
	#!/bin/sh
	JAVA_OPTS="-server -Xms1024m -Xmx2048m -XX:PermSize=64M -XX:MaxNewSize=256m -XX:MaxPermSize=128m -Djava.awt.headless=true"  //jvm启动参数
	export CATALINA_HOME=$CATALINA_HOME_1
	export CATALINA_BASE=$CATALINA_BASE_1
	export TOMCAT_HOME=$TOMCAT_HOME_1
}
这三个脚本上部。
/********************************************/
Catalina JVM启动参数
在catalina.sh上部添加
JAVA_OPTS=-server -Xms512m -Xmx2048m -XX:PermSize=256M -XX:MaxPermSize=512M
/********************************************/
新建Maven Web项目
1.用web项目构建骨架，构建一个新项目。
2.由于创建的是Dynamic Web Module，所以需要选择与java版本对应的版本的Dynamic Web Module。
3.此时会生成WebContent目录，此目录不是maven项目应有的，所以需要将其下的两个目录META-INF和WEB-INF剪切到src下的webapp下，并删除WebContent目录。
4.web.xml应该用webapp目录下原本生成的那个。
5.到项目的Deployment Assembly选项中，删除WebContent发布目录，同时添加【folder型的发布目录，此目录指向webapp目录。
6.最后需要将发布目录build path指向Maven Dependency（如果已经是则不需要改）。
 
/********************************************/
在/etc/profile文件中追加下列变量，配置全局环境变量
#add jdk1.7
export JAVA_HOME=/usr/java/jdk1.7.0_67
export CLASSPATH=.:$CLASSPTAH:$JAVA_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin

#tomcat use by webapp
CATALINA_BASE=/home/webapp/apache-tomcat-6.0.36 
CATALINA_HOME=/home/webapp/apache-tomcat-6.0.36
TOMCAT_HOME=/home/webapp/apache-tomcat-6.0.36
export CATALINA_BASE CATALINA_HOME TOMCAT_HOME
/********************************************/
1、要打包的项目jre需设为对应版本的【jdk】
2、如果项目编码是UTF-8，操作系统是GBK等其他编码，出现“编码gbk的不可映射字符”错误，在下聚合pom.xml中添加下列属性即可：
	<properties>  
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>  
		<maven.compiler.encoding>UTF-8</maven.compiler.encoding>  
	</properties>
3、run as maven build ：goals=> clean package  //只需在聚合项目中执行一次，不需install
/********************************************/
linux下运行可执行jar包（windows下也一样）：
1.从Eclipse导出可执行jar包，附加附加依赖的jar包以Package required libraries into generated JAR方式一并打进包中.
2.将打好的jar包Test.jar上传到linux（windows）系统，在包的目录下执行java -classpath kmeans -jar Run.jar。
nohup java -classpath kmeans -jar Test.jar &  //在Linux中以后台方式运行jar包
/********************************************/
@RequestMapping(value = "/setBaseModel/{token}/{modelId}" ,method = RequestMethod.PUT)
public void setBaseModel(HttpServletResponse response,@PathVariable("token") String token, @PathVariable("modelId") String modelId) {}
{token、modelId}在这个请求的URL里就是个变量，可以使用@PathVariable来获取
@PathVariable和@RequestParam的区别；
@RequestParam用来获得静态的URL请求参数；
@PathVariable用来获得动态的URL请求入参。
/********************************************/
httpPost.setHeader("Content-Type", "application/x-www-form-urlencoded");
/********************************************/
httpPost.addHeader("Content-Type", "application/xml");
/********************************************/
EL表达式取值拼接到URL，必须写在双引号内
url="${contextPath}/system/model!querySumModelList.action?realSubModelIdArr=${realSubModelIdArr}" 
/********************************************/
1、抽象类（不可实例化）
2、单例类（只可实例化一个对象）
3、多例类（可以实例化出多个对象）
/********************************************/
用PostMan或RestClient测试远程接口时，报415传输格式不支持错误时，需要设置请求报头Content-Type=application/json或其他格式。
/********************************************/
SSH框架数据库连接导致插入表中的中文乱码，可能是数据库连接url配在properties文件对字符集参数解析错误导致的。
/********************************************/
Archetype Group Id : net.alchim31.maven
Archetype Artifact Id : scala-archetype-simple
Archetype Version : 1.6或者1.5
//若下载失败在添加下面的Repository URL试试
Repository URL : http://github.com/davidB/scala-archetype-simple
/********************************************/
Archetype Group Id : org.scala-tools.archetypes
Archetype Artifact Id : scala-archetype-simple
Archetype Version : 1.2
/********************************************/
方法一：
1、要想从iframe跳出，并重定向到登录页面需要有一个中间页面来做跳板。
2、先跳到个跳板，跳板会加载到iframe，然后在跳板中执行这段js代码，就可顺次查找到页面的最外层框架。
3、然后利用最外层框架页面来跳转到登录页面。
var top = window;
while(top != top.parent){
	top = top.parent;  //循环找到父窗体
}
top.location.href = "http://www.baidu.com";

方法二：
直接将下列代码写入login.jsp页面的初始化代码块的开头。
$(function(){
	if(window != top){  
	    top.location.href=location.href;  
	}  
	setToastr();
});
/********************************************/
拦截器前段拦截方法中用来判断是否登录，
if(postId == null || postId == ""){  //如果为登录
	String XRequested =request.getHeader("X-Requested-With");
	if("XMLHttpRequest".equals(XRequested)){  //如果是ajax操作，返回数据到前端处理
		response.getWriter().write("IsAjax");
	}else{  //如果是普通的非ajax操作
		response.sendRedirect("/m-web/user/toLogin");
	}
	return false;
}
如果是ajax操作，未登录操作，需要到前端处理跳转
success : function(data) {
	if(data=='IsAjax'){
	 alert("ajax");
	 window.location.href='m-web/user/toLogin';  //其中m-web是项目名
	 return;
 }
/********************************************/
<title>大数据业务模型平台</title>
<link href="${contextPath}/image/favicon.ico" rel="shortcut icon">  //网页地址栏标题和对应的小图标
/********************************************/
1.欢迎页面和错误页面配置web.xml
<welcome-file-list>
	<welcome-file>index.jsp</welcome-file>
</welcome-file-list>
<error-page>
	<error-code>404</error-code>
	<location>/index.jsp</location>
</error-page>
<error-page>
	<error-code>500</error-code>
	<location>/index.jsp</location>
</error-page>

2.index.jsp中：
<%@ page contentType="text/html;charset=UTF-8" %>
<%
	String path = request.getContextPath();
	String basePath = request.getScheme()+"://"+request.getServerName()+":"+request.getServerPort()+path+"/";
	response.sendRedirect(basePath+"main!login.action");  //用这种方式才能避免带斜杠的错误url
%>
/********************************************/
后台向前台返回Json格式拼接数据：
JsonUtil.outJson("{success: false, msg: '" + e.getMessageKey() + "'}");
/********************************************/
//将从一张表里的查询结果作为新纪录插入到另一张【可以是同一张表】表中
<insert id="transformToBaseModel">
	INSERT INTO basemodel (
		modelId,
		executeType,
		modelName,
		modelDescription,
		hdfsLocation,
		`status`,
		version,
		createDate,
		projectId,
		iconpath,
		`function`,
		baseType,
		localLocation,
		model_status
	) SELECT
		t.modelId,
		t.executeType,
		t.modelName,
		t.modelDescription,
		t.hdfsLocation,
		t.`status`,
		t.version,
		t.createDate,
		t.projectId,
		t.iconpath,
		t.`function`,
		t.type,
		t.localLocation,
		t.model_status
	FROM
		model t
	WHERE
		t.modelId = :modelId
</insert>
/********************************************/
DATE_FORMAT(TC.CREATE_TIME, '%Y-%m-%d %H:%i:%s') AS createTime,
/********************************************/
@Deprecated  //过期注解
/********************************************/
1.在新页面窗口中打开连接
	<a href="http://www.sc-troy.com/" target="_blank">COPYRIGHT ? 2017 ALL RIGHTS RESERVED  版权所有-四川创意信息技术股份有限公司</a>
2.防止浏览器返回按钮功能
	window.history.forward(-1);
/********************************************/
//--master local表示非集群模式，
//--class com.WordCount表示应用的执行入口（主类名）
//WordCount.jar 表示应用对应的jar包（需要全路径，当前是在cmd.exe本地）
【spark-submit --master local --class com.WordCount WordCount.jar】
/********************************************/
urlEncoding="utf-8"  //在server.xml中配置此项，使tomcat知道url的编码格式
/********************************************/
SparkHistory
Spark提供了一个WEB UI来向用户展现程序的运行状态信息，
但是一旦WEB UI会随着Application的终止（成功/失败）而关闭，
SparkHistory就是为了解决这个问题而生的，提供Spark离线状态查询。
一、作用{
1.在Application执行的过程中记录下日志事件信息。
2.Spark运行在yarn、mesos等【集群管理器】上时，通过Spark的History server，
重构出一个已经终止了得Application的运行时参数信息。}
二、配置{
1.在spark_home/conf下复制模板：spark-defaults.conf.template，并将文件名修改为：spark-defaults.conf
2.修改spark-defaults.conf内的参数
	spark.eventLog.enabled true  //启用事件日志记录功能
	spark.eventLog.dir hdfs://namenode:8021/directory  //非必需 日志存放路径
}
3.启动history-server服务{
	./sbin/start-history-server.sh
	启动后就可访问
}
4.REST API{
	通过调用API得到JSON格式数据，整合到自定义应用中
	http://SparkMaster:4040/api/v1  //获取正在运行的app的信息
	http://SparkMaster:18080/api/v1  //获取历史服务器上的app的信息
}
/********************************************/
WEB UI监控

/*******************修改MySql数据存储路径*************************/
一.首先把mysql的服务先停掉。

二.更改MySQL配置文件My.ini中的数据库存储主路径
打开MySQL默认的安装文件夹C:\Program Files\MySQL\MySQL Server 5.1中的my.ini文件，
点击记事本顶部的“编辑”，“查找”，
在查找内容中输入datadir后并点击“查找下一个”转到“Path to the database root数据库存储主路径”参数设置，
找到datadir="C:/Documents and Settings/All Users/Application Data/MySQL/MySQL Server 5.1/Data/"
即是默认的数据库存储主路径设置，现将它改到C:\mysql\data（你希望的）文件夹，
正确的设置是datadir="C:/mysql/data/"。更改完成后点击菜单栏的“文件”，再点击“保存”。

三.将老的数据库存储主路径中的数据库文件和文件夹复制到新的存储主路径
将C:/Documents and Settings/All Users/Application Data/MySQL/MySQL Server 5.1/Data/文件夹中的所有文件
和文件夹拷贝到你新建的文件夹目录下。

四.重启MySQL服务

五.验证更改数据库存储主路径的操作是否成功
/********************************************/
SpringMVC中要返回JSON格式的字符串，直接return JSON字符串会出错，需用writer的方式。	
/********************************************/
将jre打到应用程序中以避免要求用户安装jre或者jre版本冲突问题：
1.将完整的jre目录拷贝到应用中（在jdk安装目录下可以找到对应版本的jre目录）。
2.在应用的bat文件前面声明运行时环境变量：
	set TROYAI_HOME=%~dp0
	set JAVA_HOME=%TROYAI_HOME%jre7
	set PENTAHO_JAVA_HOME=%JAVA_HOME%
	set classpath=%JAVA_HOME%\lib;
/********************************************/
MySql之NOT EXISTS用法：
SELECT * FROM model_au_apply t WHERE NOT EXISTS (SELECT m.modelId FROM model m WHERE m.modelId = t.MODEL_ID);
DELETE t FROM model_au_apply t WHERE NOT EXISTS (SELECT m.modelId FROM model m WHERE m.modelId = t.MODEL_ID);
/********************************************/
shutdown now  //立即关机
reboot  //立即重启
/*********************Spark StandAlone***********************/
1.分别在各主从节点的/etc/hosts文件中配置好个节点的ip-主机映射。
2.分别在各主从节点解压java、scala目录，配置好java、scala环境变量。
3.分别在各主从节点解压spark目录，配置好spark环境变量。
4.配置好各主从节点之间的免密码访问。
5.到主节点的spark_home/config目录中复制slaves.template为slaves，并在slaves文件中追加上从节点主机名。
6.在主节点的spark_home/sbin下执行start-all.sh启动所有节点。
7.分别在各节点上执行jps命令，查看启动情况，同时可以登录主节点的8080【http://192.168.88.128:8080】端口查看集群情况。
8.将调试好的程序jar包提交到Standalone集群（最后两个参数表示传入main函数的字符串参数，比如输入、输出的路径）
	spark-submit --master spark://vm0:7077 --class com.WC4Cluster ~/data/test/app/WordCount.jar ~/data/test/input/in.txt ~/data/test/output/WC4Cluster_
9.当有作业执行时，可以通过主节点的4040端口【http://192.168.88.128:4040】监控作业状态。
/********************************************/
idea中给main方法传递字符串参数：
1.打开入口类源文件，菜单栏run/edit configurations在program arguments栏填上输入的字符串参数，多个用空格分隔，点击apply。
/********************************************/s
idea中打包项目：
1.首先选中项目，菜单File/project structure/Artifacts，填写好jar包名及需要打包的目录。
2.菜单栏 Build/Build Artifacts并在弹框中选择想要的的命令（如 ：build、rebuild等），待运行完毕，即会在项目out目录下生成jar文件。
/*******************Spark History server*************************/
1.为什么需要historyServer?
在运行Spark Application的时候，Spark会提供一个WEBUI列出应用程序的运行时信息；
但是，该WEBUI随着Application的完成(成功/失败)而关闭，也就是说，Spark Application运行
完(成功/失败)后，将无法查看Application的历史记录。
而Spark history Server就是为了应对这种情况而产生的，
通过配置可以在Application执行的过程中记录下了日志事件信息，
那么在Application执行结束后，WEBUI就能重新渲染生成UI界面展现出该Application在执行过程中的运行时信息。

简单配置：
1.复制spark_home/conf/spark-defaults.conf.template 为 spark-defaults.conf
	并打开配置项spark.eventLog.enabled true的注释，表示启用事件日志。
	可以采用默认的日志位置，记录到本地的/tmp/spark-events目录下，但是以上目录必须存在。
	也可以将日志指定到本地其他目录，配置需从根目录开始，如：
	spark.eventLog.dir file:///home/youn/data/sparkLog  //必须以【file://】协议开头，并从根目录算起，三道斜杠。
2. sh spark_home/sbin/start-history-server.sh 【~/data/sparkLog】  //查资料说不用跟路径，但是不能成功
3.访问http://masterIp:18080  //显示历史作业记录列表，18080为默认端口
4.spark-history-server可以【单独运行】，不依赖节点上的spark主服务，也不在start-.sh等命令控制范围内。
详细配置：需要配置spark_home/conf/spark-env.sh文件中的配置项。
/********************************************/
Zookeeper能做什么事情呢，简单的例子：
假设我们有20个搜索引擎的服务器(每个负责总索引中的一部分的搜索任务)和一个总服务器
(负责向这20个搜索引擎的服务器发出搜索请求并合并结果集)，
一个备用的总服务器(负责当总服务器宕机时替换总服务器)，一个web的cgi(向总服务器发出搜索请求)。
搜索引擎的服务器中的15个服务器提供搜索服务，5个服务器正在生成索引。
【这20个搜索引擎的服务器经常要让正在提供搜索服务的服务器停止提供服务开始生成索引，
或生成索引的服务器已经把索引生成完成可以提供搜索服务了】。
使用Zookeeper可以保证总服务器自动感知有多少提供搜索引擎的服务器并向这些服务器发出搜索请求，
当总服务器宕机时自动启用备用的总服务器。
/********************************************/
文件系统构成：
1.文件管理程序。
2.数据结构。
3.文件实例。
磁盘读写时间 = 数据块寻址时间 + 传输时间  //在不考虑空间利用率的情况下，适当增加块大小可以提高读写速度。
/********************************************/
chmod +x myShell.sh  //修改myShell.sh文件的属性为可执行权限
/********************************************/
#!/bin/sh  //在脚本开头指定shell执行器类型，如ksh、bash、sh（支持最广泛）等，此行可以省略表示使用系统默认的执行器
#this line is comment！  //注释
a = "hello world"  //声明变量
echo $a  //打印变量
/********************************************/
Shell Script是一种弱类型语言，使用变量的时候无需类型声明。
新的变量会在本地数据区分配内存进行存储，这个变量为当前的Shell私有，
任何子进程都不能访问本地变量。这些变量与环境变量不同，环境变量被存储在另一内存区，
叫做用户环境区，这块内存中的变量可以被子进程访问。

env：用于显示用户环境区中的变量及其取值。
set；用于显示本地数据区和用户环境区中的变量及其取值。
unset：用于删除指定变量当前的取值，该值将被指定为NULL。
export：命令用于将本地数据区中的变量转移到用户环境区。
/********************************************/
maven参数化配置jar包版本
<properties>
	<spring.version>3.1.3.RELEASE</spring.version>
</properties>
<dependencies>
	<dependency>
		<groupId>com.github.snakerflow</groupId>
		<artifactId>snaker-core</artifactId>
		<version>${spring.version}</version>
	</dependency>
</dependencies>
/********************************************/
LVS：Linux Virtual Server，将一个集群虚拟为一个整体服务器，在访问者看来只有“一台”服务器，用以负载均衡。
1.LVS使用了目前效率最高的IP负载均衡技术，提供了该技术下VS/NAT、VS/TUN和VS/DR三种具体实现。
/********************************************/
uname -r  //查看系统内核
lsmod | grep ip_vs  //查看系统是否已加载了指定模块
/********************************************/
Linux配置jvm最大内存和初始最小内存参数：
export _JAVA_OPTIONS="-Xms1g -Xmx1g"  //需要在java环境变量配置文件添加下列参数
/********************************************/
配置tomcat内存参数：{
1.在tomcat_home/bin/catalina.sh中找到{
		cygwin=false
		darwin=false
		os400=false
		case "`uname`" in
		CYGWIN*) cygwin=true;;
		Darwin*) darwin=true;;
		OS400*) os400=true;;
		esac
	}
2.在其后追加：{
		JAVA_OPTS="-Xms3072m -Xmx3072m \  //分别表示初始最小堆内存、最大堆内存
					-XX:PermSize=256M \  //永久代初始内存
					-XX:MaxPermSize=512m \  //永久代最大内存
					-Xss2m \  //每个线程内存
					-Xmn1536m"
	}
}
cygwin=falsedarwin=falseos400=falsecase "`uname`" inCYGWIN*) cygwin=true;;Darwin*) darwin=true;;OS400*) os400=true;;esac
在下面添加所需要设置的java内存大小,比如设置java堆内存为3g,新生代为1536m,永久代初始值为256m,最大值为512m,每个线程大小2m,参数如下:
JAVA_OPTS="-Xms3072m -Xmx3072m  \-XX:PermSize=256M \-XX:MaxPermSize=512m \-Xss2m \-Xmn1536m"
/********************************************/
Centos同步时间：
1.yum安装ntp.x86_64服务，自动安装其依赖ntpdate.x86_64。
	chkconfig ntpd on  //将ntpd设置为开机启动
2.修改/etc/ntp.conf，以配置授时地{
	将原本有效的
	# Please consider joining the pool (http://www.pool.ntp.org/join.html).
	  server 0.centos.pool.ntp.org iburst
	  server 1.centos.pool.ntp.org iburst
	  server 2.centos.pool.ntp.org iburst
	  server 3.centos.pool.ntp.org iburst
	全注释掉
	
	在其下追加{
		server ntp.sjtu.edu.cn perfer  #复旦大学
		server 210.72.145.44  #中国国家授时中心
		server 202.112.10.36  #1.cn.pool.ntp.org
		server 59.124.196.83  #0.asia.pool.ntp.org
	}
	
	主节点按以上配置作为唯一出口，然后作为集群的唯一授时服务器，从节点需将ntp服务器指向主节点
	server node0.com iburst  //其他都注释掉
}
3.service ntpd start  //启动ntp服务

ntpstat  //查看ntp时间同步状态

4.如果时区不是CST则需要配置时区{
	1）找到要设置的时区文件如：上海/usr/share/zoneinfo/Asia/Shanghai，替换当前的/etc/localtime。
	2）cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime  //将文件替换为当前时区文件中（勿忘备份）
}
5.重启ntpd
/********************************************/
usermod -a -G groupName user  //将用户添加到新组，同时用【-a】将用户保留在之前的分组中（如果没有这个参数则变成用户只归新组）
r=4，w=2，x=1  //因此rwx=4+2+1=7
/********************************************/
Java中主要有JDK的logging和apache的log4j两个日志实现工具，但log4j更主流：
1.sl4j不是日志工具实现，它只是一个统一配置各实现的接口，以提高易用性。
2.sl4j根据加载的整个jar包来判断到底用哪种实现，项目中只能有一种实现（加入多种会出错）。
3.java类中引入sl4j的logger，在需要记录日志处调用logger的info方法。
/********************************************/
mvn help:system  //能下载某些maven插件
/******************在Navicate向MySql导入数据**************************/
右键数据库>>运行SQL文件>>指定SQL文件路径>>勾选所有选项>>编码选65001(UTF-8)>>开始。
/********************************************/
Linux下的MySQL默认是区分表名大小写的，通过如下设置，可以让MySQL不区分表名大小写：
1、用root登录，修改 /etc/my.cnf；
2、在[mysqld]节点下，加入一行：lower_case_table_names=1
3、重启MySQL即可；
其中lower_case_table_names=1，参数缺省地在Windows中这个选项为1，
在Unix中则为0，因此在window中不会遇到的问题，这就是移植到Linux就会出问题的原因
（尤其在mysql对表起名时是无法用大写字母的，而查询用了大写字母却会出查不到表的错误）
/********************************************/
安装MySql过程中清理了匿名用户、设置了root密码等安全操作后，如果不能本地root用户登录，用mysql -uroot -p然后输入密码访问。
/********************************************/
微软拼音输入法繁简切换：
Ctrl + Shift + f
/********************************************/
1.为输入框组测blur验证事件：
$().ready(function(){
	$("#add_loginId").blur(function(){
		var exist = checkUserByLoginId($(this).val());
		if (exist) {
			troy_warning("登录名重复！");
		}
		return !exist;
	});
});

2.点击保存时触发验证：
function checkSingleInput (e){
	var exist = checkUserByLoginId($(e).val());
	if (exist) {
		troy_warning("登录名重复！");
	}
	return exist;
}

3.后台交互验证函数：
function checkUserByLoginId(loginId) {
	if(!loginId){
		return;
	}
	var exist = false;
	$.ajax({
		url: "system/area!checkUserByLoginId.action",
		type: 'post',
		async: false,
		data: {
			"loginId": loginId
		},
		dataType: 'json',
		success: function(data) {
			if(data.output > 0){
				exist = true;
			}
		},
		error: function(x) {
			toastr.error("校验失败！");
		}
	});
	return exist;
}
/********************************************/
在使用spring框架配置AOP的时候，不管是通过XML配置文件还是注解的方式都需要定义pointcut"切入点"
例如定义切入点表达式  execution (* com.sample.service.impl..*.*(..))
execution()是最常用的切点函数，其语法如下所示：
整个表达式可以分为五个部分：
1、execution(): 表达式主体，此函数表示方法切入。
2、第一个*号：表示返回类型，*号表示所有的类型。
3、包名：表示需要拦截的包名，后面的两个句点表示当前包和当前包的所有子包，com.sample.service.impl包、子孙包下所有类的方法。
4、第二个*号：表示类名，*号表示所有的类。
5、*(..):最后这个星号表示方法名，*号表示所有的方法，后面括弧里面表示方法的参数，两个句点表示任何参数。
/********************************************/
spring与springmvc整合出现AOP不起作用的原因：
1、spring与springmvc加载配置文件是不同步的，springmvc加载文件对注解进行扫描后，所有的注解都被扫到容器里面，当spring同样加载配置文件扫描注解时，因为容器中已经
存在Service类，那么CGLib代理或jdk动态代理就不对它进行代理了，直接导致了applicationContext.xml中的事务不起作用，出现异常，事务不会滚。
2、解决方法：Spring MVC 和 Spring 整合的时候，SpringMVC的springmvc.xml文件中 配置扫描包，不要包含 service的注解，Spring的applicationContext.xml文件中 配置扫描包时，不要包含controller的注解，如下所示：
SpringMVC的xml配置：
<!--1、 注解探测器，对注解进行扫描 -->
<context:component-scan base-package="com">
    <context:exclude-filter type="annotation" expression="org.springframework.stereotype.Service"/>
</context:component-scan>
同样的，spring的applicationContext.xml配置文件也一样，不要扫描@Controller注解
<!-- 注解扫描器 -->
<context:component-scan base-package="com">
   <context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/>
</context:component-scan>
工作完成。
/*****************Spring-AspectJ***************************/
1.jar包依赖：spring-core、spring-webmvc、spring-aspects、aspectjrt、aspectjweaver。
2.applicationContext.xml配置：
{
	<context:annotation-config/>
	<context:component-scan base-package="com"></context:component-scan>
	<aop:aspectj-autoproxy/>
	<aop:aspectj-autoproxy proxy-target-class="true"/>
}
3.切面类编写：
【
	@Aspect
	@Component
	public class MyAspect {
		@Pointcut(value = "execution(* com.*.*(..))")
		public void onCut(){}
		@Before(value = "onCut()")
		public void beforeCut(JoinPoint joinPoint){
			System.err.println(joinPoint.getSignature().getName());
			System.err.println("beforeCut=======");
		}
		@After(value = "onCut()")
		public void afterCut(){
			System.err.println("AfterCut=======");
		}
	}
】
4.测试类编写：
【
	public class Util {
		public static void main(String[] args) {
			String[] location = {"applicationContext.xml"};
			final ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(location);
			MyAspect myAspect = context.getBean(MyAspect.class);
			Worker worker = context.getBean(Worker.class);
			worker.doSth();
		}
	}
】
/********************************************/
Seq和Set是针对现实使用场景的不同数据结构抽象。
1.Seq是列表，适合存有序重复数据，进行快速插入/删除元素等场景
2.Set是集合，适合存无序非重复数据，进行快速查找海量元素的等场景
/********************************************/
系统centos7.0_64位安装Tensorflow
1.Python版本：(注意tensorflow目前只支持python2.7版本)
python -V  //查看python版本
2.安装pip
yum update -y && yum install -y python python-devel epel-release.noarch python-pip
3.使用pip安装tensorflow
pip install https://storage.googleapis.com/tensorflow/Linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl
/********************************************/
java -classpath kmeans -jar kmeanscolumn.jar  //windows 命令执行jar
/********************************************/
利用crontab执行linux周期性任务
crontab -e  //编辑，操作类似vi
*	*	*	*	*	command
分	时	日	月	周	命令
第1列表示分钟1～59 每分钟用*或者*/1表示（*/n表示每n分钟执行一次）
第2列表示小时1～23（0表示0点）
第3列表示日期1～31
第4列表示月份1～12
第5列标识号星期0～6（0表示星期天）
第6列要运行的命令或程序路径

55 10 * * *  /home/gzm/jar1.sh
57 10 * * *  /home/gzm/jar2.sh
表示周期性调用两个shell
其中jar2.sh为标准shell
#!/bin/sh
spark-submit --class homenet --num-executors 20 --executor-cores 4 --executor-memory 10g --master yarn-client /home/gzm/test/homenet.jar hdfs://10.0.40.2:8020/user/gzm/HomeNet/output/k_out/ou
t1 hdfs://10.0.40.2:8020/user/gzm/HomeNet/input/Idcode.txt hdfs://10.0.40.2:8020/user/gzm/HomeNet/output/h_out
/***********************修改Windos域名映射表*********************/
vi /etc/hosts  //linux中hosts路径
用文本编辑器打开以下文件：
C:\Windows\System32\drivers\etc\hosts
【\t】10.0.40.5【\t】server04.bigdata  //其中的Tab分隔符不好加上，最好每个映射独占一行
/********************************************/
可以简单的把函数认为是包裹了一条或多条语句的代码块，这个代码块接受一个或若干个参数，经过代码块处理后返回结果！
Scala中函数是一等公民，可以当参数传递、被赋值、被赋给变量，这是因为函数底层是类和对象，在运行时函数是变量。这些类和对象是Scala语言
自身预定义的，同时，这些类和对象天然是可序列化和反序列化的，意义重大：
{
	 1.Scala函数的天然可序列化和反序列化特性使得函数可以方便的在分布式系统上传输！
	 2.由于函数底层是类和对象，所以函数可以适用于变量的任何场景，如被传递、被返回、被赋值、被赋给变量。
}
要点：
1.用def关键字定义函数
2.函数会自动进行类型推断【前提是函数签名与函数体间有等号】来确定返回值的类型（即最后一行语句的值），也就是说可以不显示声明返回值类型，
	var fun0 = (name: String) => {println("hello " + name)} //此时的函数不能声明默认参数和显示类型
3.当scala不能推断函数返回值类型时，必须显示声明函数类型。
4.函数参数可以有默认值，在调用这样的函数时而不传参，函数将用默认值来执行。
	def fun(name: String = "young", age: Int): Unit = {
		println("hello " + name)
	}
5.针对声明了多个参数的函数，在调用过程中可以显示的写出参数名，从而忽略参数顺序，底层原因函数是对象，参数是其成员变量，故顺序不重要。
	fun(age = 12, name = "lola");
6.如果声明函数时不能确定参数个数，可以用可变参数
	def sum(numArr: Long*): Long ={
		var result: Long = 0
		for(it <- numArr){
		  result += it
		}
		result
	}
	print(sum(1, 2, 3)  //完整元素调用
	print(sum(0L to 10L: _*))  //新式语法调用，意义同上
7.可变参数中的数据会被收集为Array，main方法的args: Array[String]也是这种。
/********************************************/
--deploy-mode [client/cluster]  //配置drive的，client：driver运行在提交命令的这个节点上，cluster：由集群管理器将driver运行到集群中某个一个节点上。
/********************************************/
function f(){}  //函数声明
var f1 = function f(){}  //表达式（非匿名）
var f2 = function(){}  //表达式（匿名）

在Javascript中，一对小括号【()】是一种运算符，跟在函数名之后，表示调用该函数，所以我们可以通过函数表达式后面跟上小括号表示调用该函数。
那么问题来了，我们是否可以在函数声明之后立即调用呢？ 就像这样：
function f(){
	console.log('test');
}();
答案是NO。

原因是：JavaScript引擎规定，如果function关键字出现在行首，一律解释成语句。因此，JavaScript引擎看到行首是function关键字之后，
认为这一段都是函数的定义，不应该以圆括号结尾，所以就报错了。那我们想要让函数声明之后立即被调用，只需要不让function出现在行首就行了，
这就是IIFE是怎么出现的了。
/********************************************/
JavaScript中没用私有作用域的概念，可以使用立即执行函数模仿一个私有作用域达到隔离的目的，用匿名函数作为一个“容器”，“容器”内部可以访问外部的变量，
而外部环境不能访问“容器”内部的变量，所以( function(){…} )()内部定义的变量不会和外部的变量发生冲突或覆盖，俗称“匿名包裹器”或“命名空间”。
立即执行函数：(function(){…})()和(function(){…}())，
要立即执行，函数必须是函数表达式，不能是函数声明，后面那对空小括号是运算符，其作用就是将函数声明显示转换为表达式。
/********************************************/
(function(){
	var printWindow = function(){
		window.print();  //js原生窗口打印函数，兼容性挺好
	};
	$().ready(function(){
		$("#printBtn").click(function(){
			printWindow();
		});
	});
}())
/********************************************/
idea激活站点
http://idea.iteblog.com/key.php  //idea激活站点
https://jetlicense.nss.im/  //clion激活站点
http://idea.uri.ci  //Pycharm激活站点
/*****************if-else***************************/
【大括号包裹的语句块是有值的，其值及值得类型由块中最后一条语句推断】
【类型推断通过变量的值的类型来确定变量的类型，在复杂算法实现中可以省略很多变量类型的声明，可以带来极大的便利】
控制结构（顺序、条件、循环）
1.if相关条件表达式是有值的，其值是根据true句柄返回的值：var value = if(1 > 0) 1 else 0
val age = 100;
var value0 = if(age > 100){101}else{99}  //语句两个结果类型相同，类型推断if语句的值value0类型为Int
var value1 = if(age > 100){"大于100"}else{99}  //语句两个结果类型不同，类型推断if语句的值value1类型为两个类型的父类Any
var value2 = if(age < 20) "all"  //语句只显示写出了一种可能的值，但其后有隐式else()语句其类型为Unit，类型推断变量value2类型为两个类型的父类Any
【由于隐式else()的存在，为避免返回()这种非常规值，一般需要返回自定义“空值”来显示声明else语句。
2.当if语句块包括多条语句时可以用大括号包裹块，块的结果值是块的最后一条语句的值。
	var x = 0
	var y = 0
	var value3 = if(age > 10){
		x += 1
		y += 1
		x + y
	}else 0
	println(value3)
/******************for**************************/
for(it <- 0 to 10 if(it > 8)){  //增强型for遍历，同时带过滤条件，（此处if条件语句用作for的条件守卫来限制结果）
  println(it)
}
"kkk222".foreach(println(_))  //遍历串中的字符
for(it <- "kkk222"){println(it + 10)}  //遍历串中的字符并运算，以字符的ASCALL码参与运算
for(it <- "kkk222"){println(it + "10")}  //遍历串中的字符并运算，以字符拼接的方式运算
(0 to 100).foreach(println(_))  //遍历数组中的元素，迭代器不可参与运算
(0 to 100).foreach(it => println(it + 3))  //遍历数组中的元素，迭代器可以参与运算
{//同义
	"hello world".split(" ").foreach(println(_))
	for(it <- "hello world".split(" ")){println(it)}
}

for(it <- 0 to 50 if(it < 30)){
	if(it % 2 == 0){
		println("偶数")
	}else{
		println("奇数")
	}
}
val x = 6
val a = if(x > 0) 1 else -1  //类似java的三目运算符(Scala中没有三目)
-----------------------------------------------------------------------------
for循环支持多个Range和条件守卫，多个Range相当于【嵌套】循环
for(i <- 0 to 5 if(i % 2 != 0); j <- 6 to 10 if(j % 2 == 0)){
	println(s"$i--$j")
}
-----------------------------------------------------------------------------
想跳出for循环，除了用if作为条件守卫，还可以用return来跳出，但会跳出方法块。
var sum = 0;
for(it <- 0 to 10){
  sum += it;
  println(sum);
  if(sum > 6){
	return;
  }
}
如果要用break需要引入包
import scala.util.control.Breaks._
breakable{  //必须包裹在这个块中
  var sum = 0;
  for(it <- 0 to 10){
	sum += it;
	println(sum);
	if(sum > 6){
	  break();
	}
  }
}

for(it <- (0 until(10, 2))){println(it)}  //参数2为until的步长参数，表示每个两个取一次值
for(it <- (0 to 10).reverse){println(it)}  //遍历反转后的数组
/**********************while**********************/
while(flag){
	println("dd")
}

var num = 1
do{
	println(num)
	num -= 1
}while(num >= 0)
/********************************************/
【大数据技术是指数据集合及对集合操作的总称，在Spark中很多集合操作算子直接沿用自Scala】
1.定长数组：Array【初始化、读、写及遍历】，长度不可变，但元素值可变
	val arr0 = new Array[Int](5)  //初始化
    arr0(0) = 10  //写元素
    println(arr0(0))  //读元素

    val arr1 = Array[Int](0, 1, 2, 3, 4, 5)  //初始化，底层调用了apply方法【最常用】
    arr1(1) = 3  //写元素
    arr1.foreach(println(_))  //遍历读元素

    val arr2 = Array(0, 1, 2, 3, 4, 5)  //初始化，底层调用了apply方法，省略泛型类型推断【最常用】
    arr2(1) = 3  //写元素
    arr2.foreach(println(_))  //遍历读元素

    val arr3 = Array.apply(0, 1, 2, 3, 4, 5)  //初始化
    arr2(1) = 3  //写元素
    for(it <- arr3){println(it)}  //遍历读元素
	println("size:" + arr3.size)  //取长度
    println("length:" + arr3.length)  //取长度
	
	var sum0 = (0 to 10).sum  //求和
	var sum1 = Array(1,2,3).sum  //求和
	var max = Array(1,2,3).max  //最大值
	var min = Array(1,2,3).min  //最小值
	var reverse = Array(1,2,3).reverse  //反转数组
	
	var sortedArr = Array(2,34,2,5,3)
    println(sortedArr.mkString(","))  //将数组转换为字符串，指定逗号为分隔符
    scala.util.Sorting.quickSort(sortedArr)  //升序排序
    println(sortedArr.mkString(","))
    println(sortedArr.mkString("Head", "separator", "Tail"))  ////将数组转换为字符串，指定分隔符、头部、尾部
    println(sortedArr.reverse.mkString(","))  //不会修改原数组

    var reversedArr = sortedArr.reverse
    println(reversedArr.mkString(","))

    println(sortedArr.mkString(","))
    println(sortedArr.toString)  //打印数组对象的hashCode

    //如果想从已有数组通过对每个元素的相同操作来得到新数组，可以用yield语法实现。
    //1.yeild操作不修改原数组，非常适合大数据。
    //2.spark中的Rdd的某些转换操作就是基于yield操作实现的。
    var arr5 = Array(1,2,3,4,5,6,7)
    var addedArr = for(it <- arr5)yield{
      it * 2
    }
    println(addedArr.mkString(" "))

    var evenArr = for (it <- Array(1,2,3,4,5,6,7) if((it % 2) == 0)) yield {
      it
    }
    println(evenArr.mkString(" "))

    var arr6 = Array(1,2,3,4,5,6,7)
    var oddArr0 = arr6.filter(it => {it % 2 > 0})
    println(oddArr0.mkString(" "))

    var oddArr1 = arr6.filter(_ % 2 > 0)  //默认迭代器语法
    println(oddArr1.mkString(" "))

    var oddArr2 =arr6.filter(_ % 2 != 0).map(_ * 2)
    println(oddArr2.mkString(" "))
	
---------------------------------------------------------
	val arr = Array(1 to 100: _*)
	val init = arr.init; //获取数组除队尾元素外的其余元素
	val rdd = sc.parallelize(arr)
	val Array(a1,a2,a3) = rdd.randomSplit(Array(0.7, 0.15, 0.15)); //将已知rdd按照 比例 随机切分为三个小数组，将产生三个新变量a1、a2、a3
---------------------------------------------------------
List操作
val intList = List(1,2,3)  //声明一个list，并为之赋值
val head = intList.head  //首个元素
val tail = intList.tail  //除了首个元素外的余下元素集合

val myList1 = 0::myList  //将一个新元素右结合原list生成新的list，List(0,1,2,3)
val intList = 3::2::1::Nil  //也可以这样初始化一个List，其中Nil表示空list

val intList1 = List(1,2)
val intList2 = List(3,4)
val intList3 = intList1:::intList2  //结合两个List生成新list
---------------------------------------------------------
2.变长数组：ArrayBuffer
	val arrBuffer0 = ArrayBuffer[Int]()  //初始化
    arrBuffer0 += 2  //默认尾部追加单个元素
    arrBuffer0 += (4, 5, 6)  //默认尾部追加多个元素
    arrBuffer0 ++= Array(7, 8, 9)  //默认尾部追加数组
    arrBuffer0.insert(0, 33, 44)  //定点插入元素，第一个参数代表索引，后续为可变参数
    arrBuffer0.insertAll(2, Array(6, 6, 6, 6, 6))  //定点插入数组
    arrBuffer0.foreach(print(_))

    println("length:" + arrBuffer0.length)
    arrBuffer0.remove(0)  //定点删除单个元素
    arrBuffer0.remove(0, 3)  //定点删除多个个元素，删除该点及气候的多个元素
    arrBuffer0.foreach(print(_))
    println("size:" + arrBuffer0.size)
    println("index4:" + arrBuffer0(4))
    val arr4 = arrBuffer0.toArray
	
/********************************************/
class ObjectOrientedProgramming {
  var name: String = "youn"
  var sayId = (id: String) => {println(id)}
  def sayAge (age: Int = 28): Unit = {println(age)}

}

object ObjectOrientedProgramming {

  def main(args: Array[String]): Unit = {
//    var OOP = new ObjectOrientedProgramming
    var OOP = ObjectOrientedProgramming()
    println(OOP.name)
    OOP.sayId("043075")
    OOP.sayAge()
  }

  def apply(): ObjectOrientedProgramming = {
    new ObjectOrientedProgramming()
  }
}
1.对象是弱耦合的并且是消息或数据驱动的。
2.对象具有独立性。
3.面向接口编程。
Java和Scala不是面向对象的语言，只是具有封装、继承、多态等特征来支持面向对象功能。
函数不依赖类，方法显式地依赖类，过程是没有返回值的函数。
1.如果class与object同名，这个object中的所有成员都是class的静态内容，所以可以【不实例化】class就调用object中的方法或变量。
  可以通过调用object中的特定方法——apply来实例化类，object中的apply方法是class的工厂方法，用于class的实例化。
2.在很多成熟框架中，一般直接调用抽象类的伴生对象的apply方法来来实例化抽象类：
	一、秘诀在于类的伴生对象的apply方法拥有对类的实力的一切生杀大权，抽象类不能直接实例化，但可以通过半生对象的apply
		方法实例化子类，比如抽象类Grahp不能实例化，其伴生对象的apply方法实际上是通过实例化了其子类GraphImpl来达到实例化Graph的。
		当然在Spark中，GraphImpl的实例化也是通过调用父类Graph的伴生对象的apply方法来完成的。
	二、这种方式的优点在于更能对应代码版本迭代和代码修改，这是更先进的面向接口编程实现。
3.伴生类可以访问伴生对象的所有成员，伴生对象也可以访问伴生类的所有成员（收private、this修饰的成员除外）。
4.在定义Scala的Class时，可以在类名后的()中加入构造参数，此时apply方法必须也要定义出该参数。
5.伴生对象中可以定义多个apply方法，即支持重载。
/********************************************/
Map和Tuple
val bigData = Map("Spark" -> "DataBricks", "Hadoop" -> "apache")  //初始化不可变map
println(bigData.get("Spark"))
for(it <- bigData){  //遍历读取键值对
  println(it)
}
bigData.foreach(println(_))  //遍历读取键值对

val carMap = Map(("ford", "good"), ("byd", "bad"))  //【另一种初始化不可变map的方法
carMap.foreach(println(_))
-------------------------------------------------------------
val university = Map("XMU" -> "Xiamen University", "THU" -> "Tsinghua University","PKU"->"Peking University")
for((k,v) <- university){  //遍历键值对
	printf("Code is : %s and name is: %s\n",k,v)
}

for(k <- university.keys){  //遍历键
	println(k)
}
for(v <- university.values){  //遍历值
	println(v)
}

university.foreach(case(k, v) => println(k+":"+v))
university.foreach{kv => println(kv._1 + ":" + kv._2)}
-------------------------------------------------------------

val lang = scala.collection.mutable.Map("java" -> 20, "scala" -> 10)  //可变map，即使被val修饰也可以添加、更新
lang("java") = 21  //通过键更新值
lang("shell") = 30
lang.foreach(println(_))  //遍历读取键值对

val myMap = new mutable.HashMap[String, Long]  //如果想直接new出实例，则必须用HashMap等子类
myMap("python") = 40L
myMap += ("ee" -> 4)  //追加键值对
for(it <- myMap){
  println(it)
}
for((k, v) <- myMap){  //用key-value形式遍历
  println(k + "=" + v)
}

println(myMap.getOrElse("ruby", 34))  //如果key不存在则返回第二个参数，可以防止报异常，还可以提供本次的默认值“34”（实际开发很有用）
println(myMap.get("ruby") + "-------------")  //上面的34只是作为单个语句的返回值，并没有放到map中
myMap("ruby") = 100
println(myMap.getOrElseUpdate("ruby", 12))  //如果key存在则返回旧的值，否则向map中添加新k-v对
println(myMap.get("ruby") + "-------------")  //由于key“ruby”在上一句之前就存在，所以此时取的是旧指“100”

for(it <- myMap.keySet){
  println(it)
}
for(it <- myMap.values){
  println(it)
}
【for推导式】通过for循环遍历一个或多个集合，对集合中的元素进行“推导”，从而计算得到新的集合
val fromMyMap = for ((k, v) <- myMap) yield {  //在旧map的基础上生成新map
  ("new" + k, v + 1)
}

//SortedMap-取出时根据键升序
val sortedMap = scala.collection.immutable.SortedMap("yuwen"-> 98, "22"-> 70, "english"-> 85, "al"-> 85)  //按键升序
for((k, v) <- sortedMap){
  println(k + "=" + v)
}

//SortedMap-可以记住键值对的存入顺序，遍历时按存入顺序取出
val likedHashMap = scala.collection.mutable.LinkedHashMap("fcb"-> 98, "rm"-> 70)
likedHashMap += ("chel" -> 90)
likedHashMap.foreach(println(_))
----------------------------------------------------------
Tuple是可以存放各种不同数据类型元素的集合，但起始位置是1。
myTuple.foreach(it => println(it._1 + "	" it._2))
当下划线【_】作为集合的迭代临时变量时，仅仅只是一个简陋迭代器，缺失高阶属性和函数，不能访问。
如果需要访问高阶属性或函数，需要自定义临时迭代器变量【it】。
/********************************************/
UNION 对两个结果集进行并集操作，会去除重复数据
UNION ALL，对两个结果集进行并集操作，重复数据全部保留
-----------------------------------------------------------
val: String myStr = "333"
println(s"nihao $myStr")  //自定义格式并引用变量

printf("hello Iam\t%s", myStr)  //C语言风格的打印输出
-----------------------------------------------------------
【将内容写入外部文件】
import java.io.PrintWriter
val out = new PrintWriter("out.txt")  //在REPL中没有特殊指定文件路径在当前用户根目录

for(i <- 0 to 10 by 1){
	out.println(i)
}
out.close()  //必须调用writer的close方法，否则字符流不会被真正写入到文件中
-----------------------------------------------------------
【将文件写到外部文件】
val writer = new PrintWriter(new File("test.txt" ))
writer.write("菜鸟教程")
writer.close()
-----------------------------------------------------------
【读取外部文件】
import scala.io.Source
val in = Source.fromFile("output.txt")
val lines = in.getLines  //返回的是一个迭代器
lines.foreach(println)
-----------------------------------------------------------
Range
val myRange0 = 1 to 5  //创建一个封闭range
val myRange1 = 1 until 5  //创建一个左闭右开range
val myRange2 = 1 to 5 by 2  //创建一个指定【步长】封闭range
val myRange3 = 0.01f to 0.09f by 0.02f  //创建一个指定【步长】封闭，【float】range

myRange3.foreach(println)  //当集合元素是简单类型时，println函数可以无参
-----------------------------------------------------------
SELECT
	COUNT(0) AS all_count
FROM
	model_au_apply au
WHERE
	au.MODEL_ID = :modelId
AND au.AU_TYPE = "07"
AND (
	au.`STATUS` = "01"
	OR au.`STATUS` = "00"
)
UNION SELECT
	COUNT(0) AS all_count
FROM
	basemodel b
WHERE
	b.modelId = :modelId;
------------------------------------------------------------
SELECT
	SUM(iv.countt) AS all_count
FROM(
	SELECT
		COUNT(0) AS countt
	FROM
		model_au_apply au
	WHERE
		au.MODEL_ID = "4944ae0075bb4bd8ac845d2ded22d42b"
	AND au.AU_TYPE = "07"
	AND (
		au.`STATUS` = "01"
		OR au.`STATUS` = "00"
	)
	UNION ALL SELECT
			COUNT(0) AS countt
		FROM
			basemodel b
		WHERE
			b.modelId = "4944ae0075bb4bd8ac845d2ded22d42b"
	) iv
/********************************************/
1.配置项目的web.xml
<!-- 加载log4j的配置文件log4j.properties -->
<context-param>
	<param-name>log4jConfigLocation</param-name>
	<param-value>classpath:log4j.properties</param-value>
</context-param>

<!-- 设定刷新日志配置文件的时间间隔，这里设置为10s -->
<context-param>
	<param-name>log4jRefreshInterval</param-name>
	<param-value>10000</param-value>
</context-param>

<!-- 加载Spring框架中的log4j监听器Log4jConfigListener -->
<listener>
	<listener-class>org.springframework.web.util.Log4jConfigListener</listener-class>
</listener>

<context-param>
	<param-name>webAppRootKey</param-name>
	<param-value>api</param-value>
</context-param>

2.配置log4j.properties
log4j.appender.R.File=${api}/WEB-INF/log/api.log
/********************************************/
Eclipse中tomat的项目发布目录：
F:\develop\workspace\.metadata\.plugins\org.eclipse.wst.server.core\tmp3\wtpwebapps\m-api
/********************************************/
map函数会对源RDD每一个输入元素进行指定的操作，然后为每一条输入元素返回一个结果对象作为目标RDD的一个元素，目标RDD与源RDD元素个数相同。
而flatMap函数则是以下两个操作的集合——正是“先映射（map）后扁平化（flat）”：
map：同map函数一样：对每一条输入进行指定的操作，然后为每一条输入返回一个集合对象
flat：最后将所有集合对象瓦解，进而合并为一个大集合对象，这个大集合对象就是目标RDD
/********************************************/
Linux-shell中获取（全局）变量的值：
echo $JAVA_HOME
/*****************ML***************************/
P(A/B)=P(B/A)*P(A)/P(B)，
该公式表示在B事件发生的条件下A事件发生的条件概率，
等于A事件发生条件下B事件发生的条件概率乘以A事件的概率，再除以B事件发生的概率。
公式中，P(A)也叫做先验概率，P(A/B)叫做后验概率。严格地讲，贝叶斯公式至少应被称为“贝叶斯-拉普拉斯公式”。
/********************************************/
转化操作：返回的是新的RDD，转化出来的RDD是惰性求值的，只有在行动操作中用到这些RDD时才会被计算。
行动操作：它们会把最终求得的结果返回到驱动器程序，或者写入外部存储系统中。
		  由于行动操作需要生成实际的输出，它们会强制执行那些求值必须用到的RDD的转化操作。
/********************************************/
基本Rdd操作
Transform{
	1.针对各个元素的转化操作{
		map(function)
		filter(function)
		flatMap(function)
		sample(isReplacement, fraction, [seed])
	}
	2.伪集合操作{
		union(Rdd)
		distinct(Rdd)  //开销大
		intersection(Rdd)
		subtract(Rdd)
		cartesian(Rdd)
	}
}
Action{
	reduce(function)  //无零值规约
	fold(zeroValue)(function)  //有零值规约
	aggregate(zeroValue, zeroValue)(function)
	collect()
	take(num)
	top(num)
	count()
	foreach(function)
	takeOrdered(num)(function)
}
/********************************************/
PairDdd操作
转化操作{
	单个PairRdd{
		reduceByKey(func)
		groupByKey()
		combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner)
		mapValues(func)
		flatMapValues(func)
		keys()
		values()
		sortByKey()
	}
	两PairRdd{
		subtractByKey
		join
		rightOuterJoin
		leftOuterJoin
		cogroup
	}
}
行动操作{
	countByKey()
	collectAsMap()
	lookup(key)
}
/********************Cache************************/
数值型Rdd{
	count()  //RDD 中的元素个数
	mean()  //元素的平均值
	sum()  //总和
	max()  //最大值
	min()  //最小值
	variance()  //元素的方差
	sampleVariance()  //从采样中计算出的方差
	stdev()  //标准差
	sampleStdev()  //采样的标准差
}

1.Spark的数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。
  这些 统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatsCounter对象返回。
2.如果你只想计算这些统计数据中的一个，也可以直接对 RDD 调用对应的方法，比如rdd. mean()或者rdd.sum()。

举例{
	val distanceDouble = distance.map(string => string.toDouble)
	val stats = distanceDoubles.stats()  //获取StatsCounter对象
	val stddev = stats.stdev
	val mean = stats.mean
	val reasonableDistances = distanceDoubles.filter(x => math.abs(x-mean) < 3 * stddev)
	println(reasonableDistance.collect().toList)
}
/********************Cache************************/
	级别			使用的空间	CPU时间		是否在内存中	是否在磁盘上	备注
MEMORY_ONLY				高		低				是				否
MEMORY_ONLY_SER 		低		高				是				否
MEMORY_AND_DISK			高		中等			部分			部分		如果数据在内存中放不下，则溢写到磁盘上
MEMORY_AND_DISK_SER 	低		高				部分			部分		如果数据在内存中放不下，则溢写到磁盘上。在【内存】中存放序列化后的数据
DISK_ONLY				低		高				否				是
/********************************************/
/********************************************/
回归分析
1.定义：回归分析就是利用样本（已知数据），产生拟合方程，从而（对未知数据）迚行预测。
2.用途：预测，判别合理性。
3.1线性回归分析：一元线性（若干自变量、一个因变量、一次，直线拟合）、多元线性（若干自变量、多个因变量、一次，面拟合）、广义线性（可以转化为线性的非线性问题）。
3.2非线性回归分析：高阶曲线拟合。
4.困难：确定变量（多元），避免多重共线性，观察拟合方程，避免过度拟合，检验模型是否合理。

1.过拟合会将误差也拟合到曲线上。
2.在线性回归中：回归曲线=回归方程=回归模型

函数关系：两者之间有直接关系，是一种确定性关系，y=3+10*x。
相关关系：两者之间没有直接关系，但是存在着某种正向或负向的相关，是一种非确定性关系。

1.使用相关系数去衡量线性相关性的强弱，用柯西不等式可以证明其结果取值范围是[-1, +1]，值越接近两端+1、-1相关性越强，也就越适合用直线拟合，
	如果相关系数为正则表明呈正相关，为负则表明呈负相关。
2.用最小二乘法求解回归参数。
/********************************************/
分类
	根据训练集构造出判别函数，用判别函数预测测试数据。
	用处：{事物归类、预测、决策}
	原理：线性判别法、距离判别法、概率判别法
/********************************************/
聚类
	无训练集。
/********************************************/
分类的结果是非数值型，回归的结果是数值型。
/********************************************/
K最近邻法 KNN：假设一个样本空间里的样本已分成几个类型，然后给定一个待分类样本，
	通过计算距离该待分类样本最近的K个样本(混合样本群)来判断这个待分类数据属于哪个分类。
	简单的说，就是由那些离自己最近的K个样本点【投票】决定待分类样本归为哪一类。
KNN算法描述：
1.计算出样本数据和待分类数据的距离。
2.为待分类数据选择K个与其距离最小的样本。
3.统计出K个样本中【多数】样本所属的分类。
4.这个分类就是待分类数据所属的分类。
注意：K应该设置为一个奇数，这样可以保证投票的时候不会有平票。
/********************************************/
卡方检验可以用来做特征选择，过滤掉相关性小的特征，属于降为算法。
/********************************************/
为了让所有东西都是对象的目标更加一致，也为了遵循函数式编程的习惯，
Scala鼓励你在变量和函数返回值可能不会引用任何值的时候使用Option类型。
在没有值的时候，使用None，这是Option的一个子类。如果有值可以引用，
就使用Some来包含这个值。Some也是Option的子类。
/********************************************/
withReplacement：表示抽样是否放回，如果有放回抽样有重复，反之则无重复。
fraction：表示采样率，是一个[0, 1]之间的实数。
seed：随机种子，用以控制抽样的随机起始位，如其值值为3，起始位置就可能是1、2、3。
sample(withReplacement: Boolean, fraction: Float, seed: Int)  //从数据集中随机抽样，
/********************************************/
连续数据离散化：比如原始特征的取值范围是[0, 10]的连续实数（保留2位小数），
此时，为了方便后续处理，可将原始特征数据分为10级来表示：
0.01至0.49划为0级
0.50至1.49划为1级
 .  至 . 划为 .
 .  至 . 划为 .
 .  至 . 划为 .
9.01至10.0划为10级
这样就可以用对应的离散整数来表示了。
/********************************************/
【A master URL must be set in your configuration】错误解决方法：
点击edit configuration，在左侧点击该项目。在右侧VM options中输入“-Dspark.master=local”
/********************************************/
普通统计分析工具如R、Weka、Saas、SPSS等单机软件，在分析数据时往往需要将数据一次性加载到内存，
当数据量大于单机内存时将无法处理，此时抽样法可以解决部分问题，但是不能解决分类、聚类、推荐这些需要全量数据参与的问题。
样本独立性强的数据更加适合分布式分析。
/********************************************/
方差越小，表示样本离均值越汇聚，整个样本取值也越集中，反之，离均值越离散，整个样本取值越分散。
/********************************************/
问题：1.不同app间无法共享同一内存数据，必须有磁盘io。
	  2.如果driver挂掉，cached数据会全部丢失。
Tackyon：1.是一种分布式内存文件系统，实现不同应用或框架（Spark、Hadoop）间高效可靠地共享内存数据，以提高内存的利用率、避免多余磁盘io。
		 内存中只有一份数据，如果数据丢失，通过lineage重算来生成。
		 2.应用挂掉后，driver控制的数据对象会丢失，但tachyon中的不会。
		 3.基于Master-Worker架构，用Zookeeper保证master的HA，基于thrift服务调用。
		 4.Journal = image + edit;  //日报记录整个系统中文件的变动。
		 5.Tackyon中的Lineage与Spark的类似，只是不同于Spark中每个节点是RDD，Tachyon中的每个节点是文件，它记录了文件的谱系图，
		 文件的重算也是通过这种谱系图来实现的，checkpoint最后一个节点。
		 6.WriteType分为几种{
			MUST_CACHE  //必须cache
			TRY_CACHE  //尽量cache
			CACHE_THROWN  //cache，同时写到底层磁盘文件系统
			THROWN  //只写到底层磁盘文件系统不cache
			ASYSC_THROWN  //调用后台线程异步写到底层磁盘文件系统
		 }
		7.ReadType分为两种{
			NO_CACHE  //将数据读进应用同时cache
			CACHE  //仅仅读取数据不缓存
		}
		8.部署模式{
			local{
				
			}
			cluster{
				slaves中需要填入集群中slave的hostname或ip 
				tachyon-env.sh修改如下配置(JAVA_HOME当然也要配置正确): 
					export TACHYON_MASTER_ADDRESS=hostname
					export TACHYON_WORKER_MEMORY_SIZE=50GB
					export TACHYON_UNDERFS_HDFS_IMPL=org.apache.hadoop.hdfs.DistributedFileSystem
					export TACHYON_UNDERFS_ADDRESS=hdfs://hostname:port
			}
		}
使用场景：
原理：
部署：
结合spark：
/********************************************/
1.特质：类似Java接口，但是可以有自己的属性还能实现部分方法。
2.抽象类：遵从Java抽象类。
3.Scala类任然遵从Java的单继承-多实现原则。
4.优先使用Trait：一个类可实现多个特质，但却也可以多继承一个抽象类。
5.当需要继承带参构造函数：只能使用抽象类，因为抽象类可以定义带参数的构造函数，而特质不行。
	例如：你不能说trait t(i: Int){}，参数i是非法的。
/********************************************/
所有的迭代器都只能被访问一次。
/********************************************/
1.数据调研
2.需求分析
3.技术方案设计
	基于现有数据，针对提出需求。
4.数据结构设计
5.编码实现
	模块功能开发
	本地测试
	生产环境测试
6.性能调优
/********************************************/
Spark Optimize
一、增加或分配更多计算资源
	性能调优的王道：增加或分配更多资源是最直接有效的提速方式，在一定范围内，资源量与性能的提升是成正比的。
	一个app实现功能后，首先，应当为其分配最有的资源配置，当资源分配到达极限后，再考虑后续性能调优。
	{
		1.分配哪些资源。{
			1.执行器个数：--num-executors 20
			2.每个执行器的核心数：--executor-cores 4
			3.每个执行器的内存大小：--executor-memory 10g
			4.驱动器内存大小：--driver-memory 100m  //不是太重要
		}
		2.在哪里分配资源。{
			在spark-submit命令中配置相应参数。
		}
		3.分配多少算合适：尽量分配最多的资源。{
			1.Standalone集群，弄清集群主机资源存量。
				例如：20台主机，每台4G内存，每台2个核心，可如下分配资源：
				--num-executors 20 --executor-cores 2 --executor-memory 4g
			2.yarn集群，有资源队列机制，所需资源从队列中分配，需要弄清目标资源队列的指标。
				例如：如某资源队列有500G内存，100个核心，可如下分配资源：
				--num-executors 50 --executor-cores 2 --executor-memory 10g
		}
		4.为什么分类了这些资源后性能会得到提升。{
			1.执行器数量越多，能并行执行的任务就越多，应用的并行度就越高，Spark调度器会将算子切分成大量的任务，然后提交到各个执行器上。
			2.增加每个执行器的核心数，也是提高并行度，有多少个核心，就能并行多少个任务。
			3.涉及RDD持久化的操作，如果执行器拥有更大的内存就能减少磁盘IO，数据处理速度就会更快。
				涉及混洗的操作，在reduce端需要内存来存放汇聚过来的数据，此时更大的内存可以减少磁盘IO，从而提高处理速度。
				在任务执行过程中，会创建大量的对象，如果执行器拥有更大的内存，就能降低JVM垃圾回收频率，从而提高性处理速度。
				
		}
	}

二、调节应用并行度
	并行度：一个作业中各个Stage同时进行的任务数量。
	资源充足的前提下，如果不人为匹配并行度或者并行度过低，就会造成资源的闲置与浪费，合理的配置应该是足够大，大到资源最大值。
	1.理想状态，Task数量至少设置为App总核心数。
	2.官方推荐，Task数量设置为App总核心数的2~3倍，因为有些Task处理耗时一些。
	3.如何设置App并行度，在代码中sparkConf.set("spark.default.parallelize", "300")
		比如集群有20台主机，共100个核心，分配20个执行器，那么设置300个Task，每个执行器就会分配到15个任务，
		每个执行器上5个核心共同完成分配到其上的15个任务。
	
	例如：20台主机，每台4G内存，每台2个核心，可如下分配资源：
		--num-executors 20 --executor-cores 2 --executor-memory 4g
		如果此时task数量设置为20，那么将有20个线程同时执行，就只利用了20个核心，其余20个闲置着。
		【
			并行度：集群不能有效的被利用，除非为每一个操作都设置足够高的并行度。
			Spark会根据每一个文件的大小自动设置运行在该文件“Map"任务的个数（你也可以通过SparkContext的配置参数来控制）；
			对于分布式"reduce"任务（例如group by key或者reduce by key)，则利用最大RDD的分区数。
			你可以通过第二个参数传入并行度（阅读文档spark.PairRDDFunctions ）或者通过设置系统参数spark.default.parallelism来改变默认值。
			通常来讲，在集群中，我们建议为每一个CPU核（core）分配2-3个任务。
		】

三、RDD持久化重构
	1.公共RDD持久化
	{
		由于在默认情况下，对一个RDD的Action操作会从头计算该RDD的所有祖先RDD，
		所以，当谱系图中某个RDD存在被多次计算时，其祖先RDD也会一并被重算，
		此时就必须考虑持久化复用。
	}
	2.相似RDD合并抽象
	{
		当一个应用中的两个RDD逻辑和数据都相同或相似时，应当尽量抽象为同一个，并持久化复用。
	}
	3.先序列化再持久化的
	{
		1.将一个RDD的每个分区序列化为一个大字节数组对象，然后通过BlockManager存到内存或磁盘，减少持久化对象对内存的占用，以避免磁盘IO，但是会涉及反序列化开销。
		2.如果序列化后内存还是不够用，那就不序列化，补充磁盘IO。
		3.内存加磁盘，序列化
		4.高可用，双副本持久化，在【内存充裕】的前提下，可以将一个持久化副本存到其他节点上，以提高可用性。
	}

四、广播大对象
	问题来源：公用外部对象会复制分发到每一个Task，当并行度很高时，大量副本对象会造成网络传输、存储、GC压力，当对象很大时，尤其严重。
	解决方法
	{
		1.广播对象首先只会在Driver进程上有一个原始母本，
		  当某个Task需要时，该Task会到其BlockManager处获取，如果没有，该BlockManager会去Driver或其他Executor的BlockManager获取。
		  当同一个Executor上的下一个Task也需要该广播对象时，就直接从该BlockManager处获得了。
		2.广播对象的个数就从Task数量级降低到了Executor数量级，在传输、存储、GC压力方面有较大的优化。
	}

五、Kryo序列化
	1.Java流的方式序列化数据，好处在于操作方便，只需对象实现Serirlizable接口，但是处理速度和数据压缩比都不高，Spark默认使用。
	2.Kryo序列化机制在处理速度和数据压缩比方面都比Java流高，但操作不方便。
	{
		1.算子函数中使用到的外部变量。
		2.持久化时，如果选择了序列化策略，会使用Kryo机制。
		3.Shuffle时，在进行Stage间的Task的Shuffle操作时，节点间的Task会进行大量的网络数据传输，此时会用Kryo序列化机制
	}
	3.如何使用Kryo序列化机制
	{
		val conf = new SparkConf()
		.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")  //声明作业使用Kryo序列化
		.registerKryoClasses(Array(MyPic.getClass, MyKmeans.getClass))  //将需要用Kryo序列化的类以数组的方式注册进来
	}
/********************************************/
1.Master和Worker是针对节点而言的。
2.Driver和Executor是针对线程而言的。
3.一个Worker节点上可以启动多个Executor线程。
4.每个作业只有一个Driver进程，他是应用jar包运行起来的那个进程。
5.Master节点上既可以运行Dirver又可以运行Executor进程，但Worker节点上只能运行Executor进程。
/********************************************/
每个Executor拥有一个BlockManager，它负责管理本Executor拥有的那部分内存和磁盘中的数据。
/********************************************/
一个App称为一个作业，
一个作业中的每个Action操作对应一个个Job，
一个Job中的每个Shuffle操作对应一个Stage，
每个Stage可拆分为多个Task（一个任务其实是一个阶段流程，由一个线程执行）。
/********************************************/
1.一个文件中至少存放一个键相同的键值对集合。
2.一个文件中可能存放不同的键。
3.相同键的键值对一定在同一个文件中。
/********************************************/
数据倾斜出现的几率只有一成左右，资源分配和并行度匹配应该优先考虑。
/********************************************/
配置参数优先级：
1.最高的是在用户代码中显式调用set()方法设置的选项。
2.其次是通过spark-submit传递的参数。
3.再次是通过spark-submit加载写在配置文件conf/myConf.conf文件中的值。
4.最后是系统的默认值conf/spark-defaults.conf文件中的值。
/********************************************/
定时任务方法
public static void timedJob(){
	ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(1);  //池中只有容量为1
	scheduledThreadPool.scheduleAtFixedRate(new Runnable(){
		public void run() {
			System.err.println("定时任务开始了*****************************");
		}
	}, 1, 2, TimeUnit.MINUTES);  //首次延迟1min执行，每隔2min周期性执行
}

public static void main(String[] args) {
	timedJob();
}
/********************************************/
CloudearManager安装
1.CM服务器只需要在Master节点上安装，Slaver节点只安装客户端就可以了。
/********************************************/
Java中访问Windows路径要用【\\】作为分隔符，如：
D:\\myPath\\subPath\\kkk.txt
/********************************************/
Spring bean
<bean id="customerService" class="com.yiibai.customer.services.CustomerService" init-method="initIt" destroy-method="cleanUp">
	<property name="message" value="i'm property message" />
</bean>
/********************************************/
这两天在整Spring的JMS
我们需要动态的设定message receiver的selector选择性的接受消息
Spring的做法是要在DefaultMessageListenerContainer里设定selector的属性，可是要写在配置文件里，这样就不能动态设定了

仔细研究了下Spring的文档，发现了解决办法：
可以使用一个类的静态或非静态的方法来指定另外一个bean的属性
要用到 org.springframework.beans.factory.config.MethodInvokingFactoryBean 这个助手类
在配置文件里配置它的targetClass--指定一个类，targetMethod-指定要调用的静态方法，这个方法返回的结果作为一个bean指派给目标bean的具体属性

举一个简单的例子：
我们要动态的设定ExampleBean里的name属性，ExampleBeanHelper是其助手类，有一个静态方法getName();

//目标类
public class ExampleBean {
    private String name;

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

}

public class ExampleBeanHelper {
    private static String name = null;

    public static String getName() {
        return name;
    }

    public static void setName(String name) {
        ExampleBeanHelper.name = name;
    }

}

//Spring配置
<beans>
    <bean id="exampleBean"
        class="com.dsrcom.ecq.service.mhc.ExampleBean">
        <property name="name">
            <ref local="beanNameHelper" />
        </property>
    </bean>

    <bean id="beanNameHelper"
        class="org.springframework.beans.factory.config.MethodInvokingFactoryBean">
        <property name="targetClass">
            <value>com.dsrcom.ecq.service.mhc.ExampleBeanHelper</value>
        </property>
        <property name="targetMethod">
            <value>getName</value>
        </property>
    </bean>
</beans>

单元测试：
public class TestDynamicBeanPropertySet extends TestCase {
    private ClassPathXmlApplicationContext context = null;

    public void setUp() {
        ExampleBeanHelper.setName("bean");
        context = new ClassPathXmlApplicationContext(
                new String[] { "classpath:com/dsrcom/ecq/service/mhc/bean.xml" });
    }
    
    public void testBean() {
        ExampleBean bean = (ExampleBean)context.getBean("exampleBean");
        
        assertEquals("bean", bean.getName());
    }
}
/********************************************/
//内嵌式写法
<bean id="valueGenerator" class="com.abc.util.ValueGenerator" />

<bean id="son1" class="com.abc.service.Son">
    <property name="age">
        <!-- 获取方法返回值：调用valueGenerator的getValue方法 -->
        <bean class="org.springframework.beans.factory.config.MethodInvokingFactoryBean">
            <property name="targetObject" ref="valueGenerator" />
            <property name="targetMethod" value="getValue" />
        </bean>
    </property>
</bean>
/********************************************/
1.到NodeJs官网下载预编译安装包：node-v7.8.0-linux-x64.tar.xz。
2.用xz -d node-v7.8.0-linux-x64.tar.xz命令解压为得到node-v7.8.0-linux-x64.tar包，然后用tar -xvf node-v7.8.0-linux-x64.tar解包为目录。
3.在~/.bashrc中添加NodeJs的环境变量
	export NODEJS_HOME=~/software/lang/node-v7.8.0-linux-x64
	export PATH=$PATH:$NODEJS_HOME/bin
4.用版本查看命令node -v测试是否安装成功。
/********************************************/
Windows安装nodejs并开启一个简单的web项目
1.在机器上执行node -v，如果提示命令无效，说明未安装nodejs，需要去官网下载安装包，然后在机器上安装。
2.只想node -v如果显示版本信息，表示已成功安装。
3.用npm -v命令如果显示版本信息，说明npm也安装成功了，npm是node内置的包管理工具，类似于yum，
	但由于npm管理的大部分包在国外，所以下载速度会比较慢，cnpm是淘宝对npm做的国内代理仓库，下载速度会快很多，
	用如下命令安装cnpm：npm install cnpm -g --registry=https://registry.npm.taobao.org。
4.Express框架是nodejs的web框架，需要安装，cnpm install -g express-generator
5.新建一个目录作为测试空间如：F:/nodejs/justFrontSite。
6.进入项目空间，执行express app构建一个项目，执行后会出现一个app目录，其中自动建立了项目基本结构。
7.在app目录下执行：cnpm install命令将项目打包安装。
8.在app目录下执行：cnpm start命令启动项目，默认端口3000。
9.在浏览器输入localhost:3000如果出现Express页面，表示一个简单的基础项目搭建成功了。
/********************************************/
$ unzip cdap-sdk-4.1.1.zip  //解压二进制包
$ cd cdap-sdk-4.1.1  //进入解压后目录
$ ./bin/cdap sdk start  //启动cdap服务器
$ ./bin/cdap sdk stop  //停止cdap服务器
http://localhost:11011  //浏览器访问cdap服务
/********************************************/
Java项目的properties配置文件最好用ISO-8859-1，不然容易出现解析错误。
/********************************************/
在Eclipse中配置tomcat服务器，如果不能访问，最大可能是发布目录需要修改为，tomcat的发布目录，而不是workspace的。
/********************************************/
import org.apache.spark.ml.linalg.Vector  //导入这个包的向量不需要泛型
1.case class myClass(vector: Vector)  //全局声明
2.使spark支持把一个RDD隐式转换为一个DataFrame
	val sparkSession = SparkSession.builder().appName("IrisGaussianMixture").master("local[3]").getOrCreate()
	import sparkSession.implicits._  //此包的导入必须在sparkSession声明之后
/********************************************/
退出Scala REPL命令
sys.exit或:quit
/********************************************/
Spark读写Sequence文件
val sparkSession = SparkSession.builder().appName(this.getClass.getSimpleName).master("local[3]").getOrCreate()
import sparkSession.implicits._

val dataSet = sparkSession.sparkContext.sequenceFile[String, Long]("sequenceInputPath")  //读取
dataSet.foreach(it => println(it._1 + it._2))  //遍历取值

val dataFrame = sparkSession.read.textFile("textInputPath")  //读取文本文件
dataFrame.map(_.split('|')).map(it => (it(0), it(1).toLong)).rdd.saveAsSequenceFile("sequenceOutputPath")  //将文本文件组织成key-value形式，保存为Sequence文件
/********************************************/
rawDataFrame.rdd.saveAsTextFile("E:\\TEST\\sparkDate\\Example2_1\\output\\textFromUsers")  //dataFrame保存为文本
rawDataFrame.write.format("parquet").save("E:\\TEST\\sparkDate\\Example2_1\\output\\parquetFromUsers")  //dataFrame保存为parquet
/********************************************/
/********************************************/
稀疏矩阵常用三元组表示法表示，即(行号，列号，值)：
	{(1, 3, 0, 9),
	(2, 0, 5, 0),
	(0, 7, 3, 0)}
上述矩阵可表示为：
	{(0,0,1),
	(1,0,3),
	(0,3,9),
	(1,0,2),
	(1,2,5),
	(2,1,7),
	(2,2,3)}
/********************************************/
RPM包管理工具
基本选项{
	q：使用查询模式
	a：查询所有软件包
	i：显示详细信息
	l：显示软件包的文件列表
	p：查询指定的文件包
	f：查询指定文件所属文件包
	v：显示命令执行过程
	h：安装软件包时显示进度信息
	e：卸载指定的软件包
	U：升级软件包
}

查询验证包{
	查询已安装的软件信息{
		rpm -q samba-client  //使用选项q和软件包名称，查询系统中是否安装有指定的软件包
		rpm -qa  //使用选项q和a显示系统中已经安装的软件包
		rpm -qa | grep ssh  //使用管道和grep命令模糊查询软件包
		rpm -qi chkconfig  //使用选项i显示软件包的概况
		
		rpm -ql chkconfig  //使用选项q和l查看软件包含的文件列表
		rpm -qpl samba-client-3.0.33-3.7.el5.i386.rpm  //使用选项p指定输入的参数为一个软件包
		rpm -qpi samba-client-3.0.33-3.7.el5.i386.rpm  //使用选项p和i查看samba-client软件包的概况
		rpm -qf /bin/ls  //使用选项f指定查询/bin/ls所属的软件包
	}
}

安装、更新、删除包{
	rpm -i cabextract-1.3-1.i386.rpm  //使用选项i安装软件包cabextract
	rpm -ivh cabextract-1.3-1.i386.rpm  //使用选项v和h显示软件包的详细信息、安装进度等
	
	rpm -Uvh bzip2-libs-1.0.3-4.e15_2.i386.rpm  //使用选项U升级软件bzip2-libs
	rpm -e samba-client  //使用选项e卸载软件包samba-client
}

其他操作{

}
/********************************************/
//声明
　　int[] a;
　　int b[];
//创建数组对象
　　a = new int[4];
　　b = new int[5];
/********************************************/
JavaScript版：{
	JSON.stringify(jsonObject);  //将一个json对象转换为一个json字符串
	JSON.parse(jsonString);  //将一个json字符串转换为一个json对象
}

JQuery版：{
	$.parseJSON(jsonString);  //将一个json字符串转换为一个json对象，他是对js原生函数的兼容性封装
}
<a href='javascript:enOrDisableBaseModel("+JSON.stringify(row)+");'>操作</a>
/********************************************/
Java中有switch-case语句，但是只能按顺序匹配简单的数据类型和表达式。
Scala中的模式匹配的功能则要强大得多，可以应用到switch语句、类型检查、“解构”等多种场合。
val colorNum = 1
val colorStr = colorNum match {
    case 1 => "red"
    case 2 => "green"
    case 3 => "yellow"
    case _ => "Not Allowed"   //最低优先级
} 
println(colorStr)

for (elem <- List(9,12.3,"Spark","Hadoop",'Hello)){
    val str  = elem match{
        case i: Int => i + " is an int value."  //类型匹配
        case d: Double => d + " is a double value."
        case "Spark"=> "Spark is found."
        case s: String => s + " is a string value."
        case _ => "This is an unexpected value."  //最低优先级
    }
	println(str)    
}

for (elem <- List(1,2,3,4)){
    elem match {
        case _ if (elem %2 == 0) => println(elem + " is even.")
        case _ => println(elem + " is odd.")
    }
}
/********************************************/
标准类库中的Option类型用case类来表示那种可能存在、也可能不存在的值。
一般而言，对于每种语言来说，都会有一个关键字来表示一个对象引用的是“无”，在Java中使用的是null。
Scala融合了函数式编程风格，因此，当预计到变量或者函数返回值可能不会引用任何值时，建议你使用Option类型。
Option类包含一个子类Some，当存在可以被引用的值时，就可以使用Some来包含这个值，例如Some("Hadoop")。
而None则被声明为一个对象，而不是一个类，表示没有值。

在Scala中，使用Option的情形是非常频繁的。在Scala里，经常会用到Option[T]类型，其中的T可以是Sting或Int或其他各种数据类型。
Option[T]实际上就是一个容器，我们可以把它看做是一个集合，只不过这个集合中要么只包含一个元素（被包装在Some中返回），
要么就不存在元素（返回None）。既然是一个集合，我们当然可以对它使用map、foreach或者filter等方法
val books = Map("hadoop" -> 100, "Storm" -> 200, "spark" -> 300)
println(books.get("hadoop"))
println(books.get("hadoo"))
books.get("hadoop").foreach(println(_))
/********************************************/
Zookeeper3.4.9单机安装
1.下载、解压安装包
2.在zookeeper_home下新建tmp目录，用以存储临时数据
3.cp zookeeper_home/conf/zoo_sample.cfg zookeeper_home/conf/zoo.cfg  //复制配置文件模板
4.vi zoo.cfg  //将dataDir=/home/youn/software/bigdata/zookeeper_home/tmp  //将临时文件目录指向准备好的目录
5.sh zookeeper_home/bin/zkServer.sh start  //启动zookeeper服务器
4.jps查看进程
	QuorumPeerMain  //是zookeeper进程
/********************************************/
Kafka2.12单机安装
1.下载、解压安装包
2.sh zookeeper-server-start.sh ../config/zookeeper.properties &  //在kafka_home/bin下以后台进程的方式启动zookeeper服务器
3.sh kafka-server-start.sh ../config/server.properties &  //在kafka_home/bin下以后台进程的方式启动kafka服务器
4.sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic younkkk  //发布一条topic
5.sh kafka-topics.sh --list --zookeeper localhost:2181  //查看已发布topic列表【2181是zookeeper的端口】
6.sh kafka-console-producer.sh --broker-list localhost:9092 --topic younkkk  //到主题下发布消息
7.sh kafka-console-consumer.sh --zookeeper localhost:2181 --topic younkkk --from-beginning  //到指定主题消费一条消息
/********************************************/
Storm1.1.0单机安装
1.下载、解压安装包
2.修改storm_home/conf/storm.yaml，配置其下storm.zookeeper.servers和nimbus.seeds两个参数的值
	storm.zookeeper.servers:
		 - "127.0.0.1"  //格式严格，空格和横杠，引号都不能少
	nimbus.seeds: ["127.0.0.1"]  //冒号后必须有空格，值必须是String数组的形式
3.启动服务
	nohup ./storm_home/bin/storm nimbus &  //以后台方式运行nimbus服务
	nohup ./storm_home/bin/storm supervisor &  //以后台方式运行supervisor服务
	nohup ./storm_home/bin/storm ui &  //以后台方式运行ui服务
4.jps查看进程
	QuorumPeerMain  //zookeeper进程
	nimbus  //storm-nimbus进程
	Supervisor  //storm-supervisor进程
5.192.168.88.128:8080  //访问storm之web-ui
/********************************************/
yum list | grep nmap-ncat  //安装nc
nc -lk 999  //启动nc并使用999端口发送消息
nc localhost 999  //启动nc并使用999端口发送消息
/********************************************/
Scala程序main函数参数正确性检查
if (args.length < 2) {
  System.err.println("Usage: NetworkWordCount <hostname> <port>")  //提示输入参数用法
  System.exit(1)
}
/********************************************/
1.Complete Mode - 即每次更新结果集时，都将整个结果集写入外部存储中；
2.Append Mode - 即每次更新结果集时，只将新添加到结果集的结果行写入外部存储中；
3.Update Mode - 即每次更新结果集时，只有被更新的结果行写入到外部存储中（Spark2.0暂不支持）；
/********************************************/
1.nohup选项是永久执行与用户是否登陆无关
运行 nohup --help
Run COMMAND, ignoring hangup signals. 可以看到是“运行命令，忽略挂起信号”
就是指，用nohup运行命令可以使命令永久的执行下去，【和用户终端】没有关系，
例如我们断开SSH连接都不会影响他的运行，注意了nohup没有后台运行的意思

2.&选项是指在后台运行，但当用户退出(挂起)的时候，命令自动也跟着退出
那么，我们可以巧妙的吧他们组合起来用就是
nohup COMMAND &  //这样就能使命令永久后台执行

举个例子nohup tail -f nohup.out
然后退出登录，再连接，用ps -ef 你会还能看到在运行

另外，nohup执行后，会产生日志文件，将命令的执行过程中的消息保存到这个文件中，一般在当前目录下，
如果当前目录不可写，那么自动保存到执行这个命令的用户的home目录下，例如youn用户，就保存在/home/youn/下

这是我们常在运行命令和脚本中用到的
/********************************************/
在Linux中为Eclipse配置Tomcat后需要修改tomcat的ServerLocation选项，将其指定为tomcat的安装路径。
/********************************************/
第三章HDFS
1.海量数据的分布式存储和处理是大数据的两大核心问题。
	HDFS特殊设计实现以下目标：
		兼容廉价硬件设备（能运行在普通pc上）
		流式数据读写（不支持随机读写，一次读写整个文件）
		能存储大体积数据集（单个文件体积超大）
		简单的文件模型（文件都是只读的且只能追加增长）
		强大的跨平台能力（源自jvm跨平台特性）
	缺点：不适合低延迟数据访问（不支持随机读写，每次都要读取整个文件）
		  无法高速存储大量小文件（nameNode需要维护过于庞大的小文件索引，会降低检索性能）
		  不支持多用户写入及任意修改文件（只能坠加修改，不支持任意修改）
/********************************************/
Idea中Scala项目导入依赖jar包
【选中项目>>点击File>>Project Structure>>Libraries>>点击加号>>类型选择java>>选择目标jar包路径>>点击Apply】
/********************************************/
Idea中Scala代码打jar包
【Build>>BuildArtifact>>在弹出框目标项目操作选项>>选择Build>>打好的项目jar包在项目目录的out路径下】
/********************************************/
Idea中Scala代码打jar包，同时包含依赖jar包
1.指定随从依赖包【选中项目>>点击File>>Project Structure>>Artifacts>>在Output Layout选项卡下>>点击加号选file>>添加指定jar包>>点击Apply】
2.编译打包【方法同项目打jar包】
/********************************************/
如果想用val关键字声明内容可变的变量，变量类型必须是【可变类】。
/********************************************/
对不可变集进行操作，会产生一个新的集，原来的集并不会发生变化。 
而对可变集进行操作，改变的是该集本身。
/********************************************/
 Linux系统日志存放路径：/var/logs可以在此目录下清除。
/********************************************/
spring-mvc中要应答ajax请求，应该尽量在action中使用PrintWriter，而不是返回json格式的字符串数据，否则易出现复杂的中文乱码。
PrintWriter.write(jsonObject.toString());
/********************************************/
Scala中多行字符串需要用【三引号】，写法如下：
val foo = """菜鸟教程
			www.runoob.com
			www.w3cschool.cc
			www.runnoob.com
			以上三个地址都能访问"""
/********************************************/
Scala中方法是类的一部分，而函数是一个独立对象，可以赋值给一个变量，换句话说在某个类中定义的函数即是该类方法。
/********************************************/
scala创建Akka Actor的四种方式：

1、通过ActorSystem的actorOf方法，通过创建Actor，然后返回其ActorRef
	val system=ActorSystem("actorSystem")
	val systemActor=system.actorOf(Props[ActorClass],name="systemActor")
2、通过context.actorOf方法，也是通过创建Actor,然后返回其ActorRef
	val actor=context.actorOf(Props[ActorClass],name="actor")
3、通过context.parent、context.self、context.children方法获取当前Actor的父Actor、
	当前Actor及子Actor的ActorRef，获取已经创建好的Actor的ActorRef
4、通过val myActor1=system.actorSelection(myActorPath)方法来获取ActorRef，也是获取已经创建好的Actor的ActorRef
/********************************************/
单机安装HBase
1、首先从官网上下载HBase安装包
	http://mirrors.hust.edu.cn/apache/hbase/1.2.2/hbase-1.2.2-bin.tar.gz
2、解压缩到安装目录 /home/youn/software/bigdata
	[root@Hadoop- 01 ~]$ tar -xzvf hbase-1.2.2-bin.tar.gz
3、修改conf/hbase-env.sh，添加以下环境变量
	[root@Hadoop- 01 ~]$vi conf/hbase-env.sh

	export JAVA_HOME=/usr/java/jdk1.7.0_75/
	export HBASE_CLASSPATH=/usr/local/hbase/conf
	export HBASE_MANAGES_ZK=true
4、修改conf/hbase-site.xml，添加如下配置
	[root@Hadoop- 01 ~]$ conf/hbase-site.xml

	<configuration>
		<property>
			<name>hbase.rootdir</name>
			<value>file:/home/youn/software/bigdata/hbase-1.3.1</value>
		</property>
	</configuration>
5、配置HBase环境变量
[root@Hadoop- 01 ~]$ vi /etc/profile

	export HBASE_HOME=/home/youn/software/bigdata/hbase-1.3.1
	export PATH=$PATH:$HBASE_HOME/bin:$PATH

	[root@Hadoop- 01 ~]$ source /etc/profile    #保存使环境变量立即生效
6、启动
	[root@Hadoop- 01 hbase]$ bin/start-hbase.sh
7、检验HBase是否正常启动：
　　① 在bin目录执行【hbase shell】，正常的话会进入hbase命令行模式，即hbase(main):001:0>。
　　② 输入create 'user' ,'personalinfo',正常的话会出现类似 0 row(s) in 1.3200 seconds 这样的结果。
　　③ 继续输入list,正常的话会列出我们刚刚创建的user表。
8、打开浏览器访问localhost:16010/master-status，进入HBase之web-ui可以查看HBase的一些状态信息。
/********************************************/
手动编译Scala类文件
1.新建类的源文件My.scala，并在里面写上伴生类和伴生对象，及main函数。
2.通过DOS命令行进入源文件所在目录，执行scalac My.scala编译命令，编译生成对应的class文件【由于涉及内部类的编译可能生成多个class文件】。
3.执行scala My arg0 arg1【非内部类class文件】命令，执行编译后的二进制文件，并传参。
/********************************************/
手动打包Scala之class文件
1.将编译好的所有class文件放到一个干净的目录【可以将源文件也放进去】。
2.通过DOS命令进入上述目录，执行jar cvf my.jar ./命令，将目录下所有文件打入my.jar包中。
3.执行scala my.jar com.My arg0 arg1运行该jar包。
/********************************************/
Thread.sleep(1000L);  //Java/Scala线程睡眠1000毫秒。
/********************************************/
Spark2.0-sparkSession、dataFrame、dataSet
SparkSQL
/********************************************/
北风网
关闭杀软
解决SVN引起的播放异常
①任意文件夹（最好是在视屏所在目录）里右键菜单
②选TortoiseSVN
③点Settings
④点Icon Overlays
⑤将Status cache这一项设成None
⑥保存（打开视频仍然出现此提示重启电脑即可）

解决RealtekCameraMan引起的播放异常
进程名Realtek Camera Man或Integrated Camera Preview Rotation Helper或rtscm64开头的找下结束进程
/********************************************/
Kettle中Job和Transformation的区别：
1.在一个Job工作流中的各个节点是阻塞式处理数据的，只有前一个节点处理完所有数据，
	后一节点才会接着处理上一步的输出数据。在特定时刻，工作流中只会有一个节点在工作，是单线程的。
2.在一个转换工作流中，数据是以流的形式被各个节点处理的，当有一小批数据被前一个节点处理后，
	其输出会立即被后一节点处理，而前一节点会继续处理下一小批数据。在特定时刻，工作流中的所有节点都在并行运行，是多线程的。
/********************************************/
Kettle中Carte服务器（Linux和Windows环境下，整个套件直接解压免安装）
1.Linux下Carte节点启动 sh kettle_home/carte.sh 127.0.0.2 8081
	Windows下Carte节点启动kettle_home/carte.bat 127.0.0.2 8081
2.每个Carte节点的配置管理文件（连接账号、密码）在kettle_home/pwd目录下如文件：
	carte-config-8081.xml
	kettel.pwd  //密码可以明文，也可以密文【必须带有密钥标识】
	#cluster: OBF:1v8w1uh21z7k1ym71z7i1ugo1v9q  //经过OBF加密的密文密码
	#youn: kkk  //明文密码
3.当Spoon客户端要通过Carte节点远程执行任务时，必须配置好与节点相对应的参数【IP、Port、账号、密码】
/********************************************/
Carte安全
默认情况Carte是使用简单的HTTP认证，在文件pwd/kettle.pwd中定义了用户名和密码。Kettle默认附带的用户名/密码都是cluster.
文件中的密码可以利用kettle自带的Encr工具来混淆。要生成一个Carte密码文件，使用-carte选项，像这个例子：
sh encr.sh –carte Password4Carte
OBF:1324324u432oj4324j3l2kj432j432lj43l2j43l2j43k24jc
使用文本编辑器，将返回的字符串追加到密码文件中用户名的后面
Someuser : OBF : 1324324u432oj4324j3l2kj432j432lj43l2j43l2j43k24jc
OBF：前缀告诉Carte这个字符串是被混淆了的，如果你不想混淆这个文件中的密码，你可以清楚的指定密码像这样:
Someuser:Password4Carte
需要注意的是：密码是被混淆了，而不是被加密了。
这个算法仅仅是用来让密码识别起来更难，但绝对不是不可能。
如果一个软件有读取这个密码的能力，你必须假设别人也能读取它[*10] ，
因此，你应该总是给这个密码文件添加一些合适的权限。
如果你阻止了对这个文件的未授权的访问，就首要的减少了某人能够破解这个密码的风险。
/********************************************/
解决Carte运行内嵌转换报错
两个问题：
1、把kettle中的lib下的commons-vfs2-2015***.jar换成commons-vfs2-2.0.jar
2、Carte所在的.kettle文件夹下的repositories.xml要修改，资源库名字，描述要和程序中的一样

/********************************************/
WebLogic启动步骤：
1.启动WebLogic节点
	到目录【/home/webadmin/Oracle/Middleware/user_projects/domains/base_domain/bin】下执行
	nohup sh startWebLogic.sh &
2.启动NodeManager
	到目录【/home/webadmin/Oracle/Middleware/wlserver_12.1/server/bin】下执行
	nohup sh startNodeManager.sh &
/********************************************/
Eclipse界面主题配置
1.Window
2.Preference/General/Appearance/
3.Theme = Dark
4.Color and Font theme = Default
/********************************************/
将Kettle作业和转换日志写入数据库
1.修改user_home/.kettle/kettle.properties文件，添加以下参数
	KETTLE_JOB_LOG_DB=dw1  //设置作业日志库名
	KETTLE_JOB_LOG_TABLE=bi_etl_job_log_table  //设置作业日志库表
	  
	KETTLE_TRANS_LOG_DB=dw1  //设置转换日志库名
	KETTLE_TRANS_LOG_TABLE=bi_etl_trans_log_table  //设置转换日志表名
/********************************************/
*当将Application提交给Spark后，会在本地或集群上（因驱动器参数而异）的某个节点启动一个Driver进程
这个Driver会执行用户代码，
1.初始化一个SparkContext，在这个初始化过程中，会在SparkContext中构造一个DAGScheduler
和一个TaskScheduler，TaskScheduler会向Master进程注册Application，
Master使用资源调度算法分别在各Worker进程上启动若干Executor，这些启动的Executor又反向注册回TaskScheduler上。
2.SparkContext初始化完成后，继续执行后续代码，遇到一个Action就划分为一个Job，这个Job会被提交给DageScheduler，
这个Job会根据Shuffle划分为多个Stage，每个Stage会对应创建一个TaskSet，每一个TaskSet都会被提交给TaskScheduler，
TaskScheduler会将每一个Task提交给之前反向注册到Driver的Executor。


*一个用户应用程序就是一个Application，
在一个Application中，每遇到一个Action操作断开为一个Job，
在每个Job中，每遇到一个Shuffle操作断开为一个Stage，
每个Stage对应创建一个TaskSet，其中包含多个Task。

*DAGScheduler（所有的Job都会被提交给DageScheduler将每个Job划分为多个Stage）
*TaskScheduler会将TaskSet中的每一个Task提交给已注册到其上的Executor去执行。
*TaskRunner将用户代码中的算子、函数等操作经过拷贝、反序列化，然后执行Task。
*Executor内有一个线程池，当接受到一个Task后，会用TaskRunner来封装Task，然后从池中取出一个线程来执行这个Task。

*Task分为两种，ShuffleMapTask和ResultTask，只有最后一个Stage才具有ResultTask，其他Stage拥有的都是ShuffleMapTask。

*通俗的讲，Spark应用的代码分作为Stage以TaskSet的方式分批提交到Executor执行，每个Task针对一个RDD的一个分区，执行代码定义的算子或函数。
/********************************************/
1.宽依赖：子RDD中的每个分区混合依赖父RDD中的多个分区，总体呈向下交错状，会出现Shuffle。
2.窄依赖：子RDD中的每个分区都只依赖父RDD中的一个分区或子RDD不共享父RDD的多个分区，总体呈向下汇聚状，不会出现Shuffle。

在一个Stage中的不同RDD间（一个lineage），一个Task只提交一次，然后整个Stage中的后续操作都以这个Task来完成，
而Shuffle操作除了会发生网络传输数据，也会换一批新Task提交到Executor。
/********************************************/
select * from `test` where binary `name` = "Test";  //MySql查询大小写敏感
ALTER TABLE sm_user MODIFY COLUMN LOGIN_ID VARCHAR (50) BINARY;  //设置指定字段大小写敏感
/********************************************/
【ResourceManager：RM，ApplicationManager：AM】
1.spark-submit --master yarn-cluster  //"Driver"运行于yarn集群上的未知节点上
	spark-submit进程向yarn的RM发送启动AM进程（Driver）的请求，
	RM会在其所管理的集群中找一台机器启动AM进程，AM又向RM请求一批Container去连接其他的NodeManager，以启动Executor，
	启动起来的每一个Executor都会到AM去反向注册，后续过程与独立集群类似。

2.spark-submit --master yarn-client  //Driver运行于提交作业的本地主机上（非集群内节点）
	spark-submit进程会在本地启动Driver，同时向RM请求启动AM，RM会在集群未知节点启动一个功能有限的AM，
	这个AM实际上是一个ExecutorLuncher，负责向RM申请一批Container去连接其他的NodeManager，以启动Executor。
	启动起来的每一个Executor都会到客户端本地的Driver反向注册。
-------------------------------------------------------------------------------
yarn-client模式：可在本地查看所有的运行日志，方便调试，适合测试环境，但本地Driver在App调度过程中会与集群发生大量通信，网络压力大。
yarn-cluster模式：没有单台机器网络压力问题，适合生产环境，但日志查看不方便，只能通过Yarn的日志/App日志查看方式。
/********************************************/
SparkContext的初始化过程：
1.TaskScheduler初始化过程：createTaskScheduler{
	1.创建一个TaskSchedulerImpl实例，也就是一个任务调度器。
	2.创建一个SparkDeploySchedulerBackend，它受上述TaskSchedulerImpl实例的控制，实际负责到Master注册App、
		受理Executor的反向注册、将Task发送到Ececutor等东西【TaskScheduler底层基于该组件工作】。
	3.创建一个SchedulePool，它有不同的优先策略。
	4.调用TaskSchedulerImpl的start方法，在这个方法内部会调用SparkDeploySchedulerBackend的start方法，
		在第二个start方法内又会创建一个AppClient，在AppClient内会启动一个ClientActor线程，ClientActor会去调用
		registerWithMaster和tryRegisterAllMaster方法发送封装了App信息的RegisterApplication对象到集群Master注册App。
}
2.DAGScheduler初始化，会启动一个DAGSchedulerEventProcessActor线程，DAGScheduler基于该线程来通信。
3.启动SparkUI，用以显示App的运行状态，实际上是启动一个Jetty服务器来提供web服务。
/********************************************/
httpGet.setHeader("Content-Type", "application/x-www-form-urlencoded");  //Rest请求免认证报头
httpGet.setHeader("Authorization", "Basic MTox");  //Rest请求基础认证报头
/********************************************/
Map<String, Object> map;  //用map封装数据到前台
for(UserAndCategory it : list){
	map = new HashMap<String, Object>();
	map.put("id", it.getId());
	map.put("userId", it.getUser().getUserId());
	map.put("userName", it.getUser().getUserName());
	voList.add(map);
}
map = null;
/********************************************/
028_Spark核心编程Spark基本工作原理与RDD

/********************************************/
031_Spark核心编程Spark架构原理
Driver：是一个进程，可以运行在集群外的提交任务的机器上，也可以运行在集群内的某个机器节点上，App代码就运行在Driver上。
Master：是一个进程，主要负责资源的管理与分配，及集群的监控。
Worker：是一个进程，首先存储RDD的某些Partition，其次是负责启动其他进程或线程对其存储的Partition进行计算处理。
Executor：是Worker进程启动的若干个另一种进程，在Executor内会启动若干个Task线程。
Task：负责并行执行App代码中的各种计算。

当一个App被提交到Spark后，Spark会启动一个Driver进程，Driver进程会想Master发送请求，进行App注册，让Master知道有一个新的App想要运行。
Master接收到App的注册之后，Master会发送请求给Worker，进行计算资源分配，也就是Worker给App分配Executor。
Worker每启动一个Executor后，这个Executor会反向到Driver去注册。
完成反向注册后就可以正式开始执行App代码了。

首先Worker从数据源（HDFS、Hive、HBase）读取数据，创建初始RDD。
Driver会根据对代码中对RDD定义的操作向Executor提交N个Task。
Executor接收到这N个Task后会启动N个线程来执行这些Task。
Task就对指定的RDD Partition执行计算。
/********************************************/
五十七、BlockManager底层机制
*BlockManager是Spark底层数据存取管理的组件，在Driver里有一个BlockManagerMaster，各节点上有BlockManager，够成一个主从集群结构。
1.Driver中的DAGScheduler里有一个BlockManagerMaster，每个BlockManagerMaster又有一个BlockMangerInfo集合，
	它代表着各个节点上真实BlockManager的元数据，其中包含了每个BlockManager的BlockStatus（包括BlockManager本身状态信息及数据状态信息），
	BlockManagerMaster会管理维护这个元数据集。
2.一个BlockManager包含以下四个关键组件：
	DiskStore：负责对磁盘上的数据的管理和存取
	MemoryStore：负责对内存上的数据的管理和存取
	ConnectionManager：负责在远程BlockManager间建立网络连接
	BlockManagerWorker：负责对远程BlockManager管辖数据的读写
3.每个BlockManager创建之后都会到BlockManagerMaster去注册，后者会为其创建元数据。
4.通过BlockManager进行写操作：
	当需要对数据进行缓存或保存临时数据时，优先写入内存，内存不足就写入磁盘，如果缓存时指定了replica冗余策略
	还会启用ConnectionManager与远程BlockManager建立连接，并用BlockManagerWorker将数据复制到远程节点。
5.通过BlockManager进行读操作：比如ShuffleRead操作，会优先通过MemoryStore、DiskStore读取内存、磁盘上的数据，
	如果本地没有数据就会启用ConnectionManager与远程BlockManager建立连接，并用BlockManagerWorker从远程BlockManager读取数据。
6.当对某个BlockManager管理的数据进行了增、删、改等操作，数据状态会同步到BlockManagerMaster中对应的BlockManagerInfo上。
/********************************************/
五十八、CheckPoint：是Spark为了应对长或复杂RDD转换链而提供的高级容错特性，当一个SparkApp的DAG很复杂、运行时间比较长时，
	可能因节点故障或内存GC等原因导致RDD数据丢失，这种故障单纯的Cache缓存都可能满足不了其容错要求。
	CheckPoint是将可复用RDD额外持久化到HDFS等高可用外部文件系统上，提供额的高可用外备份。
*如果一个RDD既做了Cache又做了CheckPoint容错处理，当发生数据丢失时，会优先读取Cache数据，
	如果不成功才是CheckPoint备份数据，如果任然不成功最后才会ReCompute操作。
1.SparkContext.setCheckPointDir("D:\\myCheckPointDir")  //为应用设置一个外部文件系统磁盘目录（HDFS或linux）
2.myRdd.checkPoint()  //将指定RDD额外持久化备份到磁盘目录

Checkpoint实现机制：
1.当对一个RDD调用了checkpoint方法后，它就被托管给了RDDCheckpointData对象。
2.RDDCheckpointData对象会将此RDD的状态标记为MarkedForCheckpoint。
3.当此RDD所在job运行完毕后，会调用job的FinalRDD的doCheckpoint方法，沿着FinalRdd的lineage向上追溯，
	遇到被标记为MarkedForCheckpoint的RDD就将它标记为CheckpointingInProgress。
4.启动一个单独的job来将lineage中被标记为CheckpointingInProgress的RDD数据写入SparkContext设置的checkpointDir中。
5.当对一个RDD完成checkpoint操作后，会改变RDD的lineage，会清除RDD的所有依赖，并强行将其父RDD设置为一个checkpointRDD，
	而且RDD状态改变为checkpointed。
*与Cache操作相比，主要区别在于Checkpoint操作除了对RDD数据进行了持久化，还改变了RDD的lineage，
*其次是Cache持久化后数据丢失的可能性比Checkpoint的大。

*建议对要checkpoint的RDD先用disk_only方式cache到磁盘，因为默认情况下如果这个RDD没有cache，
*当job完成后回溯时找不到该RDD的数据就会导致重算，然后才能持久化。

【带cache和checkpoint的RDD数据读取流程】
1.在RDD数据被计算出来前，程序会读取RDD数据，此时会调用RDD的iterator方法。
2.在iterator方法中发现storageLeve != StorageLeve.NONE，便会通过CacheManager去获取数据，会发现通过BlockManager得不到数据（还没计算）。
3.执行首次计算，并通过CacheManager的putInBlockManager()方法，将其数据通过BlockManager缓存起来。
4.当RDD所在job执行结束，就另起一个job对RDD进行checkpoint到HDFS。（有疑问？）
5.当后续需要读取此RDD数据时，任然会调用该RDD的iterator方法，经过判断去读取缓存数据，正常情况下能从BlockManager读到数据。
6.当读取缓存数据失败时，会调用computOrReadCheckpoint方法，如果判断RDD的isCheckpoint为true，就会调用父RDD的iterator方法，读取HDFS上的持久化数据。
/********************************************/
假设原始样本中有两类，其中： 
1：总共有 P个类别为1的样本，假设类别1为正例。 
2：总共有N个类别为0 的样本，假设类别0为负例。 
经过分类后：
3：有 TP个类别为1 的样本被系统正确判定为类别1，FN 个类别为1 的样本被系统误判定为类别 0，显然有P=TP+FN； 
4：有 FP 个类别为0 的样本被系统误判断定为类别1，TN 个类别为0 的样本被系统正确判为类别 0，显然有N=FP+TN； 
 
那么：
*精确度（Precision）：
P = TP/(TP+FP) ;  反映了被分类器判定的正例中真正的正例样本的比重（ 
* 
准确率（Accuracy）
A = (TP + TN)/(P+N) = (TP + TN)/(TP + FN + FP + TN);    反映了分类器统对整个样本的判定能力——能将正的判定为正，负的判定为负 
 
*召回率(Recall)，也称为 True Positive Rate:
R = TP/(TP+FN) = 1 - FN/T;  反映了被正确判定的正例占总的正例的比重 
 
*转移性（Specificity，不知道这个翻译对不对，这个指标用的也不多），也称为 True NegativeRate 
S = TN/(TN + FP) = 1 – FP/N；   明显的这个和召回率是对应的指标，只是用它在衡量类别0 的判定能力。 
 
*F-measure or balanced F-score
F = 2 *  召回率 *  准确率/ (召回率+准确率)；这就是传统上通常说的F1 measure，另外还有一些别的F measure，可以参考下面的链接 
/********************************************/
兼顾性能与线程安全的内部类式单利模式：
public class Singleton {    
    private static class LazyHolder {    
       private static final Singleton INSTANCE = new Singleton();    
    }    
    private Singleton (){}    
    public static final Singleton getInstance() {    
       return LazyHolder.INSTANCE;    
    }    
}   
---------------------------------------------------------
我的实现
private static class Holder {
	private static final CarteSingleton INSTANCE = new CarteSingleton();
}

private CarteSingleton(){
	super();
	jobMap = new HashMap<String, JobTask>();
	transMap = new HashMap<String, TransTask>();
}

public static CarteSingleton getInstance(){
	return Holder.INSTANCE;
}
/********************************************/
Scala支持尾递归优化，Java和Python不行
object TestTailRecursion {
  def sum(n: Long, total: Long): Long = {
    if(n <= 0){
		total
	}else{
		sum(n - 1, total + n)
	}
  }
  def main(args: Array[String]) {
    val total = sum(10000000, 0)
    println(total)
  }
}

普通递归
def mySum(num: Long): Long = {
	if(num == 1){
		num
	}else{
		num + mySum(num - 1)
	}
}

def myTailSum(num: Long, togal: Long): Long = {
	if(num == 1){
		num
	}else{
		myTailSum()
	}
}
/********************************************/
简单Spring定时任务
xmlns:task="http://www.springframework.org/schema/task"
xsi:schemaLocation=http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-4.0.xsd"
default-lazy-init="false">

<context:component-scan base-package="com.nicchagil.*"/>  //任务所在的类需纳入Spring容器
<task:annotation-driven/>  //启用任务注解驱动【默认单线程】

<task:annotation-driven scheduler="myScheduler"/>  //可以【多个类，多个方法】，只要纳入了Spring容器，且加了@Scheduled注解就可
<task:scheduler id="myScheduler" pool-size="20"/>  //遇到多个任务需要定时执行时，为了避免多任务对单线程的竞争，需要配置线程池

@Component
public class MyTask {
	@Scheduled(cron = "/5 * * * * ?")
	public void run()
		System.out.println("执行了定时任务");
	}
}

一个cron表达式有至少6个（也可能7个）有空格分隔的时间元素。
按顺序依次为
秒（0~59）
分钟（0~59）
小时（0~23）
天（月）（0~31，但是你需要考虑你月的天数）
月（0~11）
天（星期）（1~7 1=SUN 或 SUN，MON，TUE，WED，THU，FRI，SAT）
年份（1970－2099）
其中每个元素可以是一个值(如6),一个连续区间(9-12),一个间隔时间(8-18/4)(/表示每隔4小时),一个列表(1,3,5),通配符。由于"月份中的日期"和"星期中的日期"这两个元素互斥的,必须要对其中一个设置?.

0 0 10,14,16 * * ? 每天上午10点，下午2点，4点
0 0/30 9-17 * * ?   朝九晚五工作时间内每半小时
0 0 12 ? * WED 表示每个星期三中午12点 
"0 0 12 * * ?" 每天中午12点触发 
"0 15 10 ? * *" 每天上午10:15触发 
"0 15 10 * * ?" 每天上午10:15触发 
"0 15 10 * * ? *" 每天上午10:15触发 
"0 15 10 * * ? 2005" 2005年的每天上午10:15触发 
"0 * 14 * * ?" 在每天下午2点到下午2:59期间的每1分钟触发 
"0 0/5 14 * * ?" 在每天下午2点到下午2:55期间的每5分钟触发 
"0 0/5 14,18 * * ?" 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发 
"0 0-5 14 * * ?" 在每天下午2点到下午2:05期间的每1分钟触发 
"0 10,44 14 ? 3 WED" 每年三月的星期三的下午2:10和2:44触发 
"0 15 10 ? * MON-FRI" 周一至周五的上午10:15触发 
"0 15 10 15 * ?" 每月15日上午10:15触发 
"0 15 10 L * ?" 每月最后一日的上午10:15触发 
"0 15 10 ? * 6L" 每月的最后一个星期五上午10:15触发 
"0 15 10 ? * 6L 2002-2005" 2002年至2005年的每月的最后一个星期五上午10:15触发 
"0 15 10 ? * 6#3" 每月的第三个星期五上午10:15触发 

有些子表达式能包含一些范围或列表
例如：子表达式（天（星期））可以为 “MON-FRI”，“MON，WED，FRI”，“MON-WED,SAT”

“*”字符代表所有可能的值
因此，“*”在子表达式（月）里表示每个月的含义，“*”在子表达式（天（星期））表示星期的每一天

“/”字符用来指定数值的增量
例如：在子表达式（分钟）里的“0/15”表示从第0分钟开始，每15分钟
在子表达式（分钟）里的“3/20”表示从第3分钟开始，每20分钟（它和“3，23，43”）的含义一样

“？”字符仅被用于天（月）和天（星期）两个子表达式，表示不指定值
当2个子表达式其中之一被指定了值以后，为了避免冲突，需要将另一个子表达式的值设为“？”

“L” 字符仅被用于天（月）和天（星期）两个子表达式，它是单词“last”的缩写
但是它在两个子表达式里的含义是不同的。
在天（月）子表达式中，“L”表示一个月的最后一天
在天（星期）自表达式中，“L”表示一个星期的最后一天，也就是SAT

如果在“L”前有具体的内容，它就具有其他的含义了
例如：“6L”表示这个月的倒数第６天，“ＦＲＩＬ”表示这个月的最一个星期五
注意：在使用“L”参数时，不要指定列表或范围，因为这会导致问题
/********************************************/
在Spark的本地和集群模式下，不要在算子内部改变闭包变量（外部变量）的值，因为在并行状态下各个Executor中运行的Task分别拿到的是闭包变量的副本，
值的改变也只是在Executor内部，Driver端的值没有改变。如果有这样的需求，可以用累加器来实现。
/********************************************/
073_Spark SQLDataFrame的使用

/********************************************/
074_2.使用反射方式将RDD转换为DataFrame
1.通过反射机制，将RDD的泛型作为DataFrame的元数据。
2.
SparkSQL支持将包含Java类的RDD转换为DataFrame的操作，但是，POJO不能以自定义类型为成员变量，也就是不支持嵌套Bean。
但是，Scala接口支持嵌套型的CaseClass。
/********************************************/
String classpath = this.getClass().getResource("/").getPath().replaceFirst("/", "");
String webappRoot = classpath.replaceAll("WEB-INF/classes/", "");
/********************************************/
色调：26
饱和度：119
亮度：165

R：215
G：187
B：136
/********************************************/
String osType = System.getProperty("os.name");  //java获取当前OS类型
/********************************************/
java中向一个文件追加写入内容
public void appendToFile(String filePath, String content){
	try {
		FileWriter fileWriter = new FileWriter(filePath, true);  //参数true表示append方式
		fileWriter.write("1111");  //写入全字符串
		fileWriter.write("\n");  //写入回车
		fileWriter.write(content, 1, 2);  //写入部分字符串
		fileWriter.flush();
		if(fileWriter != null){
			fileWriter.close();
		}
	} catch (IOException e) {
		e.printStackTrace();
	}
}
/********************************************/
在Java爬虫中，先用HttpClient获取网页，然后用JSoup从网页源码中抽取想要的元素。
/********************************************/
EHcache2.x简要配置
<ehcache>
    <!--
          磁盘存储:将缓存中暂时不使用的对象,转移到硬盘,类似于Windows系统的虚拟内存
           path:指定在硬盘上存储对象的路径
    -->
    <diskStore path="D:\ehcacheTwo" />

    <!--
         defaultCache:默认的缓存配置信息,如果不加特殊说明,则所有对象按照此配置项处理
         maxElementsInMemory:设置了缓存的上限,最多存储多少个记录对象
         eternal:代表对象是否永不过期
         overflowToDisk:当内存中Element数量达到maxElementsInMemory时，Ehcache将会Element写到磁盘中
    -->
    <defaultCache maxElementsInMemory="100" eternal="true" overflowToDisk="true"/>

    <!--
        maxElementsInMemory设置成1，overflowToDisk设置成true，只要有一个缓存元素，就直接存到硬盘上去
        eternal设置成true，代表对象永久有效
        maxElementsOnDisk设置成0 表示硬盘中最大缓存对象数无限大
        diskPersistent设置成true表示缓存虚拟机重启时期的数据
     -->
    <cache name="myCache" maxElementsInMemory="1" eternal="true" overflowToDisk="true" maxElementsOnDisk="0" diskPersistent="true"/>
</ehcache>

说明：
cache元素的属性：
name：缓存名称
maxElementsInMemory：内存中最大缓存对象数
maxElementsOnDisk：硬盘中最大缓存对象数，若是0表示无穷大
eternal：true表示对象永不过期，此时会忽略timeToIdleSeconds和timeToLiveSeconds属性，默认为false
overflowToDisk：true表示当内存缓存的对象数目达到了maxElementsInMemory界限后，会把溢出的对象写到硬盘缓存中。注意：如果缓存的对象要写入到硬盘中的话，则该对象必须实现了Serializable接口才行
diskSpoolBufferSizeMB：磁盘缓存区大小，默认为30MB。每个Cache都应该有自己的一个缓存区
diskPersistent：是否缓存虚拟机重启期数据
diskExpiryThreadIntervalSeconds：磁盘失效线程运行时间间隔，默认为120秒
timeToIdleSeconds： 设定允许对象处于空闲状态的最长时间，以秒为单位。当对象自从最近一次被访问后，如果处于空闲状态的时间超过了timeToIdleSeconds属性值，这个对象就会过期，EHCache将把它从缓存中清空。只有当eternal属性为false，该属性才有效。如果该属性值为0，则表示对象可以无限期地处于空闲状态
timeToLiveSeconds：设定对象允许存在于缓存中的最长时间，以秒为单位。当对象自从被存放到缓存中后，如果处于缓存中的时间超过了 timeToLiveSeconds属性值，这个对象就会过期，EHCache将把它从缓存中清除。只有当eternal属性为false，该属性才有效。如果该属性值为0，则表示对象可以无限期地存在于缓存中。timeToLiveSeconds必须大于timeToIdleSeconds属性，才有意义
memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时，Ehcache将会根据指定的策略去清理内存。可选策略有：LRU（最近最少使用，默认策略）、FIFO（先进先出）、LFU（最少访问次数）
/********************************************/
在SpringMVC中通过Controller，以流的形式读取非webapp路径下的图片
@RequestMapping(value="showImg")
public void ShowImg(HttpServletRequest request,HttpServletResponse response) throws IOException{
   String imgFile = request.getParameter("imgFile");  //图片文件名
   String path= UrlUtil.getValue("goodsImg");  //这里是存放图片的文件夹地址，【建议放在项目配置文件中】
   FileInputStream fileIs=null;
   try {
	fileIs = new FileInputStream(path+"/"+imgFile);
   } catch (Exception e) {
	 log.error("系统找不到图像文件："+path+"/"+imgFile);
	 return;
   }
   int i = fileIs.available();  //得到文件大小
   byte data[]=new byte[i];
   fileIs.read(data);  //读数据
   response.setContentType("image/*");  //设置返回的文件类型
   OutputStream outStream=response.getOutputStream();  //得到向客户端输出二进制数据的对象
   outStream.write(data);  //输出数据
   outStream.flush();
   outStream.close();
   fileIs.close();
}
/********************************************/
1.request.setCharacterEncoding（）是设置从request中取得的值或从数据库中取出的值的编码
2.response.setContentType指定 HTTP 响应的编码,同时指定了浏览器显示的编码. 
3.response.setCharacterEncoding设置HTTP 响应的编码,如果之前使用response.setContentType设置了编码格式，
则使用response.setCharacterEncoding指定的编码格式覆盖之前的设置。
与response.setContentType相同的是,调用此方法,必须在getWriter执行之前或者response被提交之前
/********************************************/
1、加入两个外部js
	1）FileSaver.js
	2）jquery.wordexport.js
2、在页面加入一个div或者在body上面加入一个id，div和body必须包含要导出的内容
	<div id="export_word">
		<p>要导出的页面内容</p>
	</div>
3、可以指定id的元素上调用.wordExport()函数，将元素的内容导出
	function exportWord(){
		$("#export_word").wordExport();
	}
4、也可以在自己的js里面写入（具体按钮自己写入方法名称exportWord（），按钮的id="export_word"）
	function exportWord(){
		$("#export_word").hide();
		$("html").wordExport();
		$("#export_word").show();
	}
/********************************************/
MySql中EXTRACT()函数用于返回日期/时间的独立单位部分，比如年、月、日、小时、分钟中的某一部分。
SELECT
	EXTRACT(YEAR FROM t.orderDate) AS orderYear,
	EXTRACT(MONTH FROM t.orderDate) AS orderMonth,
	EXTRACT(DAY FROM t.orderDate) AS orderDay
FROM
	t_order t;
/********************************************/
Linux安装二进制版MySql（5.6也可用）
1、在官网下载对应的mysql压缩包 
　　https://cdn.mysql.com//Downloads/MySQL-5.5/mysql-5.5.54-linux2.6-x86_64.tar.gz（这个是mysql5.5的版本）
　　其他版本下载地址：https://dev.mysql.com/downloads/mysql/结合自己的需求下载
　　下载对应的二进制tar包：
2、将压缩包放在/usr/local下面并解压
	tar -zxvf mysql-5.5.54-linux2.6-x86_64.tar.gz
3、修改解压后的文件夹名称为mysql
	mv mysql-5.5.54-linux2.6-x86_64 mysql　
4、先创建一个群组
	groupadd mysql 
5、创建一个用户mysql
	useradd -r -g mysql mysql
6、修改当前目录使用权限 【目的为了生成完整的数据】
	chown -R mysql:mysql ./ #修改当前目录使用权限为mysql
7、安装数据库
	./scripts/mysql_install_db --user=mysql  #初始化数据库（在安装包目录下操作）
8、更改权限
	chown -R root:root ./ #更改当前目录的所有文件拥有者为【root用户:root组】
	chown -R mysql:mysql data #改变data的拥有者为【mysql用户:mysql组】　
9、将mysql的启动添加到系统服务中，Linux启动时会自动启动
	cp support-files/mysql.server /etc/init.d/mysqld
修改复制的mysql文件
	basedir=/usr/local/mysql #指定安装目录（不写默认该目录）
	datadir=/usr/local/mysql/data #指定保存数据目录（默认安装目录下的data）
10、将/etc/my.cnf文件覆盖，目的是改变启动路径和方式
	cp support-files/my-medium.cnf /etc/my.cnf　//【高版本的mysql可能是my-medium.cnf】　
11、配置mysql的运行环境
	vi /etc/profile 添加下面配置 并source /etc/profile
	#for mysql
	export MYSQL_HOME=/usr/local/mysql
	export PATH=$PATH:$MYSQL_HOME/bin
12、启动mysql
	service mysql start
13、让所有人可以连接
	GRANT ALL PRIVILEGES ON *.* TO 'root（账号）'@'%' IDENTIFIED BY 'root（密码）' WITH GRANT OPTION
	flush privileges; #立即生效　
14、出现错误:ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2) 
　　解决方案：
　　　　1、修改mysql安装目录权限为mysql（第6步）
　　　　2、重新安装数据库（第7步）
　　　　3、改回权限（第8步）
　　　　4、重启mysq
15、mysql设置开机启动
	/sbin/chkconfig mysql on　　
16、mysql修改密码
	mysqladmin -u root（用户） -p password "123456（密码）" flush privileges; #适用于root账户
	【mysqladmin命令说在路径为：/usr/local/mysql/bin/】
用root账号进入后执行 GRANT ALL ON *.* TO 'mysql'@'localhost' IDENTIFIED BY 'mysql' WITH GRANT OPTION; flush privileges;
	flush privileges; #（针对于普通用户）　
17、mysql创建用户
	create user 'mysql（用户名）'
-------------------------------------------
当执行命令：service mysql start报下列错误
	Redirecting to /bin/systemctl start  mysql.service
	Failed to start mysql.service: Unit mysql.service failed to load: No such file or directory.

解决办法(刷新systemctl缓存)：systemctl daemon-reload 
-------------------------------------------
/********************************************/
rpm方式安装msql
(1) 卸载包含有mariadb关键字的RPM包.
 
(2) 安装perl、net-tools
# yum install perl
# yum install net-tools 
(3) 安装mysql
    安装顺序：
    # rpm -ivh mysql-community-common-5.7.13-1.el6.x86_64.rpm 
    # rpm -ivh mysql-community-libs-5.7.13-1.el6.x86_64.rpm  
    # rpm -ivh mysql-community-client-5.7.13-1.el6.x86_64.rpm  
    # rpm -ivh mysql-community-server-5.7.13-1.el6.x86_64.rpm  
    # rpm -ivh mysql-community-libs-compat-5.7.13-1.el6.x86_64.rpm 

(4) 启动mysql服务，查看初始密码，关闭密码验证插件（先启动mysqld服务才能看密码，启动前确保/var/lib/mysql/目标目录下是空的）
启动mysql服务 # service mysqld start
查看初始密码 # grep 'temporary password' /var/log/mysqld.log
3(Vh7wQmOr<

关闭密码验证插件 /etc/my.cnf 加入validate_password=off
设置数据库编码 /etc/my.cnf  加入character-set-server=utf8
(5) 启动mysql服务，更改初始密码
 
# mysql -u root -p
输入密码登陆mysql
# set password for 'root'@'localhost'=password('root');
注：如果启动失败，可尝试mkdir –p /var/run/mysqld/创建该目录，并使用chown -R mysql:mysql /var/run/mysqld/修改目录权限给mysql。
(6)赋予root用户远程访问权限
# GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;
# FLUSH PRIVILEGES;
/********************************************/
yum安装pstree
yum list psmisc
/********************************************/
解决maven项目里pom文件中Plugin execution not covered by lifecycle configuration方式！
<build>
	<pluginManagement>
	</pluginManagement>
</build>
/********************************************/
#or, download the shell script with wget
wget -O /tmp/kickstart-static64.sh https://my-netdata.io/kickstart-static64.sh

#run the downloaded shell script (any sh is fine, no need for bash)
sh /tmp/kickstart-static64.sh

#test
http://192.168.88.128:19999  //netdata之web-ui
/********************************************/
1. make a link to /usr/java/default
	mkdir -p /usr/java
	ln -s YOUR_JAVA_HOME /usr/java/default
 
2. add sudo nopassword to cloudera-scm user
	vim /etc/sudoers
-----------------------------------
cloudera-scm    ALL=(ALL)       NOPASSWD: ALL
/********************************************/
System.setProperty("Property1", "abc");  //相当于一个全局静态变量，存在内存中
System.getProperty("name");
/********************************************/
NetCat
nc -lk 99  //在服务端开启对99端口的监听，以备客户端连接
/********************************************/
Nginx安装
1.到Nginx官网下载Unix版nginx-1.13.3.tar.gz源码压缩包
2.tar zxvf tar zxfv nginx-1.13.3.tar.gz  //可以用root也可以用其他用户
3.解压根目录/configure --prefix=/usr/local/servers/nginx  //用此命令检测系统环境，并初始化指定的安装的目录（需mkdir）
4.在解压根目录执行make命令，完成对源码的编译
5.切换到roo用户，执行ln -s NGINX_HOME/sbin/nginx /usr/local/bin/nginx  //建立软链接以方便启动等操作
	如果在启动过程中报nginx: [emerg] bind() to 0.0.0.0:80 failed (13: Permission denied)错，可能是应为没有用root用户
	进而不能用80端口，所以需conf/nginx.conf中原server对象的listen值为8080即可
6.在浏览器输入http://192.168.88.128:8080/，如果看到Nginx的欢迎页面即为安装启动成功
7.需要关闭时，直接找到其pid然后kill -9 nginxPid
/********************************************/
 .\nginx.exe  //启动服务
 .\nginx.exe -t  //检测配置语法是否合法
 .\nginx.exe -s reload  //重启服务
 .\nginx.exe -s stop  //停止服务
/********************************************/
正向代理：是站在客户端的立场来说的，被代理的是客户端，设置在客户端，服务器感知不到客户端是否做了代理。
反向代理：是站在服务器端来说的，被代理的是服务器，设置在服务端。

Nginx反向代理配置：
http{
	server {  //此虚拟服务器专门部署静态资源
        listen       8880;
        server_name vm_static;
		location / {
            root D:/;  //注意：当有其他节点用了root时，这个root需要用alias替换
        }
	}
	
	upstream vertx_cluster {  //节点簇
		ip_hash;              //均衡策略算法
		server localhost:8080;  //热机
		server localhost:8081;
		
		server localhost:8082 backup;  //热备份节点
		server localhost:8083 backup;
	}
	
	server {  //此虚拟服务器专门部署Web服务
		listen	8888;  //代理监听端口
		server_name	youn.com;  //代理服务名称，可以任意民命
		location / {  //主目录
			root D:/;  //静态资源目录
			index index.php index.html index.htm;  //首页
			proxy_pass	http://vertx_cluster;  //引用节点簇
		}
	}
	
	server {  //这个服务器用于做【请求分发】
        listen       9999;
        server_name  vm_web;
		location /get/a/ {
			proxy_pass http://localhost:9092/get/a/;  //目的地1
        }
		location /get/b/ {
			proxy_pass http://localhost:9091/get/b/;  //目的地2
        }
		location /get/b/ {
			proxy_pass http://localhost:9091/get/c/;  //目的地3，如果链接后面有其他子路径，【斜杠必不可少】
        }
	}
}
/********************************************/
在Idea中要打包出可执行jar包【不论通过maven还是其他】，需要设置主类。
/********************************************/
Java項目中引入OpenCV需同時add jar包和本地lib方能成功執行。
/********************************************/
强制清除input标签边框和阴影
.form-group input {
	border: none !important;
	outline: none !important;
	background-color: white !important;
	box-shadow: inset 0 0px 0px rgba(0,0,0,.0) !important;
}
/********************************************/
String os = System.getProperty("os.name");
if(os.toLowerCase().startsWith("win")){
	System.out.println(os + " can't gunzip");
}
/********************************************/
File A = new File("src/main/java/openCV320/x64/opencv_java320.dll");  //获取动态链接库文件，库文件放在工程目录下
String filePath = A.getAbsoluteFile().getAbsolutePath();  //获取库文件绝对路径
System.load(filePath);  //在JVM中加载库
/********************************************/
Centos7安装Opencv320支持Java
1.下载opencv unix版源码包，创建/usr/local/share/opencv，并将压缩包放进去，解压
2.在解压的目录中新建build目录
3.进入build目录，执行【cmake ..】
4.若未安装cmake，用yum安装cmake工具
5.若未安装gcc-c++，用yum安装gcc-c++工具
6.再次执行【cmake ..】
7.若遇到ippicv在线下载不成功，需手动下载【https://raw.githubusercontent.com/Itseez/opencv_3rdparty/81a676001ca8075ada498583e4166079e5744668/ippicv/ippicv_linux_20151201.tgz】
  删除先在线未下载完全的压缩包，并将之上传到/usr/local/share/opencv/opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e
8.再次执行【cmake ..】
9.在build中执行make -j2
10.在build中执行make install
11.添加库路径到系统环境中：【/bin/bash -c 'echo "/usr/local/share/opencv/opencv-3.2.0/build/lib" > /etc/ld.so.conf.d/opencv.conf'】
12.更新环境变量：【ldconfig】
/********************************************/
1.查看状态:
iptables -L -n

打开端口方法1：
	下面添加对特定端口开放的方法：
	/sbin/iptables -I INPUT -p tcp --dport 8000 -j ACCEPT  //使用iptables开放如下端口
	/etc/rc.d/init.d/iptables save  //保存
	service iptables restart  //重启服务
	/etc/init.d/iptables status  查看需要打开的端口是否生效？

打开端口方法2：
	或直接编辑/etc/sysconfig/iptables
	-A INPUT -p tcp -m tcp --dport 4000 -j ACCEPT
	保存在前面部分
	再重启:
	service iptables restart
/********************************************/
@RequestMapping(value = "getJobList", method = RequestMethod.POST)
/********************************************/
Spring-boot搭配Jackson接收json格式参数
@ResponseBody
@SuppressWarnings("rawtypes")
@PostMapping(value="/postTest")  //限定POST请求
public String postTest(@RequestBody String json) {
	ObjectMapper jacksonMapper = new ObjectMapper();  //初始化一个jacksonMapper
	try {
		HashMap mapObj = jacksonMapper.readValue(json, HashMap.class);
		System.err.println(mapObj.get("originImagePath"));
		System.err.println(mapObj.get("id"));
	} catch (IOException e) {
		e.printStackTrace();
	}
	return "postTest";
}
/********************************************/
.wxml  //相当于小程序中的html文件
.wxss  //相当于小程序中的css文件
.js  //还是原来的JavaScript脚本文件

app.js  //整个应用的全局js，里面的数据整个应用都可以拿到
app.json  //整个应用的逻辑结构配置文件（定义页面与链接映射关系）
app.wxss  //
/********************************************/
2. 设置SSH无密码访问(所有节点)
执行一下命令，一路回车，生成无密码的密钥对。
# ssh-keygen -t rsa
将公钥添加到认证文件中, 并设置authorized_keys的访问权限
# cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
# chmod 600 ~/.ssh/authorized_keys
通过scp命令将各节点的认证文件拷贝到所有其他节点使得各节点能够相互访问。
/********************************************/
hostname  //Linux查看主机名
vim /etc/sysconfig/network  //Linux修改主机名，编辑该文件【效方式】
	NETWORKING=yes  //可选
	HOSTNAME=node1  //可选
	NETWORKING_IPV6=no  //可选
	PEERNTP=no  //可选
/********************************************/
1.避免大量缓存同时大面积失效
2.对热点数据持续高并发
	如果缓存没有值就从数据库取，用jvm同步锁。
	双缓存机制，主备
/********************************************/
基于Redis的分布式锁(非阻塞，但存在一定的数据一致性风险，可以短时容忍)
1.redis分布式锁必须要有一个过期时间。
2.加锁和失效时间必须是原子操作。
3.为锁设置一个随机值是有必要的。
4.释放锁包含三个原子操作，get、判断、delete【lua脚本】。

不能在redis集群中使用，如果是redis集群建议使用RedisLock。
/********************************************/
关机做快照。
init 0 || power off
/********************************************/
#for java
export JAVA_HOME=/usr/local/lang/jdk1.8.0_101
export JRE_HOME=/usr/local/lang/jdk1.8.0_101/jre
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

#for scala
export SCALA_HOME=/usr/local/lang/scala-2.12.0
export PATH=$PATH:$SCALA_HOME/bin
/********************************************/
du -sh ./myDir/  // 查看文件夹大小
/********************************************/
weblogic安装
1.图形界面安装32的
	java -jar wls1211_generic.jar

2.图形界面安装64的，需要64的jdk【有时需要管理员权限】
	java  -d64 -jar  wls1211_generic.jar

3.控制台安装模式
	java -jar wls1211_generic.jar -mode=console
/********************************************/
缓存雪崩式由于缓存的内容过期的间隙，大量请求直接访问数据库，用JVM锁或分布式锁解决缓存雪崩问题。
/********************************************/
缓存穿透
1.根据前端发来的key先查询缓存，
	如果缓存中有，就从缓存中拿value
	如果缓存中没有，就到数据库拿value，如果从数据库拿到了value就先把value放到缓存，然后再返回到前端
2.当大量请求的key都不能在缓存中拿到value时，这些请求就会到数据库去拿，造成数据库连接耗尽。
3.布农过滤器，guava有现成实现。
4.在前端与缓存间加上布农过滤器。
	基本思想是这样的：先将所有的key值从数据库查询出来，放到过滤器中，
	然后每次外部请求发来的key到来后，都用过滤器判断是否存在，如果不存在就直接返回空，不用缓存或数据库交互
	如果存在，就与缓存或数据库交互
/********************************************/
计数器在外部定义
int i = 0;
int leng = arr.length;
for(; i< leng; i++){
	System.out.println("eee");
}

//死循环
for(;;){
	System.out.println("eee");
}

while(true){
	System.out.println("eee");
}
/********************************************/
public static void main(String[] args) {
	Thread threadTest = new ThreadTest();  //初始化继承实例，既有run又有start方法
	threadTest.start();  //只有调用start方法才是以线程方式调用run逻辑
	
	RunnableTest runnableTest = new RunnableTest();  //初始化实现实例，只有run方法，没有start方法
	Thread tmpThread = new Thread(runnableTest);  //初始化线程实例，并将实现实例作为【构造参数】传进去
	tmpThread.start();  //由于实现式没有start方法来启动线程，所以必须将其传递给一个线程类对象，才能调用外层对象来启动线程
}
/********************************************/
Java读写锁
	当某一资源读多写少，允许同时进行多个读的操作，但只允许一个写的操作，
	比如一个文件，只要其内容不变可以让多个线程同时读，不必做排他的锁定，
	排他的锁定只有在写的时候需要，以保证别的线程不会看到数据不完整的文件
/********************************************/
CentOS7使用firewalld打开关闭防火墙与端口

1、firewalld的基本使用
	启动： systemctl start firewalld
	查看状态： systemctl status firewalld 
	禁用： systemctl disable firewalld
	停止： systemctl stop firewalld
 
2.systemctl是CentOS7的服务管理工具中主要的工具，它融合之前service和chkconfig的功能于一体。
	启动一个服务：systemctl start firewalld.service
	关闭一个服务：systemctl stop firewalld.service
	重启一个服务：systemctl restart firewalld.service
	显示一个服务的状态：systemctl status firewalld.service
	在开机时启用一个服务：systemctl enable firewalld.service
	在开机时禁用一个服务：systemctl disable firewalld.service
	查看服务是否开机启动：systemctl is-enabled firewalld.service
	查看已启动的服务列表：systemctl list-unit-files|grep enabled
	查看启动失败的服务列表：systemctl --failed

3.配置firewalld-cmd
	查看版本： firewall-cmd --version
	查看帮助： firewall-cmd --help
	显示状态： firewall-cmd --state
	查看所有打开的端口： firewall-cmd --zone=public --list-ports
	更新防火墙规则： firewall-cmd --reload
	查看区域信息:  firewall-cmd --get-active-zones
	查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0
	拒绝所有包：firewall-cmd --panic-on
	取消拒绝状态： firewall-cmd --panic-off
	查看是否拒绝： firewall-cmd --query-panic
 
*那怎么开启一个端口呢
	1.添加
		firewall-cmd --zone=public --add-port=80/tcp --permanent    【--permanent表示永久生效，没有此参数重启后失效】
	2.重新载入
		firewall-cmd --reload
	3.查看
		firewall-cmd --zone=public --query-port=80/tcp
	4.删除
		firewall-cmd --zone=public --remove-port=80/tcp --permanent
/********************************************/
Windws查看cpu参数
1.wmic  //进入脚本命令行界面
2.cpu get *  //获取cpu详情
/********************************************/
D:\noSpacePath\Oracle\user_projects\domains\base_domain  //weblogic启动脚本路径
/********************************************/
oozie-4.3.0]$ sh bin/mkdistro.sh -Phadoop-2 -Dhadoop.auth.version=2.6.5 -DskipTests  //【-DskipTests】选项表示跳过检测
oozie-4.3.0]$ sh bin/mkdistro.sh -Phadoop-2 -Dhadoop.auth.version=2.6.0 -Dspark.version=2.2.0 -DskipTests
/********************************************/
发起oozie流程调度报如下错误，原因及解决方法
oozie-error-code: E0501, oozie-error-message: E0501: Could not perform authorization operation, User: cdhfive is not allowed to impersonate cdhfive, Content-Type: text/html;charset

Reason: Cause of this type of error is- You run oozie server as a hadoop user but you define oozie as a proxy user in core-site.xml file.
Solution: change the ownership of oozie installation directory to oozie user and run oozie server as a oozie user and problem will be solved.

1.需要在单机版oozie运行的系统中新建一个oozie用户和oozie用户组，并将新用户添加到新组中。
2.并将oozie程序目录的所有者及所属组都改为新用户及群组。
3.然后以新用户启动oozie服务。

/********************************************/
sudo -u oozie hdfs dfs -put lib_20170807162244/ hdfs://node1.com:8020/user/oozie/share/lib/  //最好将要上传的文件放到/tmp目录下
/********************************************/
线程的实现式和继承式
1.由于java只支持单继承，所以当线程类继承了Thread类后就不能再继承其他类。
2.而实现Runnable接口就避免了上述局限性，建议使用。
/********************************************/
线程状态：
1.就绪：调用了线程对象的start方法，此时本线程具备了执行的前提条件。
2.执行：在就绪状态下，如果线程再取得了cpu的执行权限，线程就进入了执行状态，执行线程逻辑。
3.冻结：当线程被调用了sleep，或者wait等方法时，线程将离开就绪状态，直到睡眠结束或被唤醒才能重新进入临时状态，再去争取cpu。
4.消亡：当线程顺利执行完所有逻辑，或者被stop，或者被interrupt，线程就此消亡，其内存被回收。
5.临时：线程被阻塞，相当于临时挂起，相当于在绪状态下排队等待。
/********************************************/
线程安全问题出现的原因
1.操作同一共享变量的多个线程中，某些线程只执行了部分逻辑，而后被其他线程修改了共享变量，这样就导致了线程安全问题。
2.需要同步与读写共享变量的语句块，来保证线程安全，但尽量不要将整个run方法同步，否则就成了单线程了。
由于需要判断同步锁的状态， 所以会带来额外的性能开销。
/********************************************/
1.同步函数会同步函数所在的整个对象，即调用函数的对象，this   。
2.同步代码块只会同步传入的对象。
/********************************************/
是共用一个持有entity对象的runnable对象，还是多个runnable对象共同持有一个entity对象，同步块放在entity中。
/********************************************/
懒汉式单例模式可以实现延迟加载。
1.非线程安全的懒汉模式
public class Singleton{
	private static Singleton single = null;
	public static Singleton getSingleton(){
		if(single == null){
			single == new Singleton();
		}
		return single;
	}
}

2.低效线程安全的懒汉模式
public class Singleton{
	private static Singleton single = null;
	public static synchronize Singleton getSingleton(){  //每次都判断锁，增加额外开销
		if(single == null){
			single == new Singleton();
		}
		return single;
	}
}

3.线程安全又有效率的懒汉模式
public class Singleton{
	private static Singleton single = null;
	public static Singleton getSingleton(){
		if(single == null){
			synchronized (Singleton.class){  //只判断一次锁
				if(single == null){
					single = new Singleton();
				}
			}
		}
		return single;
	}
}
/********************************************/
饿汉式是线程安全的，但是不能延迟加载
public class Singleton{
	private static fainal Singleton single = new Singleton();
	
	private Singleton(){
		
	}
	
	public static Singleton getSingleton(){
		return single;
	}
}
/********************************************/
饿汉模式
public class CarteTaskStore {

	private Map<String, JobTask> jobMap;
	private Map<String, TransTask> transMap;

	private CarteTaskStore() {
		super();
		jobMap = new HashMap<String, JobTask>();
		transMap = new HashMap<String, TransTask>();
	}

	private static class Holder {
		private static final CarteTaskStore INSTANCE = new CarteTaskStore();
	}

	public static CarteTaskStore getInstance() {
		return Holder.INSTANCE;
	}
}
/********************************************/
在线程同步中wait、notify、notifyall方法属于被锁对象，作用于持有该对象的线程。
只有同一个被动等待线程才能被此锁上的notify方法唤醒。

1. 你可以使用wait和notify函数来实现线程间通信。你可以用它们来实现多线程（>3）之间的通信。
2. 永远在synchronized的函数或对象里使用wait、notify和notifyAll，不然Java虚拟机会生成 IllegalMonitorStateException。
3. 永远在while循环里而不是if语句下使用wait。这样，循环会在线程睡眠前后都检查wait的条件，并在条件实际上并未改变的情况下处理唤醒通知。
4. 永远在多线程间共享的对象（在生产者消费者模型里即缓冲区队列）上使用wait。
5. 基于前文提及的理由，更倾向用 notifyAll()，而不是 notify()。
/********************************************/
synchronized关键字如果可以
/********************************************/
1.用while判断满足条件
2.用notifyall唤醒线程。
/********************************************/
slmgr.vbs -xpr  //查看windows激活情况
/********************************************/
.master("local")  //本地代码调试必须用这个
/********************************************/
1.数据加载
2.数据预处理
3.基于预处理数据特征推导
4.基于特征训练模型
5.迭代尝试各种算法找出最佳模型
6.将挑选出的最佳模型集成到生产系统
/********************************************/
1.KMeans聚类时最基础，也是应用最广泛的聚类算法，整个过程使用贪心机制。
2.KMeans聚类时初始中心敏感的，所以初始中心应当均匀而随机，同时需要多次尝试，选出最优的一次作为最终结果。
/********************************************/
PCA
它是一种“投影(projection)技巧”，就是把高维空间上的数据映射到低维空间。
比如三维空间的一个球，往坐标轴方向投影，变成了一个圆。球是3维的，圆是2维的。
在球变成圆的这个投影过程中，丢失了原来物体(球)的一部分“性质”---圆不是球了，
只有面积没有体积了；也保留了原来物体的一部分性质---圆 和 球 还是很像的

为什么要降维：
1.可视化，我们只能看到一维、二维、三维空间上的物体，当某物体需要4维或以上特征表示时，只能想象了……
2.特征选择(feature selection)，比如说采集到的某个样本 由 20维特征 组成，其中有一些特征属于“噪音(noise)"，
而由于某些原因，我们不想要这些“噪音”。又比如说，存在特征冗余，描述一个球，可以用如下特征：(体积、面积、
直径、半径)，其实我只需要知道半径、直径、面积、体积这些都可以通过公式求出来。因此，体积、面积 、直径这些特征，
相对于半径来说，是冗余的特征。

1.可以使用 投影后的在各个方向上所有投影样本点的 的方差大小，来衡量投影方向的好坏！
/********************************************/
1.设有矩阵A、B，要求这两个矩阵的内积C，前提条件需满足column(A) == row(B);
2.内积C的行数继承A，列数继承自B
3.内积C的第m行第n列的元素等于矩阵A的第m行的元素与矩阵B的第n列对应元素乘积之和。

4.基本性质
	乘法结合律：(AB)C=A(BC)。
	乘法左分配律：(A+B)C=AC+BC。
	乘法右分配律：C(A+B)=CA+CB。
	对数乘的结合性k(AB）=(kA)B=A(kB）。
	转置 (AB)T=BTAT。
	矩阵乘法一般不满足交换律。
/********************************************/
1.期望：就是样本的算数平均值，取值范围为实数。
2.方差：各样本与期望差值的平方，然后求和，再用上步结果对n-1求均值，结果越大表示样本越离散，波动越大，取值范围非负。
3.标准差：标准差是对方差开平方的结果，标准差、均值、原特征值的 量纲（单位）是一致的，在实际应用中更贴合，取值范围非负。
4.协方差：上述三个指标描述的都是一维样本，当维度上升为 二维 后，要描述两个特征 间 的相关性就可以用两个特征的协方差，
  其取值范围为实数。如果为负表示两者呈负相关；如果为0表示两者不相关；如果为整表示两者呈正相关（仅适用于二维样本）。
  公式：如果有X，Y两个变量，每个时刻的“X值与其均值之差”乘以“Y值与其均值之差”得到一个乘积，再对这每时刻的乘积求和并求出均值，即为协方差
5.相关系数：皮尔森系数，描述两个特征的相关性，两个特征的协方差，除以两个特征标准差之积，其取值范围为[-1, 1]。
/********************************************/
1.转置：将一个矩阵以西南向对角线翻转180°，转置的行列式、秩与原矩阵的相同。
2.秩：有行秩和列秩之分，将一个矩阵进过行或列阶梯型变换后，得到的非全零行或列的数目就称为该矩阵的行秩或列秩。
/********************************************/
1.逻辑回归适用于二分类问题。
/********************************************/
1.测量误差所引起的噪声数据是导致overfiting的最要原因。
2.分类平面要尽量远离各样本点，这样的边界对误差的的容忍度会更高，发生overfiting的几率也会更小。
/********************************************/
1.有模式或者规律可以让机器学习。
2.人类不知道或者几乎无法用代码表示上述模式或规律。
3.需要有数据类训练机器。
/********************************************/
假设检验
1.我们不希望或者不能 否证 自己的研究命题，所以我们就搞个和研究假设【相反】的虚无假设。如果我们 否证 了虚无假设，就相当于我们证明了研究假设。
  所以假设检验就是要试图否证虚无假设，或者拒绝虚无假设，通过否定对立面来迂回的证明自己的研究命题。
2.卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，
  卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。
/********************************************/
1.通过向Estimator喂食训练数据集得到Transformer
	val transformer = estimator.fit(trainData);
2.通过向Estimator喂食测试数据集得到结果数据集
	val resultData = transformer.transform(testDataFrame);
/********************************************/
1.pipeline
	1.1 评估器：是原生的算法实现，还未学习到能力，具有fit()方法，需要拟合喂食的训练集，方能学习到能力，所以，对于那些【不需要训练】的算法实现，
		天生就不是评估器。
	1.2 转换器：是原生算法实现经过拟合训练集后，得到的具有能力的算法模型，具有transform()方法，喂食给他的测试集经过转换后回生成结果集，
		结果集可以是最终结果，也可能是下一个模型的输入数据。
	1.3 pipeline：是一系列转换器和评估器串联成的一个高级“评估器”，传递训练数据并调用流水线的fit()方法后，流水线中的所有评估器都将进化为转换器。
	1.4 pipelineModel：是一系列转换器串联而成的“转换器”，调用其transform()方法后，传递进来的测试数据将被转换为结果集。
	
2.基本统计
	1.1 相关性分析：可求样本矩阵的相关性矩阵，默认相关系数类型为pearson系数，也可以指定为spearman系数。
	1.2 假设验证：支持卡方验证。
/********************************************/
导函数：也简称导数，是一个函数，是指在自变量可导区间内，函数值的变化量与自变量的变化量的比值，代表的是在该区间内函数值的变化率的表达式。
某点导数：也简称该点导数，是一个数值，是指将该点自变量值带入导函数所得到的函数值。
微分：
积分：给出某函数的导函数，求原函数。

/********************************************/
导数 = 微商 = 函数的微分 / 自变量的微分
/********************************************/
Ctrl+Shift + Enter，语句完成
“！”，否定完成，输入表达式时按 “！”键
Ctrl+E，最近的文件
Ctrl+Shift+E，最近更改的文件
Shift+Click，可以关闭文件
Ctrl+[ OR ]，可以跑到大括号的开头与结尾
Ctrl+F12，可以显示当前文件的结构
Ctrl+F7，可以查询当前元素在当前文件中的引用，然后按 F3 可以选择
Ctrl+N，可以快速打开类
Ctrl+Shift+N，可以快速打开文件
Alt+Q，可以看到当前方法的声明
Ctrl+P，可以显示参数信息
Ctrl+Shift+Insert，可以选择剪贴板内容并插入
Alt+Insert，可以生成构造器/Getter/Setter等
Ctrl+Alt+V，可以引入变量。例如：new String(); 自动导入变量定义
Ctrl+Alt+T，可以把代码包在一个块内，例如：try/catch
Ctrl+Enter，导入包，自动修正
Ctrl+Alt+L，格式化代码
Ctrl+Alt+I，将选中的代码进行自动缩进编排，这个功能在编辑 JSP 文件时也可以工作
Ctrl+Alt+O，优化导入的类和包
Ctrl+R，替换文本
Ctrl+F，查找文本
Ctrl+Shift+Space，自动补全代码
Ctrl+空格，代码提示（与系统输入法快捷键冲突）
Ctrl+Shift+Alt+N，查找类中的方法或变量
Alt+Shift+C，最近的更改
Alt+Shift+Up/Down，上/下移一行
Shift+F6，重构 – 重命名
Ctrl+X，剪切行
Ctrl+D，复制行
Ctrl+/或Ctrl+Shift+/，注释（//或者/**/）
Ctrl+J，自动代码（例如：serr）
Ctrl+Alt+J，用动态模板环绕
Ctrl+H，显示类结构图（类的继承层次）
Ctrl+Q，显示注释文档
Alt+F1，查找代码所在位置
Alt+1，快速打开或隐藏工程面板
Ctrl+Alt+left/right，返回至上次浏览的位置
Alt+left/right，切换代码视图
Alt+Up/Down，在方法间快速移动定位
Ctrl+Shift+Up/Down，向上/下移动语句
F2 或 Shift+F2，高亮错误或警告快速定位
Tab，代码标签输入完成后，按 Tab，生成代码
Ctrl+Shift+F7，高亮显示所有该文本，按 Esc 高亮消失
Alt+F3，逐个往下查找相同文本，并高亮显示
Ctrl+Up/Down，光标中转到第一行或最后一行下
Ctrl+B/Ctrl+Click，快速打开光标处的类或方法（跳转到定义处）
Ctrl+Alt+B，跳转到方法实现处
Ctrl+Shift+Backspace，跳转到上次编辑的地方
Ctrl+O，重写方法
Ctrl+Alt+Space，类名自动完成
Ctrl+Alt+Up/Down，快速跳转搜索结果
Ctrl+Shift+J，整合两行
Alt+F8，计算变量值
Ctrl+Shift+V，可以将最近使用的剪贴板内容选择插入到文本
Ctrl+Alt+Shift+V，简单粘贴
Shift+Esc，不仅可以把焦点移到编辑器上，而且还可以隐藏当前（或最后活动的）工具窗口
F12，把焦点从编辑器移到最近使用的工具窗口
Shift+F1，要打开编辑器光标字符处使用的类或者方法 Java 文档的浏览器
Ctrl+W，可以选择单词继而语句继而行继而函数
Ctrl+Shift+W，取消选择光标所在词
Alt+F7，查找整个工程中使用地某一个类、方法或者变量的位置
Ctrl+I，实现方法
Ctrl+Shift+U，大小写转化
Ctrl+Y，删除当前行


Shift+Enter，向下插入新行
psvm/sout，main/System.out.println(); Ctrl+J，查看更多
Ctrl+Shift+F，全局查找
Ctrl+F，查找/Shift+F3，向上查找/F3，向下查找
Ctrl+Shift+S，高级搜索
Ctrl+U，转到父类
Ctrl+Alt+S，打开设置对话框
Alt+Shift+Inert，开启/关闭列选择模式
Ctrl+Alt+Shift+S，打开当前项目/模块属性
Ctrl+G，定位行
Alt+Home，跳转到导航栏
Ctrl+Enter，上插一行
Ctrl+Backspace，按单词删除
Ctrl+”+/-”，当前方法展开、折叠
Ctrl+Shift+”+/-”，全部展开、折叠
【调试部分、编译】
Ctrl+F2，停止
Alt+Shift+F9，选择 Debug
Alt+Shift+F10，选择 Run
Ctrl+Shift+F9，编译
Ctrl+Shift+F10，运行
Ctrl+Shift+F8，查看断点
F8，步过
F7，步入
Shift+F7，智能步入
Shift+F8，步出
Alt+Shift+F8，强制步过
Alt+Shift+F7，强制步入
Alt+F9，运行至光标处
Ctrl+Alt+F9，强制运行至光标处
F9，恢复程序
Alt+F10，定位到断点
Ctrl+F8，切换行断点
Ctrl+F9，生成项目
Alt+1，项目
Alt+2，收藏
Alt+6，TODO
Alt+7，结构
Ctrl+Shift+C，复制路径
Ctrl+Alt+Shift+C，复制引用，必须选择类名
Ctrl+Alt+Y，同步
Ctrl+~，快速切换方案（界面外观、代码风格、快捷键映射等菜单）
Shift+F12，还原默认布局
Ctrl+Shift+F12，隐藏/恢复所有窗口
Ctrl+F4，关闭
Ctrl+Shift+F4，关闭活动选项卡
Ctrl+Tab，转到下一个拆分器
Ctrl+Shift+Tab，转到上一个拆分器
【重构】
Ctrl+Alt+Shift+T，弹出重构菜单
Shift+F6，重命名
F6，移动
F5，复制
Alt+Delete，安全删除
Ctrl+Alt+N，内联
【查找】
Ctrl+F，查找
Ctrl+R，替换
F3，查找下一个
Shift+F3，查找上一个
Ctrl+Shift+F，在路径中查找
Ctrl+Shift+R，在路径中替换
Ctrl+Shift+S，搜索结构
Ctrl+Shift+M，替换结构
Alt+F7，查找用法
Ctrl+Alt+F7，显示用法
Ctrl+F7，在文件中查找用法
Ctrl+Shift+F7，在文件中高亮显示用法
/********************************************/
psvm === main alt + /  //生成main函数
sout === sysout  //生成System.out.println();
fori  //会生成一个for循环结构

/********************************************/
ArrayList<ImageRatio> ratioImageList = jacksonMapper.readValue(RedisUtils.getByKey(key), new TypeReference<List<ImageRatio>>(){});  //利用jackson解析ImageRatio泛型json数组
/********************************************/
Hadoop之core-site.xml配置允许oozie调度
<property>
	<name>hadoop.proxyuser.hadoop.hosts</name>  //其中【hadoop】表示启动hadoop集群的用户
	<value>desk0</value>  //其中【desk0】表示启动oozie服务的主机名
</property>
<property>
	<name>hadoop.proxyuser.hadoop.groups</name> //其中【hadoop】表示启动hadoop集群的用户
	<value>oozieGroup</value>  //其中【oozieGroup】表示启动oozie服务的用户所属组
</property>
/********************************************/
Windows版Redis程序包下载后直接解压
1.如果直接点击redis-server.exe运行，无初始密码，如需配置需要修改redis.windows.conf中requirepass myPassword
2.doc命令启动服务并且带上配置参数，redis-server.exe redis.windows.conf
3.打开redis-cli.exe，开启客户端用，由于设置了访问密码，此时不能读取数据库，auth myPassword方可进入
4.登录后可以查看密码，config get requirepass
5.设置进程密码，config set requirepass myNewPassword
/********************************************/
公司现在用的是springboot+maven，想要把一些老的项目都改成这种框架。但是一些老的项目中有好多第三方的jar包或者是自己的jar包，
maven库上没有。最初的解决方案是一个个的deploy到maven库上，但是遇到太多的三方jar包就太费事了。
网上查了一下发现maven有一种方式可以将本地的Jar包依赖到项目中而不需要先deploy到maven库上。具体做法如下：

1.跟普通的java项目一样，新建个lib目录放jar包，我建在了src下，但scope必须是system。
pom文件中依赖这样写：
<!--添加外部依赖-->
<dependency>
	<groupId>Ice</groupId>
	<artifactId>Ice</artifactId>
	<version>1.0</version>
	<scope>system</scope>
	<systemPath>${basedir}/src/lib/Ice.jar</systemPath>
</dependency>
basedir是自带变量，指的是当前项目的绝对路径，三围自己随便写。

2.【并不生效】通过这种方式引入依赖后，在编码的时候没有问题，但是用mvn clean package打成jar包后运行就是报错，
正式找不到这些jar包中的相关类。把jar包解压后在BOOT-INF/lib里面也确实没有发现用上述方式引入的jar包，
看来是没有把src/lib目录里面的jar打进去，后来通过查找也终于找到方法了，修改pom文件，在【build标签】下增加如下配置：

<resources>
	<resource>
		<directory>src/lib</directory>
		<targetPath>BOOT-INF/lib/</targetPath>
		<includes>
			<include>**/*.jar</include>
		</includes>
	</resource>
	<resource>
		<directory>src/main/resources</directory>
		<targetPath>BOOT-INF/classes/</targetPath>
	</resource>
</resources>
目的就是把src/lib下的jar打到BOOT-INF/lib下，但是必须要增加第二个resource，不然src/main/resources下的配置文件又打不进jar包了。

3.如果要打成war包，则pom文件需要这样改：
<!--设置maven-war-plugins插件，否则外部依赖无法打进war包-->
<plugin>
	<groupId>org.apache.maven.plugins</groupId>
	<artifactId>maven-war-plugin</artifactId>
	<configuration>
		<webResources>
			<resource>
				<directory>lib</directory>
				<targetPath>WEB-INF/lib/</targetPath>
				<includes>
					<include>**/*.jar</include>
				</includes>
			</resource>
		</webResources>
	</configuration>
</plugin>
/********************************************/
【由于上述方法均不生效，需要直接将外部jar包放到本地repository】
1.在pom中添加依赖
	<dependency>
		<groupId>com.youn</groupId>
		<artifactId>mySfnttool</artifactId>
		<version>1.0</version>
	</dependency>
2.D:\appData\m2\repository\com\youn\mySfnttool\1.0\mySfnttool-1.0.jar
3.jar包命名、jar包在仓库中的路径，必须与pom中依赖的三维坐标、相匹配。
/********************************************/
1.用jackson将json字符串解析为指定对象时，即使字符串中缺少部分字段的key-value对，也能顺利解析，只是空缺字段的值为空白值。
2.用jackson解析json字符串，key和String类型的value都必须用双引号括起来。
/********************************************/
#for java 1.8-u144
export JAVA_HOME=/usr/local/lang/jdk1.8.0_144
export JRE_HOME=/usr/local/lang/jdk1.8.0_144/jre
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib
export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

#for scala 2.12.0
export SCALA_HOME=/usr/local/lang/scala-2.12.0
export PATH=$PATH:$SCALA_HOME/bin

#for maven 3.3.9
export MAVEN_HOME=/usr/local/apache-maven-3.3.9
export PATH=$PATH:$MAVEN_HOME/bin

#for spark2.0.0
export SPARK_HOME=/opt/spark-2.0.0-bin-hadoop2.6
export PATH=$PATH:$SPARK_HOME/bin

#for oozie 4.3.0
export OOZIE_HOME=/opt/oozie-4.3.0
export PATH=$PATH:$OOZIE_HOME/bin

#for oozie-client 4.3.0
export OOZIE_CLIENT_HOME=/opt/oozie-client-4.3.0
export PATH=$PATH:OOZIE_CLIENT_HOME/bin

#for hadoop 2.6.0
export HADOOP_HOME=/opt/hadoop-2.6.0
export PATH=$PATH:$HADOOP_HOME/bin
/********************************************/
/********************************************/
spark on yarn 提交app：【jar包需在本地，输入输出路径需在dfs上】
spark-submit --master yarn --class org.apache.oozie.example.SparkFileCopy oozie-examples.jar /user/root/sparkTest/spark/input/data.txt hdfs://127.0.0.1:9000/user/root/sparkTest/spark/output/out
/********************************************/
oozie job -oozie http://desk0:11000/oozie -config job.properties -run
/********************************************/
<!-- 长appattmpt配置，为解决attempt out of range警告导致的yarn accept app 但是一直unassisted -->
<property>
	<name>yarn.resourcemanager.am.max-attempts</name>
	<value>4</value>
</property>
/********************************************/
伪分布模式下：spark_home/conf/slaves配置文件中的节点应该配位hostname
/********************************************/
1.oozie spark on yarn报下列错误
java.lang.IllegalStateException: Library directory '/root/hadoop2.6/temp/nm-local-dir/usercache/root/appcache
/application_1504058501839_0002/container_1504058501839_0002_01_000001/./assembly/target/scala-2.10/jars' 
does not exist; make sure Spark is built.
2.解决办法：【会导致shell方式报错】
 在spark-defaults.conf中添加
 spark.yarn.jars *.jar
/********************************************/
yarn之historyserver配置
mapred-site.xml里
<property>  
	<name>mapreduce.jobhistory.address</name>  
	<value>127.0.0.1:10020</value>  
</property>
/********************************************/
1.在linux中用户的历史命令保存在/home/user_name/.bash_history下
2.用history命令可以查看到，但是默认只保存最近1000条。
3.可以用echo $HISTSIZE查看条数。
4.如果需要临时修改条数可以用HISTSIZE=200，将条数修改为200，但是重启后会重置为默认值
5.如果需要永久修改条数，则需要修改/etc/profile文件中的相关参数
/********************************************/
在hdfs-site.xml配置文件里加上如下内容：
<property>
	<name>dfs.replication</name>
	<value>2</value>  //冗余备份数量，可以设为1
</property>

<property>
	<name>dfs.blocksize</name>
	<value>2m</value>  //如果没有单位就表示Byte【51200就表示50K】
</property>

<property>
	<name>dfs.namenode.fs-limits.min-block-size</name>
	<value>2m</value>  //块最小尺寸，默认值为128M，所以如果块大小被修改为小于128M后需要指定最小值，不得大于块尺寸
</property>
然后重启Hadoop集群，新上传的文件就以2MB的块大小存储了
也可以重新格式化，但是hdfs上的所有内容将被删除。
/********************************************/
启动VMWares虚拟机实例，出现模块“Disk”启动失败错误
1.找到虚拟机所在的目录将.vmx文件打开
	将文件vmci0.present = "TRUE"
	改为 vmci0.present = "FALSE"
2.删除以.lck为后缀名的文件
3.重新打开虚拟机
/********************************************/
多个标量有序排列后形成向量，多个向量有序排列后形成矩阵，多个矩阵有序排列后形成三维张量（3D tensor）。 
所以标量可以视为元素个数为1的向量，向量可以视为元素个数为1的矩阵，矩阵可以视为元素个数为1的三维张量（3D tensor）。
/********************************************/
下列代码可以打印程序运行时的【根路径】：【需要读取文件时，在文件全路径的基础上，截去此根路径，剩下的部分就可作为读取路径】
File f = new File(".");
System.out.println(f.getAbsolutePath());
/********************************************/
对于一个指示函数集，如果存在h个数据样本能够被函数集中的函数按所有可能的2^h【2的h次方】种形式分开 ，则称函数集能够把h个数据样本打散（shatter）。
函数集的VC维就是能打散的最大数据样本数目h。若对任意数目的数据样本都有函数能将它们shatter，则函数集的VC维为无穷大。
VC维在机器学习中的意义还是很大的，它是统计学习理论用来衡量函数集性能的一种指标——VC维越大，则学习过程越复杂。
/********************************************/
System.out.println(new Random().nextInt(10));  //取[0, 10)区间的随机数，左闭右开。
/********************************************/
如果是CPU密集型应用，则线程池大小设置为N+1
如果是IO密集型应用，则线程池大小设置为2N+1
/********************************************/
保留两位小数【不四舍五入】
public static float getFloatRemainTwoDot(final float originFloat){
   return ((float)((int) (originFloat * 100))) / 100;
}
/********************************************/
Win10输入法之繁简切换【繁體簡體】
Ctrl + Shift + F
/********************************************/
property = new Properties();
InputStream inputStream = SessionUtil.class.getClassLoader().getResourceAsStream(SESSION_FILE_PATH);
property.load(inputStream);
/********************************************/
一定要用【getClassLoader】
InputStream inputStream = ExcelUtils.class.getClassLoader().getResourceAsStream("/" + fileName);
/********************************************/
动态加载少出问题/更实用
property = new Properties();
property.load(new FileInputStream(SESSION_FILE_PATH));
/********************************************/
File.separator;  //该静态变量会返回与当前文件系统匹配的分隔符
/********************************************/
byte：8位，最大存储数据量是255，存放的数据范围是-128~127之间。
short：16位，最大数据存储量是65536，数据范围是-32768~32767之间。
int：32位，最大数据存储容量是2的32次方减1，数据范围是负的2的31次方到正的2的31次方减1。
long：64位，最大数据存储容量是2的64次方减1，数据范围为负的2的63次方到正的2的63次方减1。
float：32位，数据范围在3.4e-45~1.4e38，直接赋值时必须在数字后加上f或F。
double：64位，数据范围在4.9e-324~1.8e308，赋值时可以加d或D也可以不加。
boolean：只有true和false两个取值。
char：16位，存储Unicode码，用单引号赋值。
/********************************************/
在Spring-Boot项目中添加下列依赖，项目启动后就可以通过浏览器中，
在访问应用地址后输入/health 或者/metrics去检查应用的健康情况或者指标。
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
/********************************************/
File.separator;  //根据程序运行环境【linux/windows】，自动判断文件系统分割符，输出：【\】
File.pathSeparator[Char];  //输出：【;】
/********************************************/
@Value("${thread.pool.size}")
private int threadPoolSize;  //动态读取配置文件，但是成员变量不能用static修饰
/********************************************/
spring-boot项目url根路径配置
server.context-path=/have  //必须以【/】开始
/********************************************/
应用启动类Application.java应置于root package下，这样使用到@ComponentScan【它被融合在@SpringBootApplication中】的时候默认的扫描起点就是当前所在类的package。
/********************************************/
如果一个常量需要调用构造函数，如果其构造过程不涉及额为逻辑，可以不用静态块，此时常量也会在类加载时被初始化！
static final ExecutorService fixedPool = Executors.newFixedThreadPool(6);
/********************************************/
机器学习：从训练集出发，找到一个能够预测测试集的的函数。
数据挖掘：从大规模数据出发，找到数据中的有价值属性或特征。
/********************************************/
Hibernate之注解配置多对多关联，必须通过【中间关系表】
@ManyToMany
@JoinTable(name = "hbz_user_role", joinColumns = {@JoinColumn(name = "role")}, inverseJoinColumns = {@JoinColumn(name = "user")})  //外联关系表
private List<HbzUser> users;
其中：joinColumns是主操作表的中间表列，而inverseJoinColumns是副操作表的中间表列。
/********************************************/
一个简单的枚举类
public enum RoleType {
    Admin("管理"), Driver("司机"), Consignor("货主");
    private String name;
	
    private RoleType(String name) {
        this.name = name;
    }
	
    public String getName() {
        return name;
    }
}
/********************************************/
Bean注解都打在有返回的方法上
@Bean
public MyBean myBean() {
	return new MyBean();  //初始化一个实例对象，并将其交给Spring之IOC容器管理
}
/********************************************/
Spring-data-jpa中查询方法名的规范和语义
And --- 等价于 SQL 中的 and 关键字，比如 findByUsernameAndPassword(String user, Striang pwd)；
Or --- 等价于 SQL 中的 or 关键字，比如 findByUsernameOrAddress(String user, String addr)；
Between --- 等价于 SQL 中的 between 关键字，比如 findBySalaryBetween(int max, int min)；
LessThan --- 等价于 SQL 中的 "<"，比如 findBySalaryLessThan(int max)；
GreaterThan --- 等价于 SQL 中的">"，比如 findBySalaryGreaterThan(int min)；
IsNull --- 等价于 SQL 中的 "is null"，比如 findByUsernameIsNull()；
IsNotNull --- 等价于 SQL 中的 "is not null"，比如 findByUsernameIsNotNull()；
NotNull --- 与 IsNotNull 等价；
Like --- 等价于 SQL 中的 "like"，比如 findByUsernameLike(String user)；
NotLike --- 等价于 SQL 中的 "not like"，比如 findByUsernameNotLike(String user)；
OrderBy --- 等价于 SQL 中的 "order by"，比如 findByUsernameOrderBySalaryAsc(String user)；
Not --- 等价于 SQL 中的 "！ ="，比如 findByUsernameNot(String user)；
In --- 等价于 SQL 中的 "in"，比如 findByUsernameIn(Collection<String> userList) ，方法的参数可以是 Collection 类型，也可以是数组或者不定长参数；
NotIn --- 等价于 SQL 中的 "not in"，比如 findByUsernameNotIn(Collection<String> userList) ，方法的参数可以是 Collection 类型，也可以是数组或者不定长参数；
/********************************************/
https
1.认证正在访问的网站，确保用户访问的不是钓鱼网站。
2.保证所传输数据的私密性和完整性，确保数据在传输过程中不被窃取、篡改。
/********************************************/
工作机制
1.client向server发送首次访问请求。

2.server向client返回server证书，其中包含server非对称公钥。

3.client判断接收到的证书是否由受信任机构颁发。
4.client随机生成clientRSA非对称公私钥、client对称会话密钥。
5.client使用server非对称公钥加密client非对称公钥、client对称会话密钥，并发送给server。

6.server接收到数据后，使用server私钥还原出client非对称公钥、client对称会话密钥。
7.server随机生成server对称会话密钥。
8.server用client非对称公钥加密server对称会话密钥，并发送给client。

9.client用client私钥还原数据，得到server对称会话密钥。

至此，client和server都分别持有对方的非对称公钥和对称会话密钥，
接下来的数据传递就用这两对的对称会话密钥来加密和还原。
之所以使用两个对称密钥，是出于性能的考量。
/********************************************/
工作过程
1.认证服务器。浏览器内置一个受信任的CA机构列表，并保存了这些CA机构的证书。第一阶段服务器会提供经CA机构认证颁发的服务器证书，
如果认证该服务器证书的CA机构，存在于浏览器的受信任CA机构列表中，并且服务器证书中的信息与当前正在访问的网站（域名等）一致，
那么浏览器就认为服务端是可信的，并从服务器证书中取得服务器公钥，用于后续流程。否则，浏览器将提示用户，根据用户的选择，决定是
否继续。当然，我们可以管理这个受信任CA机构列表，添加我们想要信任的CA机构，或者移除我们不信任的CA机构。

2.协商会话密钥。客户端在认证完服务器，获得服务器的公钥之后，利用该公钥与服务器进行加密通信，协商出两个会话密钥，分别是用于加
密客户端往服务端发送数据的客户端会话密钥，和用于加密服务端往客户端发送数据的服务端会话密钥。在已有服务器公钥，可以加密通讯的前提下，
还要协商两个对称密钥的原因，是因为非对称加密相对复杂度更高，在数据传输过程中，使用对称加密，可以节省计算资源。另外，会话密钥是随
机生成，每次协商都会有不一样的结果，所以安全性也比较高。

3.加密通讯。此时客户端服务器双方都有了本次通讯的会话密钥，之后传输的所有Http数据，都通过会话密钥加密。这样网路上的其它用户，
将很难窃取和篡改客户端和服务端之间传输的数据，从而保证了数据的私密性和完整性。
/********************************************/
Spring-Boot替换Tomcat容器
1.排除掉默认的tomcat
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-web</artifactId>
	<exclusions>
		<exclusion>
			<groupId>org.springframework.boot</groupId>
			<artifactId>spring-boot-starter-tomcat</artifactId>
		</exclusion>
	</exclusions>
</dependency>
2.添加新的容器依赖
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-jetty[undertow]</artifactId>
</dependency>
/********************************************/
在Java中将字符串表达式转换为运算表达式，下面是一个计算器的实现
class Calculator {

    private final static ScriptEngine scriptEngine = new ScriptEngineManager().getEngineByName("JavaScript");

    public static Object calculate(String expression) throws ScriptException {
        return scriptEngine.eval(expression);
    }

    public static void main(String[] args) {
        try {
            System.out.println(calculate("3 >= 4"));
        } catch (ScriptException e) {
            e.printStackTrace();
        }
    }
}
输出结果为false。
/********************************************/
1.BIO模型：是同步阻塞，通信采用“一请求一线程”模型，即服务端每接收到一个客户端请求都会新启动一个线程来应答，
	对应的，每一次请求结束对应线程也随之销毁，当并发量很大时服务端资源消耗严重，甚至挂掉。
2.伪BIO模型：是同步阻塞，只是在BIO模型的实现中将线程的创建和销毁交给线程池来管理，这样可以控制线程的数量不至于爆掉，
	但是由于线程数有限，高并发的情况下所有线程都在使用中时，必然会导致新请求的等待，这个弊端不能避免。
2.NIO模型：是同步非阻塞。
4.AIO模型：是异步非阻塞。
/********************************************/
一个接口的多个实现类被注入Spring容器的正确处理方式。
org.springframework.beans.factory.NoSuchBeanDefinitionException: No unique bean of type 
[com.xxx.xxx.xxx.integration.dao.IDAO] is defined: expected single matching bean but found 2: [aDAOImpl, bDAOImpl]

1.两个类实现同一个接口，在一个实现类加上@Component("aDAOImpl")，一个加上@Component("bDAOImpl")，
2.在引用的时候加上@Resource(name="aDAOImpl")或者@Resource(name="bDAOImpl")，就会指定用哪个实现类，
3.在引用出如果不加resource注解就会出现上面那个错误。
【注意】当有其它的service引用到上面的某个实现类时，如果没用@Resource的装配方式是会报错的，
	需把所引用到实现类的注解由@Autowired改为@Resource。
/********************************************/
JPA及Hibernate映射的方向性的价值：保证从乙方出发能get到另一方，其实质体现在【实体间的持有关系】上。
/********************************************/
【1】一对一：
【2】一对多：
	存在三种情况：单向1对多、单向多对1和双向1对多，在数据库表达方面都是一样的，都是多的一方使用外键指向一的一方的主键，但是java的表达存在不同。
	【2.1】单向一对多：可以从1的一方（部门）找到多的一方(员工)，反过来就不可以。
			
	【2.2】单向多对一：只在多的一方实体持有单的一方对象作为属性，或者只在单的一方实体持有多的集合作为属性。
		在JPA中，用@OneToMany来标识一对多的关系。实现一对多的单向关联，只需在代表【一】的实体(User)中，在【多的集合属性上】使用@OneToMany映射标注就可以了，
		代表多的实体不需要使用任何映射标注。
		单向关联，有两种方式实现
		1.一种是在只使用@OneToMany和@ManyToOne及给他们其中一个mapped属性来标识，这种方式是自动生成一张第三方【关系表】来保存关联关系的。
		2.另一种是使用@OneToMany和@JoinColumn两个标注，这种方式是在多的一方(Phone)的表中增加一个外键列来保存关系。
		情景假设：一个用户拥有多个手机，从User到Phone的一对多
	【2.3】双向：需要在单的一方实体持有多的集合，同时要在多的一方实体持有单的乙方对象。
		
【3】多对多：

/********************************************/
级联：用来描述对主对象的某种操作，是否会对从对象有同样的影响；
	比如一个Department对象持有多个Employee对象，并设置了delet型级联，若删除Department对象，其持有的多个Employee也会被删除，修改等其他操作也类似。
	【一般】多对多、多对一不设级联；一对一、一对多要设级联。
控制：表示由哪个实体的数据库表设置外键来维护关联关系。
/********************************************/
尽量用@JoinColumn(name = "userId")来标识关联字段，不用mappedBy属性。
不推荐mappedBy = "user"  //其中user对应对方实体的成员变量名，并且以这个属性所代表的对象id作为外键
/********************************************/
Spring-Boot热部署
1.使用spring-load包（只能实现方法内代码的热部署，添加、删除方法后不能热响应）
	1.1在pom.xml中的build节点添加plugin，spring-boot:run方式启动，优点是配置、启动方便，但是存在Tomcat容器不能停止问题，必须通过进程管理器才能杀掉。
	1.2使用将spring-load.jar加入到项目下，在时配置jvm参数加载此jar包，优点是能顺利杀死进程，但是配置、启动略显麻烦。
2.spring-load-devtools，通过两个classLoader分别加载永久class和瞬态class，当代码改动时立即重启应用，
  但是由于只替换瞬态restartClassLoader，可以实现数秒内完成重启，这样就能完美而彻底的刷新一切代码的改动。
	2.1在pom.xml中添加spring-boot-devtools依赖（注意作用域）。
	2.2在pom.xml中添加build相关的plugin。
/********************************************/
【devtools插件配置】
1.在pom.xml中添加devtools依赖
	<dependency>
		<groupId>org.springframework.boot</groupId>
		<artifactId>spring-boot-devtools</artifactId>
		<!-- optional=true,依赖不会传递，该项目依赖devtools；之后依赖myboot项目的项目如果想要使用devtools，需要重新引入 -->
		<optional>true</optional>
	</dependency>
2.project 中添加 spring-boot-maven-plugin,主要在eclipse中使用，idea中不需要添加此配置。
	<build>
		<plugins>
			<plugin>
				<groupId>org.springframework.boot</groupId>
				<artifactId>spring-boot-maven-plugin</artifactId>
				<configuration>
					<fork>true</fork>
				</configuration>
			</plugin>
		</plugins>
	</build>
3.更改idea配置
	3.1“File” -> “Settings” -> “Build,Execution,Deplyment” -> “Compiler”，选中打勾 “Build project automatically” -> done
	3.2组合键：“Shift+Ctrl+Alt+/” ，选择 “Registry” ，选中打勾 “compiler.automake.allow.when.app.running” -> done
4.禁用Chrome浏览器缓存
　　F12或者“Ctrl+Shift+I”，打开开发者工具，“Network” 选项卡下 选中打勾 “Disable Cache(while DevTools is open)” 
/********************************************/
Java多线程五个注意点
1.共享变量要用volatile修饰，以保证变量对多个线程的可见性，防止缓存对赋值过程的延迟，保证每个线程的写操作是即时刷写到内存的，读操作也是直接读取内存的。
	内存--->CPU多级缓存--->CPU寄存器--->CPU核心
2.对int、long类型的共享变量的多线程读写应该使用原子类型，或者longAdder。
3.HashMap类型的共享变量的多线程读写要使用ConcurrentHashMap来保证线程安全，因为HashMap不是线程安全的可能出现死锁。
4.
5.被synchronize修饰的同步变量必须要是final修饰的，由于synchronize修饰的同步变量是给对象地址加锁的，旧地址加锁后一旦引用指针指向新地址，
	新地址未加锁，那么代码中的锁就失效了；所以要用final修饰同步变量杜绝指针指向的改变，保证自始至终变量指向的是同一块地址，这才能保证该对象是真正被锁住的。
/********************************************/
Interface新特性
静态变量：可以直接通过接口访问，类似普通类的静态变量访问机制。
静态函数：可以有实现，可以通过接口直接访问，类似普通类的静态函数访问机制。
默认函数：可以有实现，但是只能通过实现类访问，类似函数的继承。
/********************************************/
/**
 * 手动构造查询条件
 * @param hbzOrderDTO
 * @return
 */
public Specification<HbzOrder> getHbzOrderSpecification(HbzOrderDTO hbzOrderDTO) {
	return (root, query, criteriaBuilder) -> {
		List<Predicate> predicateList = new ArrayList<>();

		if(hbzOrderDTO.getOrderType() != null && StringUtils.isNotBlank(hbzOrderDTO.getOrderType().getName())) {
			predicateList.add(criteriaBuilder.equal(root.get("orderType").get("name"), hbzOrderDTO.getOrderType().getName()));
		}
		if(StringUtils.isNotBlank(hbzOrderDTO.getOrderName())) {
			predicateList.add(criteriaBuilder.like(root.get("orderName"), "%" + hbzOrderDTO.getOrderName() + "%"));
		}
		if (StringUtils.isNotBlank(hbzOrderDTO.getOrderNo())) {
			predicateList.add( criteriaBuilder.equal(root.get("orderNo"), hbzOrderDTO.getOrderNo()) );
		}

		/*Predicate[] predicates = new Predicate[predicateList.size()];
		return criteriaBuilder.and(predicateList.toArray(predicates));*/

		Predicate[] predicatesArr = new Predicate[predicateList.size()];
		List<Order> sortOrderList = new ArrayList<>();
		sortOrderList.add(criteriaBuilder.asc(root.get("lastUpdatedDate")));

		query.orderBy(sortOrderList);
		return query.where(predicateList.toArray(predicatesArr)).getRestriction();
	};
}

/**
 * 使用查询条件进行分页查询
 * @param hbzOrderDTO
 * @return
 */
@Override
public Page<HbzOrderDTO> getSpecialLineGoodsListByPage(HbzOrderDTO hbzOrderDTO, Pageable pageable) throws Exception {
	Specification<HbzOrder> specification = getHbzOrderSpecification(hbzOrderDTO);

	Page<HbzOrderDTO> hbzOrderPage = hbzOrderRepository.findAll(specification, pageable).map(hbzOrder -> {
		HbzOrderDTO hbzOrderDTO1 = new HbzOrderDTO();
		BeanUtils.copyProperties(hbzOrder,hbzOrderDTO1);
		return hbzOrderDTO1;
	});

	return hbzOrderPage;
}
/********************************************/
本地缓存：guava cache。

HashMap知识点，了解get put rehash方法。

任何缓存最头疼、最难实现的两个方向为rehash方式、淘汰算法。

1.hashmap的rehash方式，当hashmap的key数量到达一定程度（阈值）后，整个hashMap对象需要扩容，
rehash就是泛指这个扩容过程，这个过程很费时间和资源。
这是因为：扩容后绝大多数key的指针都要被重新映射到新地址，rehash方法不够优化，此时需要消耗大量计算资源。
	原来有n个槽位时，某key -> hashCode -> x槽位
	扩容为m个槽位时，该key -> hashCode -> y槽位
	如果能预见hashMap的size，应该在初始化时指定其size大小。

2.hashMap没有实现淘汰策略，容量不够用rehash就会自动扩容
	guava cache可以设定容量大小，当达到阈值就会调用淘汰算法来淘汰掉一部分数据，淘汰策略
	传统的LIFO、FIFO侧率都比较笨拙，
	guava默认使用LRU，使用双向链表，某元素如果最近被使用，那么以后也会有大概率被使用
	如果很久没被使用，就可推测他在后来也不会被使用
	
3.caffeine cache优化了LRU

分布式缓存
1.memcached只有key-value，value只能是string，没有数据结构，但天生支持集群。
2.redis除了key-value，其中value除了支持string数据结构外，还有key-list、key-set、key-map、key-sortset(调表)等数据结构，后来才支持集群。
3.两者都有容灾机制，引入了一致性hash算法来避免rehash操作时调整原有key位置的缺点，将所有的节点当成一个环，当有节点进出时，只需要将断开处的节点上的数据放到下一个几点上，减少了元素的移动
4.都采用LRU淘汰机制。
5.Redis速度极快，当数据量未超过机器可用内存的一半时，单机完全能胜任。
	纯内存模式：数据只保存在内存。
	备份模式：定期将数据用RDB或AOF机制持久化到硬盘，重启时会重新加载到内存。
		RDB：定期全量拷贝到内存。
		AOF：第一次全量拷贝，之后会记录每个操作，重启时重放操作记录来恢复数据。
		

服务降级策略：当短期有大量失败请求到缓存，此时需要屏蔽掉请求，保证数据库不被冲击。
也可以将数据库分散，保持各服务数据库的独立。
切忌过度优化，出现问题再进行优化。
google-leveldb速度很快，就是基于跳表实现的。


Redis服务端是单线程的，足够快，多线程复杂度过高。
6379：


对象最好以json格式的String存储在redis，因为java原生Servrializable，必须保证每个对象序列化ID一致，
不然不能反序列化，特别是跨应用读取，所以尽量不用，另一方面也不够高效。
json不是最快的，也不是最好的，但可读性好，跨平台性好。

redis内存有扫描线程，会定时删除过期的key。
如果扫描漏掉了，当去取数据的时候发现了，也会删掉它。

强烈建议设置过期时间，且尽量不要设置的太长，应该用访问延期和过期重存的机制。
/********************************************/
通过枚举类名获取包下所有枚举类
@RequestMapping(value = "/api/public/enums", method = RequestMethod.POST)
public ResponseDTO queryEnum(@RequestBody EnumQueryDTO enumQuery) {
	Map<String, Object> kv = new LinkedHashMap<>();
	try {
		Class<?> enumClass = Class.forName("com.troy.keeper.hbz.type." + enumQuery.getEnumname());
		if (!enumClass.isEnum()) {
			return new ResponseDTO(Const.STATUS_ERROR, "失败");
		} else {
			Method valuesMethod = enumClass.getMethod("values");
			Object[] values = (Object[]) valuesMethod.invoke(null);
			for (Object value : values) {
				String key = value.toString();
				Method getNameMethod = enumClass.getMethod("getName");
				String name = (String) getNameMethod.invoke(value);
				kv.put(key, name);
			}
		}
	} catch (Exception e) {
		return new ResponseDTO(Const.STATUS_ERROR, "失败");
	}
	return new ResponseDTO(Const.STATUS_OK, "成功", kv);
}
/********************************************/
中文转unicode
public static String transChineseToUnicode(String chineseString) {
	StringBuilder stringBuilder;
	final String UNICODE_PREFIX = "\\u";

	if (StringUtils.isBlank(chineseString)) {
		return null;
	} else {
		stringBuilder = new StringBuilder();
		char[] charArr = chineseString.toCharArray();
		for (char it : charArr) {
			int intChar = (int) it;
			if (intChar >= 19968 && intChar <= 171941) {
				stringBuilder.append(UNICODE_PREFIX).append(Integer.toHexString(intChar));
			} else {
				stringBuilder.append(it);
			}
		}
	}

	return stringBuilder.toString();
}
/********************************************/
public static void main(String[] args) {
	Random random = new Random();  //最普通，默认以主机时间为seed
	System.out.println(random.nextInt(10));

	SecureRandom secureRandom = new SecureRandom();  //更安全，默认以主机时间、cpu温度等瞬时参数为seed
	System.out.println(secureRandom.nextInt(10));

	System.out.println(ThreadLocalRandom.current().nextInt(10));  //更高效，多线程换进最好用这个
}
/********************************************/
枚举类对象的相等性比较可以用【==】，也可以用【equals】。
/********************************************/
消息队列的目的
1.解耦：解决多个系统间，通过接口强耦合的问题，不需要实时返回的数据都可以用MQ来同步。
2.冗余：利用订阅发布机制向多个系统，多个系统同时消费。
3.高扩展性/灵活性：消息队列可以分布式集群。
4.缓冲数据压力，异步处理消息。

activeMq：java实现，企业应用较多，轻量级。
rabbitMq：erlang实现，主要应用于业务逻辑处理，消息可靠性有保障，效率有所欠缺
kafka：是为了日志收集开发的，效率较高

选型优先级：kafka > rabbitMq > activeMq。

投递策略：
1.消息至少被投递一次，可能收到重复消息。
2.消息至多被投递一次，不会重复，但是会漏掉消息。
3.消息只会被投递一次，既不会重复，也不会丢失。
选择优先级：3 > 1 > 2

kafka-1.0.0：
1.支持1、3投递策略
2.支持事务
3.增加了header，附加信息，报头和内容分离。
4.增加了ssl协议，保证安全，消费者需要密钥。
不只是针对log，还可以处理事务和数据逻辑。

1.时间复杂度为0(1)的方式提供消息持久化能力，对于TB级数据，也是瞬间完成。
2.高吞吐量，单机支持1000*1000条消息，1000字节的消息。
3.支持消息分区，分布式消息。
4.保证消息的顺序行。
5.保证消息的可到达行。
6.支持水平扩展，来提高性能。

RabbitMQ比较重量，也实现了Broker(代理)架构
消息首先发布到中心队列排队，并进行持久化，然后再发给消费者。

ActiveMQ有两种模式，代理人和点对点模式(生产者和消费者直接连接，没有中转，存在隐患)

zeroMQ速度快，但是不支持消息持久化，没有很好的客户端，client需要自己实现。

redis也可以作为消息队列，轻量级，对大消息处理能力不够，不建议作为MQ使用。
/********************************************/
Broker：kafka集群包含一个或多个Broker服务器
Topic：每条发到集群的消息都有一个主题，不同主题的消息是分开存储的，会分散到多个Broker，但对用户是透明的。
	在逻辑上可以看作是一个Quee，
Partition：为了性能，每个Topic有多个分区，物理上的概念，相当于一个文件目录，每个主题都已一个或多个partition。
Producer：
Consumer：
ConsumerGroup：每个Consumer属于一个特定的CG，每个

producer向broker推消息，consumer从broker拉消息。
推模式：难适应消费速度不同的消费者，一般只管推的效率。
拉模式：简化了broker的设计，由于是消费者主动拉取消息，所以速度由消费者来控制。
	及消费方式，可以多条拉取也可以单条拉取。
/********************************************/
Spring-Boot已经通过Spring-kafka项目集成了kafka-client，方便使用

/********************************************/
@Mapper(componentModel = "spring")  //给注解添加了componentModel属性后，Mapper的实现类会被加上@Component注解，被spring容器管理
public interface AppleMapper {}
/********************************************/
1.开发人员的本地库之间不同步。
2.开发和测试、生产环境数据库接口不同步。

用数据库同步工具FlyWay或者liquibase保持数据库的同步，可以实现本地开发库与别的库结构的实时关联。
1.在pom里面添加FlyWay依赖。
2.在yaml文件中配置flyaway数据库连接。
3.在resource目录下建立db.migration目录，然后在里面放上sql文件。
相当于是通过版本管理工具来管理数据库的sql文件，达到数据库的统一性。
/********************************************/
actuator是spring-boot提供的监控工具，用于监控、审计、度量、粗粒度服务控制
1.监控：
/********************************************/
多数据源场景：
1.需要访问异构数据库。
2.同构数据库，但是有多个数据库实例。
3.为了分散压力提高性能，做了读写分离，主库只写，从库只读。
/********************************************/
/********************************************/
Role.EnterpriseAdmin == Role.valueOf("EnterpriseAdmin");
/********************************************/
Linux中非root用户，不能使用小于1024的端口。
/********************************************/
代码统计工具cloc
EnumSet和EnumMap用于封装Enum型集合，效率比Hash型集合高得多。
/********************************************/
/**
 * java日期正则表达式
 */
public static final String REGEX_YYYY_MM_DD_HH_MM = "^((((1[6-9]|[2-9]\\d)\\d{2})-(0?[13578]|1[02])-(0?[1-9]|[12]\\d|3[01]))|(((1[6-9]|[2-9]\\d)\\d{2})-(0?[13456789]|1[012])-(0?[1-9]|[12]\\d|30))|(((1[6-9]|[2-9]\\d)\\d{2})-0?2-(0?[1-9]|1\\d|2[0-8]))|(((1[6-9]|[2-9]\\d)(0[48]|[2468][048]|[13579][26])|((16|[2468][048]|[3579][26])00))-0?2-29-)) (20|21|22|23|[0-1]?\\d):[0-5]?\\d$";
public static final String REGEX_YYYY_MM_DD_HH_MM_SS = "^((((1[6-9]|[2-9]\\d)\\d{2})-(0?[13578]|1[02])-(0?[1-9]|[12]\\d|3[01]))|(((1[6-9]|[2-9]\\d)\\d{2})-(0?[13456789]|1[012])-(0?[1-9]|[12]\\d|30))|(((1[6-9]|[2-9]\\d)\\d{2})-0?2-(0?[1-9]|1\\d|2[0-8]))|(((1[6-9]|[2-9]\\d)(0[48]|[2468][048]|[13579][26])|((16|[2468][048]|[3579][26])00))-0?2-29-)) (20|21|22|23|[0-1]?\\d):[0-5]?\\d:[0-5]?\\d$";

public static void main(String[] args) {
	String date = "2017-02-10 23:7";
	System.out.println(Pattern.matches(Const.REGEX_YYYY_MM_DD_HH_MM, date));
}
/********************************************/
调用List的toArray方法时，尽量使用有参的实现，传递事先声明好的目标数组对象，
因为无参方法返回的数组是Object，后续操作需要强转
public static void main(String[] args) {
	List<String> stringList = new ArrayList<>();
	stringList.add("12");
	stringList.add("23");

	String[] stringArr = new String[stringList.size()];
	stringList.toArray(stringArr);
	for(String it : stringArr) {  //不需强转
		System.out.println(it.concat("+++++"));
	}
}
/********************************************/
1.创建自定义线程池

//推荐这种方式，因为所有细节都都可控
private static final ThreadPoolExecutor threadPool = new ThreadPoolExecutor(
            6,
            8,
            60,
            TimeUnit.SECONDS,
            new LinkedBlockingDeque<>(16),
            Executors.defaultThreadFactory(),
            new ThreadPoolExecutor.DiscardOldestPolicy()
);

//这种方式虽然简单，但是不推荐，因为屏蔽了某些可能诱发问题细节
static final ExecutorService fixedPool = Executors.newFixedThreadPool(6);

2.创建Callable实现类
package com.example.demo.caller;
import com.example.demo.util.gifUtil.AnimatedGifEncoder;
import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import javax.imageio.ImageIO;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.util.concurrent.Callable;

/**
 * @Author：YangJx
 * @Description：Gif图片转换线程类
 * @DateTime：2017/12/18 10:36
 */
@Slf4j
@Data
@NoArgsConstructor
@AllArgsConstructor
public class GifCaller implements Callable<String> {

    /**
     * 原图路径
     */
    private String sourceFramePath;

    @Override
    public String call() throws Exception {
        return transImageToGif(this.sourceFramePath);
    }

    /**
     * 根据图片在OS上的存储路径，将图片转为gif
     *
     * @param sourcePath
     * @return
     */
    public String transImageToGif(String sourcePath) {
        AnimatedGifEncoder animatedGifEncoder = new AnimatedGifEncoder();
        animatedGifEncoder.setDelay(1);
        animatedGifEncoder.setRepeat(-1);
        animatedGifEncoder.setQuality(50);

        int lastIndexOfDot = sourcePath.lastIndexOf('.');
        String targetGifPath = sourcePath.substring(0, lastIndexOfDot) + ".gif";
        animatedGifEncoder.start(targetGifPath);
        InputStream inputStream_1 = null;
        InputStream inputStream_2 = null;
        try {
            inputStream_1 = new FileInputStream(sourcePath);
            inputStream_2 = new FileInputStream(sourcePath);
            animatedGifEncoder.addFrame(ImageIO.read(inputStream_1));
            animatedGifEncoder.addFrame(ImageIO.read(inputStream_2));
            animatedGifEncoder.finish();
        } catch (FileNotFoundException e) {
            log.error(e.getMessage(), e);
        } catch (IOException e) {
            log.error(e.getMessage(), e);
        } finally {
            if (inputStream_1 != null) {
                try {
                    inputStream_1.close();
                } catch (IOException e) {
                    log.error(e.getMessage(), e);
                }
            }
            if (inputStream_2 != null) {
                try {
                    inputStream_2.close();
                } catch (IOException e) {
                    log.error(e.getMessage(), e);
                }
            }
        }
        return targetGifPath;
    }
}
3.将创建线程并提交到线程池执行，并获取结果
public static List<String> transImageToGigWithMultThread(List<String> framePathList) {
	List<String> targetPathList = new ArrayList<>(framePathList.size());
	for (String path : framePathList) {
		FutureTask<String> futureTask = null;
		try {
			futureTask = new FutureTask<String>(new GifCaller(path));
			threadPool.submit(futureTask);
			targetPathList.add(futureTask.get());
		} catch (Exception e) {
			log.error(e.getMessage(), e);
		}
	}

	List<String> tempList = new ArrayList<>(targetPathList.size());
	tempList.addAll(targetPathList);
	while (tempList.size() != 0) {
		Iterator<String> it = tempList.iterator();
		if (it.hasNext() && ifFileExist(it.next())) {
			it.remove();
		}
	}
	return targetPathList;
}
/********************************************/
指定Spring-boot项目的启动配置文件
java -jar -Dspring.profiles.active=dev demo.jar
/********************************************/
1.查看java进程对系统资源的概览
	top
2.查看java进程的GC情况
	jstat -gcutil 24814
3.查看指定java进程中，各线程资源使用情况
	top -p 19614 -H
4.将各线程的状态打印到文本以便分析
	jstack -l 19614 > ./kkk.txt
/********************************************/
1.创建线程池
private static final ThreadPoolExecutor threadPool = new ThreadPoolExecutor(
		6,
		8,
		60,
		TimeUnit.SECONDS,
		new LinkedBlockingDeque<>(16),
		Executors.defaultThreadFactory(),
		new ThreadPoolExecutor.DiscardOldestPolicy()
);

2.创建返回值list
List<String> destPathList = new ArrayList<>();

3.创建futureTaskList
List<FutureTask<String>> futureTaskList = new ArrayList<>();

4.匿名对象方式 创建futureTask【runnable的实例化也类似】
/*FutureTask<String> futureTask = new FutureTask<>(new Callable<String>() {
	@Override
	public String call() throws Exception {
		SfntTool.main(paramArr);
		return paramArr[3];
	}
});*/

4.lambda表达式方式 创建futureTask
Callable<String> caller = () -> {
	SfntTool.main(paramArr);
	return paramArr[3];
};
FutureTask<String> futureTask = new FutureTask<>(caller);

5.将futureTask提交到线程池
threadPool.submit(futureTask);

6.将返回结果放到返回值列表中
futureTaskList.add(futureTask);
/********************************************/
public static void main(String[] args) {
	User user = new User();
	new Thread(() -> user.man()).start();  //lambda式初始化一个runnable实例
	new Thread(() -> user.women()).start();
}
/********************************************/
我们可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池，但是它们的实现原理不同，shutdown的原理是只是将线程池的状态设置成SHUTDOWN状态，
然后中断所有没有正在执行任务的线程。shutdownNow的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。
shutdownNow会首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表。
只要调用了这两个关闭方法的其中一个，isShutdown方法就会返回true。当所有的任务都已关闭后,才表示线程池关闭成功，这时调用isTerminaed方法会返回true。
至于我们应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用shutdown来关闭线程池，如果任务不一定要执行完，则可以调用shutdownNow。
/********************************************/
CPU密集型任务配置尽可能少的线程数量，如配置Ncpu+1个线程的线程池。IO密集型任务则由于需要等待IO操作，线程并不是一直在执行任务，
则配置尽可能多的线程，如2*Ncpu。混合型的任务，如果可以拆分，则将其拆分成一个CPU密集型任务和一个IO密集型任务，只要这两个任务执行的时间相差不是太大，
那么分解后执行的吞吐率要高于串行执行的吞吐率，如果这两个任务执行时间相差太大，则没必要进行分解。我们可以通过Runtime.getRuntime().availableProcessors()
方法获得当前设备的CPU个数。
/********************************************/
正确的结束线程池中的线程的顺序
shutdown  //只是改变了池的SHUTDOWN属性的状态，然后中断所有没有正在执行任务的线程
awaitTermination
shutdownNow //原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止，
				shutdownNow会首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表

try {
	// 向学生传达“问题解答完毕后请举手示意！”
	pool.shutdown();

	// 向学生传达“XX分之内解答不完的问题全部带回去作为课后作业！”后老师等待学生答题
	// (所有的任务都结束的时候，返回TRUE)
	if(!pool.awaitTermination(awaitTime, TimeUnit.MILLISECONDS)){
		// 超时的时候向线程池中所有的线程发出中断(interrupted)。
		pool.shutdownNow();
	}
} catch (InterruptedException e) {  
	// awaitTermination方法被中断的时候也中止线程池中全部的线程的执行。
	System.out.println("awaitTermination interrupted: " + e);
	pool.shutdownNow();
}
/********************************************/
1.在spring-mvc中使用java原生参数验证
2.WarehouseDTO类中的各个字段需要配上验证注解
@PostMapping("/createWarehouse")
public ResponseDTO createWarehouse(@RequestBody @Valid WarehouseDTO warehouseDTO, BindingResult validResult) {
	List<FieldError> fieldErrorList = validResult.getFieldErrors();
	if (fieldErrorList.size() > 0) {
		return new ResponseDTO(Const.STATUS_ERROR, "输入错误！", fieldErrorList.stream().map(FieldError::getDefaultMessage).collect(Collectors.toList()));
	}
}
/********************************************/
List< Order> orders=new ArrayList< Order>();
orders.add(new Order(Direction.ASC, "c"));
orders.add(new Order(Direction.DESC, "d"));
Pageable pageable= new PageRequest(pageNumber, pageSize, new Sort(orders));
jpaRepo.findByAAndB(a,b,pageable);
/********************************************/
过程式
@Override
public Page<WarehouseVO> getMyWarehouseListByPage(@RequestBody WarehouseDTO warehouseDTO, Pageable pageable) {
	//构造查询条件
	Specification<Warehouse> warehouseSpecification = new Specification<Warehouse>() {
		@Override
		public Predicate toPredicate(Root<Warehouse> root, CriteriaQuery<?> query, CriteriaBuilder criteriaBuilder) {
			List<Predicate> predicateList = new ArrayList<>();
			predicateList.add(criteriaBuilder.equal(root.get("id"), warehouseDTO.getId()));
			predicateList.add(criteriaBuilder.equal(root.get("createdBy"), warehouseDTO.getCreatedBy()));
			Predicate[] predicateArr = new Predicate[predicateList.size()];
			predicateList.toArray(predicateArr);
			return criteriaBuilder.and(predicateArr);
		}
	};

	//执行分页查询
	Page<Warehouse> warehousePage = warehouseRepository.findAll(warehouseSpecification, pageable);

	//将查询记录封装为VO
	Page<WarehouseVO> warehouseVOPage = warehousePage.map(new Converter<Warehouse, WarehouseVO>() {
		@Override
		public WarehouseVO convert(Warehouse warehouse) {
			WarehouseVO warehouseVO = new WarehouseVO();
			BeanUtils.copyProperties(warehouse, warehouseVO);
			return warehouseVO;
		}
	});
	return warehouseVOPage;
}

lambda式
@Override
public Page<WarehouseVO> getMyWarehouseListByPage(@RequestBody WarehouseDTO warehouseDTO, Pageable pageable) {
	//构造查询条件
	Specification<Warehouse> warehouseSpecification = (root, criteriaQuery, criteriaBuilder) -> {
		List<Predicate> predicateList = new ArrayList<>();
		predicateList.add(criteriaBuilder.equal(root.get("id"), warehouseDTO.getId()));
		predicateList.add(criteriaBuilder.equal(root.get("createdBy"), warehouseDTO.getCreatedBy()));
		Predicate[] predicateArr = new Predicate[predicateList.size()];
		predicateList.toArray(predicateArr);
		return criteriaBuilder.and(predicateArr);
	};

	//执行分页查询
	Page<Warehouse> warehousePage = warehouseRepository.findAll(warehouseSpecification, pageable);

	//将查询记录封装为VO
	Page<WarehouseVO> warehouseVOPage = warehousePage.map((warehouse) -> {
		WarehouseVO warehouseVO = new WarehouseVO();
		BeanUtils.copyProperties(warehouse, warehouseVO);
		return warehouseVO;
	});
	return warehouseVOPage;
}
/********************************************/
在Windows系统中，多条dos命令可以保存为批处理文件中，方便阅读、修改和备份；
批处理文件分为两种，他们的命令都是dos命令，只是cmd支持的命令更多而不支持旧版系统，bat支持的命令少一些但是适用于所有版本的系统。
.bat：ms dos批处理文件
.cmd；windows nt命令脚本
/********************************************/
Windows系统开启远程监控java程序接口
java ^
-Djava.rmi.server.hostname=192.168.22.113 ^  //必须配置为服务器ip地址
-Dcom.sun.management.jmxremote ^
-Dcom.sun.management.jmxremote.port=8421 ^
-Dcom.sun.management.jmxremote.authenticate=false ^
-Dcom.sun.management.jmxremote.ssl=false ^
-jar ./server-k171220am9.jar
/********************************************/
Linux系统开启远程监控java程序接口
【shell脚本文件的创建不能用Windows之txt文件改后缀的方式，必须在Linux系统下新建.sh文件】
nohup java \
-Djava.rmi.server.hostname=119.23.39.35 \  //必须配置为服务器ip地址
-Dcom.sun.management.jmxremote \
-Dcom.sun.management.jmxremote.port=8421 \
-Dcom.sun.management.jmxremote.authenticate=false \
-Dcom.sun.management.jmxremote.ssl=false \
-jar ./server-k171220am9.jar &

【其中nohup和&操作符可以放在shell内，也可以在启动shell脚本的命令中：(nohup sh startup.sh &)这种方式】
/********************************************/
远程Idea中如何远程debug Spring-Boot项目【经过测试可用】
【Idea配置】
1.点击Run命令，选中Edit Configurations
2.点击绿色加号，在列表中选择Remote
3.在右侧Configuration选项卡中For JDK1.4.x栏
	-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005  //表示监听5005作为调试端口
	下面填写上服务器的主机地址和与上面一致的端口号
这样一个RomoteDebug实例就建好了，在服务器在debug模式下启动，就可以在Idea中打开这个实例，打上断点愉快的调试。

【远程服务启动参数】
1.常规启动命令：
	java -jar test-tool.jar
2.开启远程监听，需要添加额外启动参数
	java -jar -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005 test-tool.jar
3.查看监听端口状态
	netstat -anp | grep 5005
/********************************************/
使用MapStruct组件做对象转换时，实体必须具有getter和setter方法
使用Lombok组件生成的getter和setter方法由于编译顺序的原因，会引发问题，所以要手动写出。
/********************************************/
返回的是一个百分比，用以测试某个字段在表中的区分度
SELECT
	COUNT(DISTINCT(t.amount)) / COUNT(*)
FROM
	hbz_order t;
/********************************************/
1.定义函数
private static Function<String, List<String>> my = platArr -> {
	int len = platArr.length();
	return Arrays.asList(platArr.substring(1, len - 1).split(","));
};

2.在方法中调用函数
private static List<String> call(String platArr, Function<String, List<String>> fun) {
	return fun.apply(platArr);
}

3.次级调用
public static void main(String[] args) {
	call("['11', '22', '33']", my).forEach(System.out::print);  //方法式调用
	my.apply("['11', '22', '33']").forEach(System.out::print);  //直接调用
}
/********************************************/
使用Spring的@Value注解读取配置文件的值时，不能用static去修饰变量
@Value("${staticImagePrefix}")
private String staticImagePrefix;
/********************************************/
.sorted(Comparator.comparing(it -> it.getAnnotation(ExcelCell.class).sortNo()))
/********************************************/
Java8之Interface特性
1.可以定义多个静态成员变量，成为常量，自动编译为 public static final修饰，三个关键字都可以省略不写，【只能】通过接口名直接访问（类似于普通类的静态成员变量）
2.可以实现多个静态成员方法，【只能】通过接口名直接访问（类似于普通类的静态成员方法）
3.可以实现多个默认方法，【只能】通过实现类访问（类似于普通类的继承关系，可以覆盖）
4.添加了@FunctionalInterface注解后，只能定义一个未实现方法，这个方法就是函数式方法
5.如果接口只有一个未实现方法，即使不添加@FunctionalInterface注解，这个接口也会自动编译为函数式接口，方法变为函数式方法
5.如果接口定义了多个未实现方法，这个接口就不再是函数式接口，所有方法都是普通方法
/********************************************/
1.HotSpot支持解释执行和编译执行，有三种工作状态【mixed mode, interpreted mode, compiled mode】
	用java -version命令可以查看当前JVM的运行模式，默认是混合模式
	java -Xcomp -version  //本次开启的虚拟机为编译模式
	java -Xint -version  //本次开启的虚拟机为解释模式
	java -Xmixed -version //本次开启的虚拟机为混合模式
	java -Xcomp jar app.jar  //应用会在编译模式下运行
2.HotSpot支持Client、Server两种运行模式，可以通过java -version命令查看机器上的JVM运行模式
	默认情况下JVM会根据系统资源，自动判断选择适合的运行模式，比如在64位系统下更倾向于server模式。
	可以通过【-client、-server】参数指定，java -version查看
	client模式：虚拟机启动阶段收集的系统信息较少，所以启动速度快，进入JIT编译需要达到的调用阈值更低，堆空间分配更少，适合开发测试环境。
	server模式：由于启动阶段收集的系统信息较多，所以启动稍慢，但是稳定后，优化更全面，所以运行速度更快，同时JIT阈值更高，堆空间也更大，适合生产环境。
	java -XX:+PrintFlagsFinal -client -version > D:\a.txt
3.发生OOM后的排查方法(Windows下)
	*先准备栈打印脚本D:/tools/jdk1.7_40/bin/jstack -F %1 > D:/a.txt
	*然后在启动时附带执行脚本
	-Xmx20m -Xms5m  “-XX:OnOutOfMemoryError=D:/tools/jdk1.7_40/bin/printstack.bat %p”  -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=d:/a.dump
4.由于显示代码调用System.gc()会直接触发Full GC，同时对老年代和新生代进行回收，一般情况下我们认为，垃圾回收应该是自动进行的，
	无需手工触发。如果过于频繁地触发垃圾回收对系统性能是没有好处的。因此虚拟机提供了一个参数DisableExplicitGC控制手工触发GC。
	手动触发FullGC的代码：Runtime.getRuntime().gc();
	-XX:-+DisableExplicitGC  //这个启动参数就禁用了手动出发GC的功能
/********************************************/
相同环境下，不同执行方式，循环十万次比较(执行过程中可以通过工具查看对应JVM实例的执行方式)
执行方式		命令			耗时(sec)
解释执行	java -Xint KkInt	【16，7】7，7
编译执行	java -Xcomp KkComp   【9，5】8，5
混合执行	java -Xmixed KkMixed	【5，5】6，5
默认执行	java KkDefault	【5，5】5，5
--------------------------------------------------
测试代码
public class KkInt {
    public static void looper() {
		long start = System.currentTimeMillis();
        IntStream.range(1, 100000)
                .map(it -> it + 1)
                .forEach(System.out::println);
		long end = System.currentTimeMillis();
        System.err.println((end - start) / 1000L + "++++++++++");
    }

    public static void main(String[] args) {
		try {
            Thread.sleep(4000L);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("开始------");
        looper();
		
		try {
            Thread.sleep(4000L);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
		looper();
        System.out.println("22222开始------");
    }
}
/********************************************/
Object类的toString方法
public String toString() {
	return getClass().getName() + "@" + Integer.toHexString(hashCode());  //类名 + @ + hashCode的十六进制
}
/********************************************/
String类的hashCode方法实现
public int hashCode() {
	int h = hash;  //hash为私有成员变量
	if (h == 0 && value.length > 0) {
		char val[] = value;

		for (int i = 0; i < value.length; i++) {  //遍历String对象的CharArray数组
			h = 31 * h + val[i];  //当前段的hash值，等于前一段的hash值 * 31 + 当前字符的ASCALL值
		}
		hash = h;
	}
	return h;
}
/********************************************/
public class JvmRuntime {
	public static void getRuntimeMetrics() {
        System.out.println("当前JVM参数如下：");
        System.out.println("CPU数：" + Runtime.getRuntime().availableProcessors());
        System.out.println("最大内存：" + Runtime.getRuntime().maxMemory() / 1024 / 1024);
        System.out.println("空闲内存：" + Runtime.getRuntime().freeMemory() / 1024 / 1024);
        System.out.println("总内存：" + Runtime.getRuntime().totalMemory() / 1024 / 1024);
    }
    public static void main(String[] args) {
        getRuntimeMetrics();
    }
}
/********************************************/
Spring之BeanUtils
属性拷贝copyProperties
0.参与拷贝的目标和源对象必须要有getter和setter方法，lombok注解有效。
1.同名属性，如果类型相同将直接拷贝，如果类型不同且是简单类型，将会简单转换，如果是复杂类型将抛出异常。
2.执行拷贝后属性会被覆盖，即使源属性为null，所以要防止不安全的覆盖，就要通过添加ignoreArr来忽略指定字段。
/********************************************/
不能多次调用同一个Tread对象的start方法，
因为第一次调用后线程进入可执行状态后续还会进入其他状态，
如果此时再次调用start方法就会迫使线程改变状态导致状态混乱，
会报线程状态错误的异常。
/********************************************/
Windows也使用的java线程查看器
1.用jps命令列出正在运行的java进程
2.根据进程查看进程下的所有线程状况 jstack pId
（1）"Thread-1"表示线程名称
（2）"prio=6"表示线程优先级
（3）"tid=00000000497cec00"表示线程Id
（4）"nid=0x219c"表示本地线程id
/********************************************/
List初始化方法
public static void main(String[] args) {
	List<String> name1 = Arrays.asList("xxx","yyy","zzz");  //返回的是Arrays的内部list类，不能完全支持List集合的方法
	name1.forEach(System.out::println);

	List<String> name2 =new ArrayList<>(Arrays.asList("xxx","yyy","zzz"));  //返回的是一个完整ArrayList对象，支持全部List的方法
	name2.add("99999999");
	name2.forEach(System.out::println);
}
/********************************************/
等待唤醒机制
1.只能调用加锁对象之wait和notify或notifyAll方法
2.等待唤醒的是持有锁对象的线程
3.以上方法必须在同步块中调用
/********************************************/
在IntelliJ IDEA 15中使用Maven时，IDEA将默认的编译版本、源码版本设置为jdk5。
编译项目的时候出现警告："Warning:java: 源值1.5已过时, 将在未来所有发行版中删除"，
并且无法编译jdk1.5以上的代码。
出现这样的原因应该是Maven插件的默认配置有问题。
解决方法是在"pom.xml"里加入如下代码
<properties>
   <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>          
   <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
   <java.version>1.8</java.version>
   <maven.compiler.source>1.8</maven.compiler.source>
   <maven.compiler.target>1.8</maven.compiler.target>
   <encoding>UTF-8</encoding>
</properties>
/********************************************/
Linux系统日志查看
1./var/log/message  //最常用日志
1./var/log/secure  //安全日志
1./var/log/message  //
重则整个系统假死(终端没有反应，SSH可以建立连接，但是登录不上，系统可以ping通,这个问题是由于 Linux 系统的 OOM killer 所致：下面来看下介绍：
什么是Overcommit和OOM？ 
overcommit的策略 
当oom-killer发生时，linux会选择杀死哪些进程

在Unix中，当一个用户进程使用malloc()函数申请内存时，假如返回值是NULL，则这个进程知道当前没有可用内存空间，就会做相应的处理工作。许多进程会打印错误信息并退出。
Linux使用另外一种处理方式，它对大部分申请内存的请求都回复"yes"，以便能跑更多更大的程序。因为申请内存后，并不会马上使用内存。这种技术叫做Overcommit。
当内存不足时，会发生OOM killer(OOM=out-of-memory)。它会选择杀死一些进程(用户态进程，不是内核线程)，以便释放内存。
Overcommit的策略
Linux下overcommit有三种策略(Documentation/vm/overcommit-accounting)：
0. 启发式策略。合理的overcommit会被接受，不合理的overcommit会被拒绝。
1. 任何overcommit都会被接受。
2. 当系统分配的内存超过swap+N%*物理RAM(N%由vm.overcommit_ratio决定)时，会拒绝commit。
overcommit的策略通过vm.overcommit_memory设置。
overcommit的百分比由vm.overcommit_ratio设置。

# echo 2 > /proc/sys/vm/overcommit_memory
# echo 80 > /proc/sys/vm/overcommit_ratio

当oom-killer发生时，linux会选择杀死哪些进程
选择进程的函数是oom_badness函数(在mm/oom_kill.c中)，该函数会计算每个进程的点数(0~1000)。
点数越高，这个进程越有可能被杀死。每个进程的点数跟oom_score_adj有关，而且oom_score_adj可以被设置(-1000最低，1000最高)。
/********************************************/
Windows更新文件目录
C:\Windows\SoftwareDistribution\Download
/********************************************/
匿名对象方式
@Override
public Page<UserInformationDTO> findByCondition(UserInformationDTO userInformationDTO, Pageable pageable) {
	Page<UserInformation> page = userInformationRepository.findAll(new Specification<UserInformation>() {
		@Override
		public Predicate toPredicate(Root<UserInformation> root, CriteriaQuery<?> criteriaQuery, CriteriaBuilder criteriaBuilder) {
			List<Predicate> predicateList = new ArrayList<>();
			Subquery subquery = criteriaQuery.subquery(Long.class);
			Root<OutsourcingGoods> outsourcingGoodsRoot = subquery.from(OutsourcingGoods.class);
			subquery.select(outsourcingGoodsRoot.join("userInformation").get("id"));
			subquery.where(criteriaBuilder.notEqual(outsourcingGoodsRoot.join("outsourcingDetails").get("shippingStatus"), ShippingStatus.DURING_SHIPPING));

			predicateList.add(criteriaBuilder.not(root.get("id").in(subquery)));
			predicateList.add(criteriaBuilder.equal(root.get("userClassification"), "2"));
//          predicateList.add(criteriaBuilder.equal(root.get("userClassification"),"2"));
			Predicate[] ps = new Predicate[predicateList.size()];
			return criteriaBuilder.and(predicateList.toArray(ps));
		}
	}, pageable);
	return converterDto(page);
}

lambda + 匿名对象式
@Override
public Page<UserInformationDTO> findByCondition(UserInformationDTO userInformationDTO, Pageable pageable) {
	Page<UserInformation> page = userInformationRepository.findAll((root, criteriaQuery, criteriaBuilder) -> {
		List<Predicate> predicateList = new ArrayList<>();
		Subquery subquery = criteriaQuery.subquery(Long.class);
		Root<OutsourcingGoods> outsourcingGoodsRoot = subquery.from(OutsourcingGoods.class);
		subquery.select(outsourcingGoodsRoot.join("userInformation").get("id"));
		subquery.where(criteriaBuilder.notEqual(outsourcingGoodsRoot.join("outsourcingDetails").get("shippingStatus"), ShippingStatus.DURING_SHIPPING));

		predicateList.add(criteriaBuilder.not(root.get("id").in(subquery)));
		predicateList.add(criteriaBuilder.equal(root.get("userClassification"), "2"));
//          predicateList.add(criteriaBuilder.equal(root.get("userClassification"),"2"));
		Predicate[] ps = new Predicate[predicateList.size()];
		return criteriaBuilder.and(predicateList.toArray(ps));
	}, pageable);
	return converterDto(page);
}
/********************************************/
Spring的依赖注入多实现问题
1.Controller层依赖声明
	@Autowired
	private AgreementTemplateService test;  //第一个实现类的Bean
	@Autowired
	private AgreementTemplateService agreementTemplate;  //第二个实现类的Bean

2.两个不同Service实现类的声明，需要显式声明不同的Bean名称
@Service(value = "test")
@Service(value = "agreementTemplate")

3.在依赖注入处的变量名必须与申明的实现类名严格一致。
/********************************************/
public class SerializeUtil {
    public static void main(String [] args){
        Jedis jedis = new Jedis("172.16.135.2");
        String keys = "name";
        // 删数据
        //jedis.del(keys);
        // 存数据
        jedis.set(keys, "zy");
        // 取数据
        String value = jedis.get(keys);
        System.out.println(value);
        
        //存对象
        Person p=new Person();  //peson类记得实现序列化接口 Serializable
        p.setAge(20);
        p.setName("姚波");
        p.setId(1);
        jedis.set("person".getBytes(), serialize(p));
        byte[] byt=jedis.get("person".getBytes());
        Object obj=unserizlize(byt);
        if(obj instanceof Person){
            System.out.println(obj);
        }
    }
    
    //序列化 
    public static byte [] serialize(Object obj){
        ObjectOutputStream obi=null;
        ByteArrayOutputStream bai=null;
        try {
            bai=new ByteArrayOutputStream();
            obi=new ObjectOutputStream(bai);
            obi.writeObject(obj);
            byte[] byt=bai.toByteArray();
            return byt;
        } catch (IOException e) {
            e.printStackTrace();
        }
        return null;
    }
    
    //反序列化
    public static Object unserizlize(byte[] byt){
        ObjectInputStream oii=null;
        ByteArrayInputStream bis=null;
        bis=new ByteArrayInputStream(byt);
        try {
            oii=new ObjectInputStream(bis);
            Object obj=oii.readObject();
            return obj;
        } catch (Exception e) {
            e.printStackTrace();
        }
        return null;
    }
}
/********************************************/
循环语句优化技巧
1.嵌套循环尽量将次数少的放在外层，次数多的放在内层。
2.尽可能的将计算量放在循环外，循环中只做最必要、最简单的计算部分，提取出与循环无关的表达式。
3.尽可能少的在循环语句中调用函数，尽可能使循环中用到的参数“常量化”，消除循环终止条件的方法调用。
4.不要在循环体内捕获异常，而应该捕获整个循环体。
/********************************************/
1.多线程间的通信问题；

/********************************************/
1.两个及以上的线程共享同一对象。
2.所有参与的线程需要竞争同一把锁。
3.
/********************************************/
等待线程存在锁持有的阻塞队列中：
	1.notify();方法只会唤醒第一个线程。
	2.notifyAll();方法则会唤醒队列中的所有线程。

wait、notify、notifyAll方法是通过共享变量调用的，只能作用在参与竞争的线程上
其中wait是作用在当前持有对象的线程上，后两者的作用对象作用在锁的阻塞队列中的等待线程。
/********************************************/
多对多的生产-消费者模型需要解决两方面的问题：
1.多个生产者线程覆盖式生产。
2.多个消费者线程对同一个产品重复消费。
3.解决方案：用wile语句来判断共享变量的标记位来决定是否等待，同时用notifyAll();方法唤醒所有阻塞线程。
/********************************************/
notifyAll();方法会同时唤醒本方线程，并不是完美的方式。
使用Lock-Condition机制就可以更细粒度的控制，只唤醒对方。
一个锁可以对应多个Condition，每个condition都有其await、singal方法，即可灵活实现等待唤醒机制
还能有效避免死锁
/********************************************/
读写锁
对于写锁：多个写操作是互斥的，一个写操作和读操作也是互斥的；
对于读锁：多个读取操作是可以共存的。
1.悲观锁：所有操作都是抢占式的，如果某种操作次数过多可能造成另一种操作的饥饿。
2.乐观锁：
/********************************************/
1.堆内存：
2.方法区：
3.线程栈：
4.程序计数器：
5.本地方法栈
/********************************************/
1.有继承
2.有重写
3.父类引用指向子类对象
/********************************************/
私钥加密体系：加密和解密的密钥相同，加密算法公开，双方都需要保护相同的密钥，一方用密钥加密的内容，对方用同样的密钥和算法就可解密。
公钥加密体系：加密和解密的密钥不相同，加密算法公开，只需要保护私钥，公钥对外公开，一方用私钥加密的内容对方只能用公钥解密，同时一方用公钥加密的内容只能用对方的私钥解密。
/********************************************/
在通信过程中：公钥加密机制用于通信方身份的确认和后续通信用的对称密钥及算法的商定和传输。
/********************************************/
由于任何人都可以生成非对称密钥对，这就使得以非对称密钥确定身份的方式不可靠。
数字证书解决的是非对称加密体系中公钥的可靠性问题。
公钥本身是通过证书来传递的。
/********************************************/
我们"ABC Company"作为服务器，从“SecureTrust CA”申请到这个证书后，把证书投入使用，我们在通信过程开始时会把证书发给对方，
对方如何检查这个证书的确是合法的并且是我们"ABC Company"公司的证书呢？首先应用程序(对方通信用的程序，例如IE、OUTLook等)
读取证书中的Issuer(发布机构)为"SecureTrust CA" ，然后会在操作系统中受信任的发布机构的证书中去找"SecureTrust CA"的证书，
如果找不到，那说明证书的发布机构是个水货发布机构，证书可能有问题，程序会给出一个错误信息。 如果在系统中找到了"SecureTrust CA"的证书，
那么应用程序就会从证书中取出"SecureTrust CA"的公钥，然后对我们"ABC Company"公司的证书里面的指纹和指纹算法用这个公钥进行解密，
然后使用这个指纹算法计算"ABC Company"证书的指纹，将这个计算的指纹与放在证书中的指纹对比，如果一致，说明"ABC Company"的证书肯定没有被修改过并
且证书是"SecureTrust CA" 发布的，证书中的公钥肯定是"ABC Company"的。对方然后就可以放心的使用这个公钥和我们"ABC Company"进行通信了。
/********************************************/
验证http2：使用Chrome的网络工具，在地址栏中输入如下地址：
	chrome://net-internals/#http2
可以看到你的服务端口也在上面，有一列为protocolNegotiated的数值为h2的则是http2了
/********************************************/
1）FixedThreadPool 和 SingleThreadPool:   允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。
2）CachedThreadPool 和 ScheduledThreadPool:   允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。
/********************************************/
使用ThreadPoolExecutor线程池，
1.线程数低于核心容量时，会直接创建新线程。
2.当核心容量被用完，会将请求放入阻塞队列。
3.当阻塞队列容量用完，会新建线程，直到最大容量备用完。
/********************************************/
1.ThreadLocal类似于一个集合，接受泛型参数，
在多线程环境下如果要避免共享对象的某个属性因为被共用而产生的线程安全问题，同时这个集合可以声明为static
将这个属性定义为ThreadLocal是最合适的。
2.其原理可以这样理解，这个属性被定义为一个线程安全的Map，其键是当前进来的线程，
每个线程访问这个变量时都是以当前线程对象本身为键对值进行读写，
这样就保证了属于其他线程的值不会被访问到。
3.一个典型的应用就是为每一个请求都建立一个数据库连接，
每个请求的数据库交互都在一个数据库连接完成，
这其中就包括连接的建立和关闭
由于客户端的每一次请求都是一个线程，
所以为了保证每次请求中的数据库操作是互补干扰的就需要线程级别的隔离
/********************************************/
Thread.currentThread.getContextClassLoader();  //获取当前线程的ClassLoader
Class.forName("com.youn.have.classLoad.ClassLoadTest", isInit, getCurrentClassLoader()).getSimpleName());  //通过指定加载器加载类
Class.forName("com.youn.have.classLoad.ClassLoadTest").getSimpleName();  //通过默认参数加载类
/********************************************/
-- 修改表的存储引擎为InnoDB
ALTER TABLE t_macb_entity ENGINE = INNODB;

-- 计算一个十进制数的二进制值
SELECT BIN(4);

-- 2.1
-- 出于性能上的考量SQL中要少用正则表达式

-- 2.2
-- MySql内置随机函数rand()，可以返回0到1之间的随机数
SELECT RAND();
-- 通过在order by子句后面搭配随机函数rand()对查询结果集进行随机排序
SELECT * FROM t_macb_entity ORDER BY RAND();
-- 对查询结果随机排序，然后获取3条数据，可用于抽样
SELECT * FROM t_macb_entity ORDER BY RAND() LIMIT 3;

-- 3.2
-- 普通group by子句，可以实现多级分组【参与分组的字段必须包含在查询字段中】，但是展示的信息有限，
-- 比如只能展示最低级聚合结果，不能分级执行聚合函数
SELECT t.`name`, t.age, SUM(t.PK_ID) FROM t_macb_entity AS t GROUP BY t.`name`, t.age;

-- 使用group by子句同with rollup子句可以检索出更多的分组聚合信息，比如可以分级统计出各个级别的聚合结果，
-- 但是不易把控需配合聚合函数，with rollup不能与order by搭配
SELECT t.`name`, t.age, SUM(t.PK_ID) FROM t_macb_entity AS t GROUP BY t.`name`, t.age WITH ROLLUP;

-- 2.4 
-- group by语句可以与bit_and、bit_or函数搭配以完成统计工作，这两个函数主要用于数值间的逻辑位运算
SELECT t.`name`, BIT_AND(t.age) FROM t_macb_entity t GROUP BY t.`name` WITH ROLLUP;

-- 2.5
-- 使用外键需要注意的问题
-- 注意：Innodb引擎才支持外键，myisam引擎虽然能创建外键但不起作用，MySql中不鼓励使用外键

-- 2.6命令行下执行help
-- 查看帮助
?;
-- 可以查出所有可执行命令
?%;
-- 展示可以与create配合的命令
? create;
-- 展示函数相关的命令
? functions;

? string funciton;
-- 将每一条记录以个体的方式显示出来，在列比较多的情况下更便于阅读
SHOW PLUGINS \G;

-- 3、SQL语句优化
-- 3.1SQL语句优化的一般步骤
-- 3.1.1
-- 第一步：通过慢查询日志定位到慢SQL语句
-- 第二步：解析该SQL语句查看其影响行数
-- 第三步：通过影响行数来判断是否应该加索引，及索引的位置
-- SHOW [SESSION | GLOBAL] STATUS，global表示这个数据库启动以来，session表示本次链接中，默认是session级别
-- 下列语句可以查看数据库主要执行那种操作为主，这能让优化目标更清晰
SHOW GLOBAL STATUS LIKE "com_select%";
SHOW GLOBAL STATUS LIKE "com_insert%";
SHOW GLOBAL STATUS LIKE "com_update%";
SHOW GLOBAL STATUS LIKE "com_delete%";

-- 只针对InnoDB存储引擎，但是查看的都是影响行数
SHOW GLOBAL STATUS LIKE "innodb_rows_read%";
SHOW GLOBAL STATUS LIKE "innodb_rows_inserted%";
SHOW GLOBAL STATUS LIKE "innodb_rows_updated%";
SHOW GLOBAL STATUS LIKE "innodb_rows_deleted%";

-- 查询连接数，包括失败的连接
SHOW STATUS LIKE "CONNECTIONS";
-- 查看运行时间
SHOW STATUS LIKE "uptime";
-- 慢查询次数
SHOW STATUS LIKE "slow_queries";
-- 查看数据库慢查询记录功能的状态
-- SHOW STARTS LIKE "slow_queries";
SHOW VARIABLES LIKE "%slow%";
-- 查看慢查询界定时长
SHOW VARIABLES LIKE "%long%";

-- 3.1.2
-- 定位执行效率较低的SQL语句，下列两条语句等效
EXPLAIN SELECT * FROM t_macb_entity;
DESC SELECT * FROM t_macb_entity \G;
*************************** 1. row ***************************
           id: 1
  select_type: SIMPLE  //简单的单表查询
        table: t_macb_entity
         type: ALL  //全表扫描，没有用到任何索引
possible_keys: NULL  //可能的索引，是一个建议值
          key: NULL  //用到的索引
      key_len: NULL
          ref: NULL
         rows: 4  //影响行数
        Extra:
1 row in set (0.00 sec)

-- 3.2索引问题
-- 3.3两个简单实用的优化方法
-- 3.4常用SQL优化
/********************************************/
-- 3.2索引问题
-- 索引优化是数据库中最常见也是最重要的手段之一，通过索引可以解决大多数SQL性能问题。
-- 将索引加在where、having、order by、group by这些条件性子句字段上通常能得到更高的执行效率。

-- 3.2.1索引的存储分类
-- MyISAM存储引擎而言，每张表会分为三个文件来存储数据（frm结构文件、data数据文件、i结尾的索引），这样可以减少系统资源消耗
-- InoDB存储引擎而言，每张表的数据和索引是存储在同一个表空间中的ib_data文件中，但是可以分多个文件片来存储（共性表空间、独立表空间（最好分离索引和数据文件））。

-- 创建一个复合索引(一定要把影响行数降到最低)
-- 当需要用到事务和外键机制时就必须用InnoDB存储引擎
CREATE INDEX ind_id_name ;

ALTER TABLE t_macb_entity ADD INDEX ind_name(`name`);
-- 加上索引后可以解析查询语句的属性
DESC SELECT * FROM t_macb_entity t WHERE t.`name` = "dd";

-- 正则表达式的匹配查询效率比like语句低。
-- 使用like的模糊查询，后面如果是常量，且只有%符号不在第一个字符时，索引才会命中。
-- 这两条语句做对比，会发现第二条语句没有用上索引
DESC SELECT * FROM t_macb_entity t WHERE t.`name` LIKE "dd%";
DESC SELECT * FROM t_macb_entity t WHERE t.`name` LIKE "%dd%";

-- 当以IS NULL字段能命中索引，解析显示，下面三条语句都能命中索引
DESC SELECT * FROM t_macb_entity t WHERE t.`name` IS NULL;
DESC SELECT * FROM t_macb_entity t WHERE t.`name` IS NOT NULL;
DESC SELECT * FROM t_macb_entity t WHERE ISNULL(t.`name`);

-- 2、存在索引但是不使用索引
-- (1)如果MySql估计使用索引比全表扫描还慢，则不会使用索引，例如索引列均匀分布在一个区间，
EXPLAIN SELECT * FROM t_macb_entity t WHERE t.PK_ID > 1;
EXPLAIN SELECT * FROM t_macb_entity t WHERE t.PK_ID >= 1 AND t.PK_ID <= 100;

-- 在where条件语句中，由and或or连接的多个条件字段必须全部都有索引，此时索引才能起作用，只要一个字段没有索引，其他都会失效。
DESC SELECT * FROM t_macb_entity t WHERE t.`name` = "dd" OR t.age = 12;
DESC SELECT * FROM t_macb_entity t WHERE t.`name` = "dd" AND t.PK_ID = 1;

-- 如果条件字段是varchar类型，但是查询的时候传入的参数是数字类型，则字段上的索引将失效
DESC SELECT * FROM t_macb_entity t WHERE t.`name` = 33;
DESC SELECT * FROM t_macb_entity t WHERE t.`name` = "33";

-- 3.2.3查看索引使用频率，执行如下语句，如果结果中的Handler_read_rnd_next字段值很高，表示需要添加索引，
-- 至于具体在哪里添加，需要通过慢查询定位及查询语句解析来确定
SHOW STATUS LIKE "Handler_read%";

-- 3.3表优化
-- 3.3.1检查一个或多个表是否存在错误，视图也一样
CHECK TABLE t_macb_entity;
CHECK TABLE view_t_macb_entity;

-- 3.3.2优化表空间，由于长时间的数据操作会造成数据库文件碎片和空洞的积累，影响性能，可以定期做表空间的优化，
-- 如下优化命令只对Myisam、innodb、bdb引擎的表起作用，但是建议在空闲时做，且在做大表操作时系统资源占用很高
OPTIMIZE TABLE t_macb_entity;
? OPTIMIZE;

-- 3.4.1大批量插入数据
-- 传统方式是用执行sql语句的方式来实现数据导入/导出的，这个过程中会执行大量的建表语句和记录插入语句，数据量较小时还可以，一旦量大了就不行。

-- 将查询结果导出到文件，生成的文件中只有数据本身，没有表结构及插入语句
SELECT * FROM t_macb_entity t INTO OUTFILE "D:\kkk.txt";
TRUNCATE t_macb_entity;
SELECT * FROM t_macb_entity;
LOAD DATA INFILE "D:\kkk.txt" INTO TABLE t_macb_entity;

-- load data的方式在导入数据时会同时添加纪录的索引，会影响导入速度
-- 可以将这两个过程分离开来，在导入数据时不添加索引，当数据导入完成后再统一添加，这样会节省更多时间
-- 可以暂时关闭表的非唯一索引，然后导入数据，
-- 下列命令关闭非唯一索引，Myisam引擎
SELECT * FROM t_macb_entity t INTO OUTFILE "D:\kkk.txt";
TRUNCATE t_macb_entity;
SELECT * FROM t_macb_entity;
SHOW INDEX FROM t_macb_entity;
ALTER TABLE t_macb_entity DISABLE KEYS;
SHOW INDEX FROM t_macb_entity;
LOAD DATA INFILE "D:\kkk.txt" INTO TABLE t_macb_entity;
ALTER TABLE t_macb_entity ENABLE KEYS;
SHOW INDEX FROM t_macb_entity;

-- 在实在确保导入数据的唯一性后可以关闭表的唯一索引，然后倒入数据，导完后再打开唯一索引
SET UNIQUE KEYS = 0;
SET UNIQUE KEYS = 1;

-- 对于InnoDB引擎，如果导出文件中的数据是按主键升序或降序排列的，导入时速度会更快
-- 对于InnoDB引擎，不像Myisam的实务机制，在Innodb中默认会自动提交，会每导入一条后提交一次，势必会花更多的时间，所以，可以等全部导入后再一次性提交。
SET autocommit = 0;
SET autocommit = 1;

-- 3.4.2Insert语句的优化，数据插入时尽量使用sql拼接方式，连接一次然后将数据全部插入，再统一提交然后关闭连接，导入dump文件就是用的这种方式。
-- 通常用load data方式比insert语句方式的速度快20倍

-- 3.4.3优化group by
/********************************************/
-- MySql主从集群技术是从服务器级别的负载均衡技术，来实现大数据的存储应用
-- MySql表分区技术是在单个服务器内以切分文件的形式来提高大数据量存储性能的方式

-- 当一个表存储了超过1000万条记录后，单个表的数据文件会变得很大，此时linux系统在这样的大文件中检索数据会非常吃资源
-- 如果将这种大的表文件有规律的切割成较小的多个文件，linux在检索数据时性能会有所提升

-- 一亿条仅仅具有一列id的表体积大概为1G，如果对者列进行索引，索引文件将达到2G，总共就3G了，并且索引添加过程耗时较长。

-- 逻辑优化：SQL优化和添加索引。
-- 物理优化：表的拆分。

-- 分表技术：
-- 	垂直分表：竖着切，将具有多个字段的大表切分为具有较少字段的相互关联的小表，各小表与原表相比结构发生改变。
-- 	水平分表：横着切，将大表根据某个字段的取值特性分为多个小表，各小表和原表结构一致。
-- 	【但是会导致逻辑改变，需要改变SQL语句的写法，代码层面会改动较多，增加了程序的维护及扩展难度】

-- 表分区技术：能从物理上降低操作系统的开销，同时能在逻辑上保持程序的不变。
-- Rang分区：基于属于一个给定连续区间的列值，把多行分配给分区。
CREATE TABLE student(
id INT NOT NULL,
name VARCHAR(20),
birthday DATE NOT NULL DEFAULT "2018-1-31"
)
PARTITION by RANGE (id)(
	PARTITION p0 VALUES Less than (4000),
	PARTITION p0 VALUES Less than (8000),
	PARTITION p0 VALUES Less than (12000),
	PARTITION p0 VALUES Less than (16000)
);

CREATE TABLE student(
id INT NOT NULL,
name VARCHAR(20),
birthday DATE NOT NULL DEFAULT "2018-1-31"
)
PARTITION by RANGE YEAR(birthday)(
	PARTITION p0 VALUES Less than (1000),
	PARTITION p0 VALUES Less than (2000),
	PARTITION p0 VALUES Less than (3000),
	PARTITION p0 VALUES Less than MAXVALUE
);
磁盘上生成的文件样式
student.frm
student.par
student#P#p0.MYD
student#P#p0.MYI
student#P#p1.MYD
student#P#p1.MYI

List分区：可以看作第一种方法的特例，类似Range分区，区别在于分区是基于列值匹配一个离散值列表中的某个值来进行选择。
CREATE TABLE student(
id INT NOT NULL,
name VARCHAR(20),
birthday DATE NOT NULL DEFAULT "2018-1-31",
class INT
)
PARTITION by LIST (class)(
	PARTITION p0 VALUES IN (1,2,3),
	PARTITION p0 VALUES IN (4.5.6),
	PARTITION p0 VALUES IN (7.8.9),
	PARTITION p0 VALUES IN (10,11)
);
Hash分区：在插入记录时，通过用户定义的hash算法将数据平均的放到各个分区中，这个算法只要是MySql中有效的，能产生非负整数的算法就可以，
由于一个分区中数据相关性不高，但是效果明显多用于测试。
CREATE TABLE student(
id INT NOT NULL,
name VARCHAR(20),
birthday DATE NOT NULL DEFAULT "2018-1-31"
)
PARTITION by HASH(YEAR(birthday)) PARTITIONS 12;
Key分区：与hash分区类似，但是key可以不是整数，可以是字符串等。
/********************************************/
-- 3.4.3优化group by语句
-- group by语句默认使用了order by对组内结果进行了升序排序，如果不需要排序可以关闭，以降低开销
-- 可以用ORDER BY NULL语句来关闭排序

-- 3.4.5嵌套查询优化，由于嵌套查询只有内层查询能命中索引，外层查询不能命中索引，所以应该避免使用嵌套查询
-- 建议使用普通多表查询（用逗号分隔）、内链接、左连接、右连接等查询方式，这几种方式都能很好的命中索引

-- 四、数据库优化
-- 1.引擎的选择：如果不用外键和事务机制，尽量用MyIsam，因为它比innodb更快。
-- 2.将大表拆分成较小的文件来减少cpu的开销
-- 3.使用中间表或视图来提高统计查询的速度，为了提高灵活性，优先用中间视图的方式，因为试图中的数据是一个映射，会随着源表数据的改变而改变

-- 五、服务器优化
-- 一般在备份数据时才会显式的加锁
-- 1.myisam读锁：自己能读、别人也能读，但是任何人都不能写，包括自己
-- LOCK TABLE t1 READ;
-- UNLOCK TABLE t1;
-- UNLOCK TABLES;

-- 2.myisam写锁：自己能读能写，其他人不能读更不能写。
-- LOCK TABLE t1 WRITE;
-- UNLOCK TABLE t1;
-- UNLOCK TABLES;

-- 六Mysql服务器优化
6.1四种字符集问题
	服务器、数据库、客户端、连接字符集统一设为UTF-8
	在MySql安装路径下的my.ini配置文件中：
		[client]之default-character-set=utf8  //控制客户端和连接字符集
		[mysqld]character-set-server=utf8  //控制服务器和数据库字符集及继承下来的表字符集
		[mysqld]collation-server=utf8_general_ci  //校验字符集，用于校验字段值，比如排序用

6.2binary log日志
6.3slow log慢查询日志
6.4soket问题
6.5root密码丢失
/********************************************/
一、在spring的应用中我们存在两种过滤的用法，一种是拦截器、另外一种当然是过滤器。我们这
	里介绍过滤器在springboot的用法，在springmvc中的用法基本上一样，只是配置上面有点区别。
二、filter功能，它使用户可以改变一个 request和修改一个response. Filter 不是一个servlet,它不能产生一个response,它能够在一个
	request到达servlet之前预处理request,也可以在离开 servlet时处理response.换种说法,filter其实是一个”servlet chaining”(servlet 链).
	一个Filter包括：
	1）、在servlet被调用之前截获;
	2）、在servlet被调用之前检查servlet request;
	3）、根据需要修改request头和request数据;
	4）、根据需要修改response头和response数据;
	5）、在servlet被调用之后截获.
三、Spring-Boot实例
@Component
@ServletComponentScan
@WebFilter(urlPatterns = "/login/*",filterName = "loginFilter")
public class LoginFilter implements Filter{

    @Override
    public void init(FilterConfig filterConfig) throws ServletException {

    }

    @Override
    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {

    }

    @Override
    public void destroy() {

    }
}
四、解释：
	1、@Component 这个注解的目的是将LoginFilter交给容器来处理。也就是让LoginFilter起作用
	2、@ServletComponentScan 这个使用来扫描@WebFilter 的让@WebFilter起作用。当然对于servlet线管注解也是可以的。
		这个@ServletComponentScan最好写在Apllication上面，通用配置。我这里因为只有一个Filter所以没有写在Application上面。
	3、@WebFilter 这个用处显而易见，针对于什么链接做过滤，filter的名称是为什么。
/********************************************/
传统SpringMVC实例
1、写的方法还是一样的都是实现Filter接口，来实现3个方法处理
2、丢入容器：这个需要配置在web.xml里面
 <filter>
	<filter-name>loginFilter</filter-name>
	<filter-class>com.troy.boot.filter.LoginFilter</filter-class>
</filter>
<filter-mapping>
	<filter-name>loginFilter</filter-name>
	<url-pattern>/*</url-pattern>
</filter-mapping>
/********************************************/
1.创建一个ServerSocket，监听并绑定
2.一个端口一系列客户端来请求这个端口
3.服务器使用Accept，获得一个来自客户端的Socket连接对象
4.启动一个新线程处理连接
	4.1读Socket，得到字节流
	4.2解码协议，得到Http请求对象
	4.3处理Http请求，得到一个结果，封装成一个HttpResponse对象
	4.4编码协议，将结果序列化字节流
	4.5写Socket，将字节流发给客户端
5.继续循环步骤3

/********************************************/
svn up hbz
cd hbz

mvn install:install-file -Dfile=/opt/hbz/hbz/hbz-sdk/libs/alipay/alipay-sdk-java20171012102412.jar -DgroupId=com.alipay -DartifactId=sdk-jar -Dversion=1.0 -Dpackaging=jar
mvn install:install-file -Dfile=/opt/hbz/hbz/hbz-sdk/libs/alipay/commons-logging-1.1.1.jar -DgroupId=com.alipay -DartifactId=log1-jar -Dversion=1.0 -Dpackaging=jar
mvn clean install package
kill -9 $(ps -ef|grep hbz | grep -o [0-9]* | grep -m1 [0-9]*)
killall -9 java
cd ..

#nohup java -Xdebug -Xrunjdwp:transport=dt_socket,address=55891,suspend=n,server=y -jar -Dspring.profiles.active=common,dev -Dserver.port=81 /opt/hbz/hbz/hbz-starter/target/hbz-starter-0.0.5-
SNAPSHOT.jar &
#nohup java -Xdebug -Xrunjdwp:transport=dt_socket,address=55892,suspend=n,server=y -jar -Dspring.profiles.active=common,dev -Dserver.port=82 hbz/hbz-management/target/hbz-management-0.0.1-SNA
PSHOT.jar &

nohup java -jar -Dspring.profiles.active=common,dev -Dserver.port=81 /opt/hbz/hbz/hbz-starter/target/hbz-starter-0.0.5-SNAPSHOT.jar &
nohup java -jar -Dspring.profiles.active=common,dev -Dserver.port=82 hbz/hbz-management/target/hbz-management-0.0.1-SNAPSHOT.jar &
/********************************************/
Nginx
/********************************************/
在JavaNIO中，涵盖两个方面，一是磁盘IO，二是网络IO
1.磁盘IO的性能有所提升，依托的是内存映射技术实现的零拷贝（由于内核缓存的有限性，零拷贝在大文件传输时优势更明显），
	但是映射的内存为本地内存它不受JVM管理，所以存在GC延迟等问题（磁盘IO仍然时阻塞的）。
2.网络IO引入了支持了异步处理机制，是非阻塞的，基于事件机制的多路复用技术，主要在这方面实现性能的提升。
/********************************************/
Java之NIO
一、对于磁盘和网络：
		BIO面Stream，流是单向的，流除了负责连接与传输，同时也负责数据的包装。
		NIO面向Channel和Buffer，通道是双向的，只负责连接与传输，由Buffer负责数据的包装。
	对于网络IO：
		NIO是非阻塞式的，BIO是阻塞式的，NIO具有多路复用选择器Selector，BIO没有。
二、缓冲区
	1.三个核心属性：
		capacity：容量，表示缓冲区的最大容量，一旦申明不可改变。
		limit：界限，表示缓冲区中可以操作的数据的大小（limit及其后位置的数据是拒绝操作的）。
		position：表示缓冲区中正在操作的位置，类似当前的游标。
		
		mark：用于标记上次position的位置，可以通过reset方法将position恢复到上次mark的位置
		四个属性存在如下关系：0 <= mark <= position <= limit <= capacity
	2.两种模式：
		写模式：capacity和limit位置相同，position处在当前欲写入位置。
		读模式：capacity不变，limit处在当前数据量的下一位，position处在欲读取的位置。
		通过flip方法切换读写两种模式，随着模式的切换属性值会随之变化。
	3.直接缓冲区与非直接缓冲区：
		非直接缓冲区：通过allocate()方法分配的缓冲区，建立在JVM内存中。
		直接缓冲区：通过allocateDirect()方法分配的缓冲区，建立在操作系统的物理内存中，可以不受大小限制，提高效率。
		BIO和非直接缓冲区的读写过程：
			App从磁盘读取数据过程：App向系统发出read请求，首先系统将数据从磁盘读取到内核地址空间，再从内核地址空间copy到用户地址空间，App最后才从用户地址空间读取数据。
			App向磁盘写入数据过程：App向系统发出write请求，App首先将数据写入用户地址空间，再将数据从用户地址空间copy到内核地址空间，然后再将数据从内核地址空间写入磁盘。
		直接缓冲区的数据读写过程：（仅ByteBuffer支持）
			直接再系统内存中建立缓冲区，数据读写路径变为这样：
				磁盘<-->直接缓冲区<-->APP
				省略了一个copy过程，提高了效率，此即为ZeroCopy机制，适合大量或长时间停留内存的数据。
				弊端：内存消耗更大，内存的回收由系统控制，JVM持有资源得不到及时释放。
三、通道
	1.早期计算机IO接口也由CPU独立负责，当IO并发较高时大量CPU计算资源消耗在了IO接口的管理上。
	2.后来设计了直接存储器DMA来全权管理IO接口，但是DMA仍然会向CPU申请权限，但是读写请求量过大时花发生总线冲突问题，也会影响性能。
	3.后来内存和IO接口间添加了Channel，它完全独立的附属与CPU的处理器，功能上彻底和CPU独立，
	
	1、用于源节点和目标节点的连接，在JavaNIO里负责Buffer中数据的传输，需要配合Buffer才能传输数据。
	2、通道分类
		|--FileChannel  //用于文件IO，只有阻塞模式
		|--SocketChannel  //用于TCP客户端
		|--ServerSocketChannel  //用于TCP服务器端
		|--DatagramChannel  //用于UDP
		
		|--Pipe.SinkChannel、
		|--Pipe.SourceChannel）
	
	3、获取通道
		1.Java针对支持通道的类提供了getChannel()方法
			文件IO：
				FileInputStream
				FileOutputStream
				RandomAccessFile
			网络IO：
				Socket
				ServerSocket
				DatagramSocket
		2.JDK1.7后，提供了NIO.2针对各个通道类提供了静态的open()方法
		3.JDK1.7后，提供了NIO.2 Files工具类提供了newByteChannel()方法
	4、通道间的数据传输（直接内存缓冲区）
		transferFrom()
		tranferTo()
	5、分散读取与聚集写入（不支持直接缓冲区）
		Scatter：将一个通道中的数据从前到后依此按缓冲区容量分散到多个缓冲区中
		Gathering：将多个缓冲区内的数据先后顺序依此聚集写入到一个通道中
	6、字符集
		编码：字符串->字节数组
		解码：字节数组->字符串
----网络IO
1.单线程时代：服务器端只有一个线程负责处理所有的请求，请求到达服务端后需排队等待，只有前面的请求处理完了，后面的才有机会，CPU利用率很低，吞吐量也低。
2.多线程时代：服务器端为每一个到来的请求都单独开启一个线程来处理，这样提高了CPU的利用率，吞吐量得到提高，但是服务器能分配的线程数总是有限的，
				请求数量较大时服务器线程用完，有部分线程阻塞时，服务器端的吞吐量也会很低。
				
3.非阻塞时代：
	服务端启动有个Selector，所有客户端连接的Channel都注册到这个Selector上，只有当某个Channel的数据完全准备就绪，
	Selector才真正将这个Channel分配给服务器的一个或多个线程去处理，所以，如果某个请求的数据没有完全准备就绪，
	它就不会占用一个线程，服务器就可以用这些省下来的线程资源去处理别的就绪连接或者其他事情，线程的利用率就提高了，
	CPU的真实利用率也提高了。
4.Pipe是两个线程间的单向数据连接
/********************************************/
JavaNIO之ZeroCopy工作机制及演进
1.单线程阻塞地既处理请求的连接，又处理请求的后续连接。
2.单线程阻塞地处理请求连接，请求的后续逻辑另启线程处理。
3.单线程阻塞地处理请求连接，请求的后续逻辑用线程池机制来处理。
/********************************************/
Java之BIOSocket通信演变
一、对于所有请求只起一条线程，该线程即负责连接的建立又负责后续业务逻辑处理，所以必须等待上一次请求处理完毕，
下一次请求才会得到机会连接，也就是上一次连接的阻塞会导致下依此连接的等待。
public void startService(){
 2         while (true) {
 3             Socket socket = null;
 4             try {
 5                 socket = serverSocket.accept();
 6                 System.out.println("new connection accepted " + socket.getInetAddress() + ":" + socket.getPort());
 7                 BufferedReader br = getBufferReader(socket);//获得socket输入流,并将之包装成BufferedReader
 8                 PrintWriter pw = getWriter(socket);//获得socket输出流,并将之包装成PrintWriter
 9                 String msg = null;
10                 while ((msg = br.readLine()) != null) {
12                     pw.println(echo(msg));//服务端的处理逻辑,将client发来的数据原封不动再发给client
13                     pw.flush();
14                     if(msg.equals("bye"))//若client发送的是 "bye" 则关闭socket
15                         break;
16                 }
17             } catch (IOException e) {
18                 e.printStackTrace();
19             } finally {
20                 try{
21                     if(socket != null)
22                         socket.close();
23                 }catch(IOException e){e.printStackTrace();}
24             }
25         }
26     }
上面用的是while(true)循环，这样，Server不是只接受一次Client的连接就退出，而是不断地接收Client的连接。
1）第5行，服务器线程执行到accept()方法阻塞，直至有client的连接请求到来。
2）当有client的请求到来时，就会建立socket连接。从而在第8、9行，就可以获得这条socket连接的输入流和输出流。输入流(BufferedReader)负责读取client发过来的数据，输出流(PrintWriter)负责将处理后的数据返回给Client。
下面来详细分析一下建立连接的过程：
Client要想成功建立一条到Server的socket连接，其实是受很多因素影响的。其中一个就是：【Server端的“客户连接请求队列长度”】。它可以在创建ServerSocket对象时在构造方法中的backlog参数指定：JDK中 backlog参数的解释是： 
requested maximum length of the queue of incoming connections.
    public ServerSocket(int port, int backlog) throws IOException {
        this(port, backlog, null);
    }
看到了这个：incoming commections 有点奇怪，因为它讲的是“正在到来的连接”，那什么又是incoming commections 呢？这个就也TCP建立连接的过程有关了。
TCP建立连接的过程可简述为三次握手。
第一次：Client发送一个SYN包，Server收到SYN包之后回复一个SYN/ACK包，此时Server进入一个“中间状态”--SYN RECEIVED 状态。
这可以理解成：Client的连接请求已经过来了，只不过还没有完成“三次握手”。因此，【Server端需要把当前的请求保存到一个队列里面】，
直至当Server再次收到了Client的ACK之后，Server进入ESTABLISHED状态，此时：serverSocket 从accpet() 阻塞状态中返回。
也就是说：当第三次握手的ACK包到达Server端后，Server从该请求队列中取出该连接请求，同时Server端的程序从accept()方法中返回。
那么这个请求队列长度，就是由 backlog 参数指定。那这个队列是如何实现的呢？这个就和操作系统有关了，感兴趣的可参考：How TCP backlog works in Linux
此外，也可以看出：服务器端能够接收的最大连接数 也与 这个请求队列有关。
对于那种高并发场景下的服务器而言，
	1.首先就是请求队列要足够大。
	2.其次就是当连接到来时，要能够快速地从队列中取出连接请求并建立连接，因此，执行建立连接任务的线程最好不要阻塞。
现在来分析一下上面那个：单线程处理程序可能会出现的问题：
服务器始终只有一个线程执行accept()方法接受Client的连接。建立连接之后，又是该线程处理相应的连接请求业务逻辑，这里的业务逻辑是：把客户端发给服务器的数据原封不动地返回给客户端。
显然，这里一个线程干了两件事：接受连接请求 和 处理连接（业务逻辑）。好在这里的处理连接的业务逻辑不算复杂，如果对于复杂的业务逻辑 而且 有可能在执行业务逻辑过程中还会发生阻塞的情况时，那此时服务器就再也无法接受新的连接请求了。

二、启动一条线程专门负责所有请求连接的建立，针对每个请求新起一条业务逻辑线程，即使有业务线程阻塞了，也不会影响其他连接的建立与后续逻辑的处理。
1     public void service() {
 2         while (true) {
 3             Socket socket = null;
 4             try {
 5                 socket = serverSocket.accept();//接受client的连接请求
 6                 new Thread(new Handler(socket)).start();//每接受一个请求 就创建一个新的线程 负责处理该请求
 7             } catch (IOException e) {
 8                 e.printStackTrace();
 9             } 
10             finally {
11                 try{
12                     if(socket != null)
13                         socket.close();
14                 }catch(IOException e){e.printStackTrace();}
15             }
16         }
17     }
从上面的单线程处理模型中看到：如果线程在执行业务逻辑中阻塞了，服务器就不能接受用户的连接请求了。
而对于一请求一线程模型而言，每接受一个请求，就创建一个线程来负责该请求的业务逻辑。尽管，这个请求的业务逻辑执行时阻塞了，
只要服务器还能继续创建线程，那它就还可以继续接受新的连接请求。此外，负责建立连接请求的线程 和 负责处理业务逻辑的线程分开了。
业务逻辑执行过程中阻塞了，“不会影响”新的请求建立连接。
然而，如果Client发送的请求数量很多，那么服务器将会创建大量的线程，而这是不现实的。有以下原因：
	1）创建线程是需要系统开销的，线程的运行系统资源（内存）。因此，有限的硬件资源 就限制了系统中线程的数目。
	2）当系统中线程很多时，线程的上下文开销会很大。比如，请求的业务逻辑的执行是IO密集型任务，经常需要阻塞，这会造成频繁的上下文切换。　　
	3）当业务逻辑处理完成之后，就需要销毁线程，如果请求量大，业务逻辑又很简单，就会导致频繁地创建销毁线程。

三、起一条专门的线程负责连接的建立，业务逻辑线程交给线程池去处理，既有效的将连接处理和业务逻辑两类线程分开，有用资源池机制控制了线程开销。
1 public class EchoServerThreadPool {
 2     private int port = 8000;
 3     private ServerSocket serverSocket;
 4     private ExecutorService executorService;
 5     private static int POOL_SIZE = 4;//每个CPU中线程拥有的线程数
 6     
 7     public EchoServerThreadPool()throws IOException {
 8         serverSocket = new ServerSocket(port);
 9         executorService = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors() * POOL_SIZE);
10         System.out.println("server start");
11     }
12     
13     public void service(){
14         while(true){
15             Socket socket = null;
16             try{
17                 socket = serverSocket.accept();//等待接受Client连接
18                 executorService.execute(new Handler(socket));//将已经建立连接的请求交给线程池处理
19             }catch(IOException e){
20                 e.printStackTrace();
21             }
22         }
23     }
24     public static void main(String[] args)throws IOException{
25         new EchoServerThreadPool().service();
26     }
27 }
采用线程池最大的优势在于“重用线程”，有请求任务来了，从线程池中取出一个线程负责该请求任务，任务执行完成后，线程自动归还到线程池中，
而且java.util.concurrent包中又给出了现成的线程池实现。因此，这种方式看起来很完美，但还是有一些问题是要注意的：
1）线程池有多大？即线程池里面有多少个线程才算比较合适？这个要根据具体的业务逻辑来分析，而且还得考虑面对的使用场景。
一个合理的要求就是：尽量不要让CPU空闲下来，即CPU的复用率要高。如果业务逻辑是经常会导致阻塞的IO操作，一般需要设置 N*(1+WT/ST)个线程，
其中N为可用的CPU核数，WT为等待时间，ST为实际占用CPU运算时间。如果业务逻辑是CPU密集型作业，那么线程池中的线程数目一般为N个或N+1个即可，
因为太多了会导致CPU切换开销，太少了（小于N），有些CPU核就空闲了。
2）线程池带来的死锁问题
线程池为什么会带来死锁呢？在JAVA 1.5 之后，引入了java.util.concurrent包。线程池则可以通过如下方式实现：
ExecutorService executor = Executors.newSingleThreadExecutor();
//ExecutorService executor = Executors.newFixedThreadPool(2);
executor.execute(task);// task implements Runnable
executor.shutdown();
Executors可以创建各种类型的线程池。如果创建一个缓存的线程池:
ExecutorService executor = Executors.newCachedThreadPool();
对于高负载的服务器而言，在缓存线程池中，被提交的任务没有排成队列，而是直接交给线程执行。也就是说：只要来一个请求，如果线程池中没有线程可用，
服务器就会创建一个新的线程。如果线程已经把CPU用完了，此时还再创建线程就没有太大的意义了。因此，对于高负载的服务器而言，一般使用的是固定数目的线程池(来自Effective Java)
主要有两种类型的死锁：①线程A占有了锁X，等待锁Y，而线程B占用了锁Y，等待锁X。因此，向线程池提交任务时，要注意判断：提交了的任务(Runnable对象)会不会导致这种情况发生？
②线程池中的所有线程在执行各自的业务逻辑时都阻塞了，它们都需要等待某个任务的执行结果，而这个任务还在“请求队列”里面未提交！
3）来自Client的请求实在是太多了，线程池中的线程都用完了(已无法再创建新线程)。此时，服务器只好拒绝新的连接请求，导致Client抛出：ConnectException。
4）线程泄露
导致线程泄露的原因也很多，而且还很难发觉，网上也有很多善于线程池线程泄露的问题。比如说：线程池中的线程在执行业务逻辑时抛异常了，怎么办？
是不是这个工作线程就异常终止了？那这样，线程池中可用的线程数就少了一个了？看一下JDK ThreadPoolExecutor 线程池中的线程执行任务的过程如下：
复制代码
       try {
            while (task != null || (task = getTask()) != null) {
                w.lock();
                if ((runStateAtLeast(ctl.get(), STOP) ||
                     (Thread.interrupted() &&
                      runStateAtLeast(ctl.get(), STOP))) &&
                    !wt.isInterrupted())
                    wt.interrupt();
                try {
                    beforeExecute(wt, task);
                    Throwable thrown = null;
                    try {
                        task.run();
                    } catch (RuntimeException x) {
                        thrown = x; throw x;
                    } catch (Error x) {
                        thrown = x; throw x;
                    } catch (Throwable x) {
                        thrown = x; throw new Error(x);
                    } finally {
                        afterExecute(task, thrown);
                    }
                } finally {
                    task = null;
                    w.completedTasks++;
                    w.unlock();
                }
            }
            completedAbruptly = false;
        } finally {
            processWorkerExit(w, completedAbruptly);
        }
复制代码
从上面源码看出：线程执行出异常后是由 afterExecute(task, thrown) 来处理的。至于对线程有何影响，我也没找到很好的解释。
另外一种引起线程泄露的情况就是：线程池中的工作线程在执行业务逻辑时，一直阻塞下去了。那这也意味着这个线程基本上不干活了，
这就影响了线程池中实际可用的线程数目。如何所有的线程都是这种情况，那也无法向线程池提交任务了。此外，关于线程池带来的问题还可参考：
Java编程中线程池的风险规避  另外， 关于JAVA线程池使用可参考下：Java的Executor框架和线程池实现原理
到这里，阻塞通信的三种模式都已经介绍完毕了。在网上发现了一篇很好的博文，刚好可以配合我这篇文章的代码演示一起来看：架构设计：系统间通信（1）——概述从“聊天”开始上篇
/********************************************/
1.点对点：(point to point， queue)
	消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。
	注意：消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。
	Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。
2.发布/订阅：(publish/subscribe，topic)
	消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。
/********************************************/
curl -sSL https://get.docker.com/ | sh  //通过curl执行远程脚本来安装docker（网络未成功）
yum list | grep docker  //通过yum安装docker
yum -vy install docker.x86_64

systemctl start docker  //启动docker
docker run -d centosImage  //以后台方式用指定镜像启动一个容器
docker ps  //查看当前运行中的容器
docker ps -a  //查看所有容器
docker attach containerName  //登录到容器中
docker stop containerName  //停止一个容器
docker rm containerName  //删除已停止的容器
/********************************************/
https://110.188.66.14:60002
帐号：Develop10
密码：Develop@2018。
/********************************************/
1.Redis作为缓存提高应用吞吐量的原理：
	对于热点数据先查看缓存中有没有需要的数据，如果有就直接从Redis中拿并返回给前端，如果没有就从DB中拿然后存到Redis再返回给前端，数据更新需要同步更新缓存中的数据。
2.缓存击穿：前台发来大量请求的数据在Redis中都没有，这样请求就绕开了缓存导致缓存失效，导致大量请求都到达DB导致DB性能下降。
/********************************************/
MySql并发数极限：
	机械硬盘：300
	固态硬盘：700
/********************************************/
Hibernate缓存
1.一级缓存：范围为单个session，内置且不可卸载。
2.二级缓存：范围为sessionFactory，即整个应用进程范围，可插拔。
/********************************************/
redis可以作为二级缓存，用来减少数据的压力提高响应速率，redis还有个用法是作为架构的系统缓存，
例如加载初始化属性、静态资源、结点间共享数据等，MainController中有此种用法
/********************************************/
HashOperations的void put(H key, HK hashKey, HV value);方法具有三个参数，第一个代表hash集合对象的key，第二个hashKey表示hash集合内元素的key。
/********************************************/
默认情况下session是存在站点内存中的，添加如下的空配置类即可实现将session转存到Redis中（内存中就没有了），有利于分布式session的实现
@Configuration
@EnableRedisHttpSession
public class RedisSessionConfig {
}
/********************************************/
Redis事件通知机制
1、可以利用Redis键过期触发客户端其他事件，利用这一机制可以实现定时器。
2、出于性能方面的考量，Redis服务器默认是关闭了此功能的所以需要在配置文件redis.windows-service.conf中打开，notify-keyspace-events Ex，只需要追加“Ex”参数。
3、需要java项目中配置监听器，监听器容器，监听器线程池。
4、项目启动后如果redis内有key过期，java项目将收到通知。
/********************************************/
Repository<T, ID extends Serializable>;  //顶层接口，只是是一个标记接口
CrudRepository<T, ID extends Serializable> extends Repository<T, ID>  //中级接口，主要负责CRUD操作
PagingAndSortingRepository<T, ID extends Serializable> extends CrudRepository<T, ID>  //次级接口，主要负责分页和排序

JpaSpecificationExecutor<T>  //不属于Repository体系

BaseRepository<T, I extends Serializable> extends PagingAndSortingRepository<T, I>, JpaSpecificationExecutor<T>  //兼具上面两个体系的能力，供业务repository继承
/********************************************/
曾供职亚信科技，具备多年铁路、电信行业大数据系统设计与开发经验，
擅长高并发、分布式系统设计开发，曾参与多个智能检测、识别、推荐项目，
目前主要从事机器学习领域的研究与开发。
/********************************************/
Content-Type：text/html;  //代表发送端发送的数据格式是html。
Accept：text/xml;  //代表客户端希望接受的数据类型是xml类型
/********************************************/
LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyy-MM-dd hh:mm"));
/********************************************/
show VARIABLES like '%max_allowed_packet%';
set global max_allowed_packet = 2*1024*1024*10
/********************************************/
Spring-data-jpa之Repository继承关系
Repository > CrudRepository > PagingAndSortingRepository > JpaRepository
															JpaSpecificationExecutor
/********************************************/
马士兵JVM调优
1.JVM内存结构
	运行时数据区：{
		堆空间：
		方法区(现在叫permanent即永久区)：用于存储静态变量、常量、常量池、类Class对象等永久对象。
		栈空间：在这里为每个线程开辟独立的栈内存，每个线程中的一个方法对应这块栈内存中的一个栈帧，这个方法中的所有局部变量都放在方法对应的栈帧中。
		本地方法栈：本地C语言或者会变函数执行时使用的内存空间。
		程序计数器：程序运行时的临时计数器。
	}
2.堆内存结构
	heap{
		new{  //新生，占堆的1/4
			eden{},  //占新生代8成
			survivorFrom{  //占新生代1成
			},
			survivorTo{  //占新生代1成
			},
		},
		old(tenured){  //老年代，占堆的3/4
		}
	}
	*当一个对象被实例化出来后，如果对象特别大，会被直接放入老年代，否则放到新生代中，同时经过多轮GC任然存活的对象也将被放入老年代。
	*整个新生代经过一次GC，如果eden中有对象没被回收，将被放入survivorTo中，
	 此时survivorFrom中没被回收的对象也会被全部高效拷贝到survivorTo中，
	 且survivorFrom被清空，每轮GC都如此，但to和from角色转变。
	 
	*Java对象分配
		栈上分配（server模式默认开了这种优先级的，由于栈会在线程结束后就销毁所以就不用GC，效率更高）：
			线程私有小对象
			无逃逸（不能有外部引用，如果有引用就代表逃逸了，如果有则会分配到栈外）
			支持标量替换（即变量可以作为基本数据类型存储于栈上）
			无需调整
		线程本地分配缓存TLAB（相等于cpu缓存，Valentino关键字作用地点）：
			在Eden开辟，默认占Eden的1%
			多线程的时候不用竞争Eden就可以直接申请空间，提高效率
			小对象
			无需调优
		老年代：
			大对象
		Eden：
		【首先考虑分配到栈上如果放不下就考虑线程本地分配，如果TLAB放不下且对象不大就分配到Eden，如果大的话就分配到老年代。】
		程序运行的虚拟机参数：
			-XX:-DoEscapAnalysis  //代表禁用逃逸分析（默认是开启的，会用逃逸分析自动判断，如果无逃逸就会考虑分配到栈）
			-XX:-EliminateAllocation  //警用标量替换（默认是开启的）
			-XX:-UseTLAB  //禁用线程本地缓存（默认是开启的）
			-XX:+PrintGC  //开启GC打印（默认是未开启的）
			-XX:+PrintGCDetails  //开启GC细节打印（默认是未开启的）
			
3.堆内存调参
	- 标准参数，即所有JVM必须都支持
	-X 非标准参数，可能不同JVM实现得有差异
	-XX 不稳定参数，不同版本实现都可能不同，上一版本有的参数下一版本就可能没有了
	
	3.1使用VisulVM监控JVM堆信息
4.垃圾收集算法
	4.1引用类型：强、软、弱、虚引用，通常我们直接new出来的对象叫强引用。
	4.2什么是垃圾：程序中用不到的对象就是垃圾，没有被引用的对象，没有被外界引用的循环引用对象
	4.3如何确定垃圾：
		4.3.1引用计数法【不采用】：对象被引用一次就做一次计数，如果引用次数大于0则表示仍然被引用，不认为是垃圾。  //会产生垃圾间的循环引用问题
		4.3.2正向可达法【采用】：从GCroots对象出发，如果某对象可通过直接引用或间接引用到达则不认为是垃圾，main方法中的对象、classLoader可以作为GCroot对象。
	4.4垃圾收集算法：
		Mark-Sweep【不采用】：经过GC确认部分对象为垃圾，然后将垃圾表时出来，将对应内存块标记为可覆盖状态，新来的对象就能写在上面，
							  但是会导致内存使用不连续出现碎片化问题，当构造出新的比较大的对象时需要连续内存而无法存放，此时就会触发fullGC，
							  fullGC会将全部对象向内存区的一端压缩，此过程效率很低，会导致延迟。
		Copping【默认用在新生代】：将内存划分为两部分，一半置空，一半存放对象，一次GC后，将所有存活的对象连续的拷贝到空的一半内存中，效率高，
				 也解决了内存碎片化问题，但是浪费了空间。
		Mark-Compack【默认用在tenured】：将垃圾标记出来，然后将存活的对象移动到一端，放在连续的内存上。
5.垃圾收集器选择
	5.1Serial Collector：串行收集器，速度快，单线程，不适合多核环境。
	5.2并行收集器：并发量大，每次收集时需要JVM较长停顿。
	5.3并发收集器：
		CMS并发标记清除：分块多线程，各线程负责收集一块，停顿短。
		G1收集器：停顿短，并发大。
6.调优Tomcat并使用JMeter进行评测
	-Xms10M  //程序起始的堆内存大小
	-Xmx10M  //程序最大的堆内存大小
	-Xss512k  //栈大小
	-XX +AggressiveOpts  //侵略性调优（即开启所有调优项，包括JVM升级后的调优项）
	-XX +UseBiasedLocking  //使用偏置所
	-XX:PermSize=64M  //永久代空间初始大小
	-XX:MaxPermSize=300M  //永久代最大空间
	-XX +DisableExplicitGC  //禁用掉显式GC调用（System.gc();）
	
	-XX:+UseConcMarkSweepGC  //使用CMS缩短响应时间，并发收集，低停顿
	-XX:+UseParNewGC  //并行收集新生代
	-XX:+CMSParallelRemarkEnable  //并行的使用CMS，尽量减少remark的时间
	-XX:+UseCMSCompactAtFullCollection  //使用并发收集时开启对老年代的压缩，以减少碎片
	-XX:LargePageSizeByte=128  //设置内存分页大小，以提高性能
	-XX:+UseFastAccessorMethods  //将getter和setter转为本地代码
	-Djava.awt.headless=true  //修复Linux下Tomcat处理图表时可能产生的一个bug
/********************************************/
内存溢出
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=d:/jvmDump.dump
-Xms10M  //程序起始的堆内存大小
-Xmx10M  //程序最大的堆内存大小【调优时一般起始堆内存稍小于最大堆内存，或者直接设为相等，这样就避免了运行过程中频繁检测和分配内存的大小，节省了计算开销】
/********************************************/
-Xss128k  //栈空间大小为128k（默认为128k，如果设置得比较小，线程数就可以比较多，适合高并发，如果设置的比较大，所以就可以更深层的调用）
/********************************************/
持久加压：测峰值
稳定运行：测平均吞吐量
/********************************************/
利用Jmeter性能测试
1.打开Jmeter：双击或者用命令执行jmeterw.cmd脚本，启动Jmeter桌面端软件
2.创建线程组：右击TestPath>Add>Threads(Users)>ThreadGroup>填写线程组名称等参数
3.创建测试用例：右击新建的线程组>Add>Sampler>HttpRequest>填写好相关的ip、port、url等参数
4.新建测试图表：右击新建的线程组>Add>Listener>Aggregate Graph>填写名称等参数
5.保存测试用例，点击run按钮，查看执行统计图表的throughput即吞吐量，可以清除本次测试结果等操作
/********************************************/
VirtualBox安装Centos
1.安装虚拟机时需要排除掉软驱启动，同时将光驱排到第一位置，这样才能加载系统镜像。
2.安装成功后，如果使用NAT网络模式，开启虚拟机ip后只能虚拟机向宿主机和外网单向通信，所以无法从宿主机通过SSH连接。
	这是由于VirtualBox只虚拟了一张网卡，无法实现双向通信（VMware虚拟了两张），要想从宿主机远程管理虚拟机需要用虚拟机实例的网络端口转发功能。
/********************************************/
本地工作空间
	git status  //查看工作区中文件的变动状态
	git add File.java  //将文件添加到本地暂存区
本地暂存区
	git status  //查看变动文件是否成功添加到了本地暂存区
	git commit -m "本次注释内容"  //将暂本地存区的改动提交到本地仓库
本地仓库
	git fetch origin master  //从远程仓库拉去最新版本到本地
	git merge FETCH_HEAD  //将拉取的最新版本与本地版本合并
远程仓库
	git push -u origin master  //将本地仓库推送到远程仓库
/********************************************/
基于TensorFlow的神经网络：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重或参数来得到模型。
张量：即多维数组，标量即为0阶张量，向量为一阶张量，矩阵为二阶张量，三维数组即为三阶张量。
/********************************************/
Nginx将http请求转发至https
server {
    listen 80;
    server_name www.hg-cloud.cn;
    rewrite ^(.*)$ https://$host$1 permanent;
}
/********************************************/
单体应用的不足：
1.项目代码量大
2.编译部署耗时长
3.改动成本高
4.扩容消耗资源
/********************************************/
Spring-Cloud组件：
1.commons：Spring-Cloud基础公共包。
2.Spring-Cloud-Config：一个可以基于Git、SVN或者操作系统文件系统进行保存的分布式统一配置管理中心。
	对标产品有disconf(百度)、diamond（阿里）、apollo（携程比前面两个更好），但是他们都是通过数据库来保存配置的
	Spring-Cloud-Config的优点：
	1.和Spring生态无缝对接。
	2.完全支持Spring的各种注解。
	3.配置不需要保存在DB中。
	Spring-Cloud-Config>apollo>diamond>disconf
3.Spring-Cloud-Console：
	Spring-Cloud-Console/Spring-Cloud-Zookeeper/Spring-Cloud-Etcd/eurka
	Zookeeper：java开发，基于pax算法，
/********************************************/
spring-cloud-netflix
1.Eureka
	服务端只需要引入eureka-server
	客户端（包括生产者和消费者）都是引入eureka-client，同时需要在项目classPath下建立config.properties文件（文件可以包含配置项），同时还要引入web-starter才能正常启动并完成注册。
2.Ribbon
3.Hystrix
4.Zuul
/********************************************/
Centos7上Nexus-3.x安装配置
1.安装JDK。
2.到Nexus官网https://www.sonatype.com下载Linux版安装包：nexus-3.11.0-01-unix.tar.gz。
3.将压缩包上传到系统，解压。
4.
mvn deploy:deploy-file -DgroupId=com.yang -DartifactId=fastdfs-client-java -Dversion=1.24 -Dpackaging=jar -Dfile=E:\appData\mavenRepo\com\squareup\okio\okio\1.14.0 -Durl=http://118.24.122.172:8777/nexus/repository/xuyinPrivate/ -DrepositoryId=xuyinPrivate
/********************************************/
1.Java7中HashMap的实现方式是hash函数+entry数组+entry链表的方式，首先调用key的hash函数计算出其hashcode用以在entry数组中定位，
	如果存在hashcode相同即发生hash碰撞的情况就再调用key的equals方法，如果相等则替换掉这个key对应的value，
	如果不相等即发生了hash碰撞，则以后来居上的规则在这个元素上添加链表成员，保存新来的元素，
	这种处理碰撞的方式在put和get时会对碰撞元素上的链表进行遍历，这个过程会逐个equals链表的所有元素，存在性能浪费。
	entry数组有0.75的负载因子，即当元素个数超过数组长度的75%，数组就会扩容resize1.5倍，
	扩容过程会重新将所有的entry（包括链表上的元素也会重新映射）映射到新数组上，这样性能开销较大，
2.Java8中HashMap，沿用了Java7的实现机制，但是当碰撞链表长度大于8时，且所有元素个数大于64时，
	就会将链表转换为红黑树，此时除了添加外，其他操作的性能都更高因为使用了分治法，扩容操作树上元素的重定位也更快。
3.Java7中ConcurrentHashMap使用了分段加锁的方式，有一个默认值为16并发级别的参数，即将entry数组分为16段，每个段中又有16个链表，
4.Java8中ConcurrentHashMap中废弃了分段和锁的机制，同时也使用entry数组+链表+红黑树的数据结构，而是使用cas算法，用无锁的方式来实现线程安全减少锁带来的开销。
5.Java7中方法区是堆中永久区的一个特殊区域，几乎不会被回收，Java8中废弃了方法区而是叫做metaspace，这个空间使用物理内存，不受jvm进程的内存分配限制，
	有了足够的空间就不会因为类过多而引发gc，从而提高了性能。
/********************************************/
1.症状：使用Mybatis“Invalid bound statement (not found):com.beauxie.wxj.dao.UserMapper.findUserByCondition ”，
	说明这个异常是在调用Mapper接口时发生的，看到这个异常时，首选想到的是配置可能出了问题，或者没有加注解，
	但反复检查以后，发现配置啥的都没问题，其中xml配置文件中，关于DAO层与Mapper文件的映射配置如下：
2.病因：对于MAVEN项目，Eclipse会自动把项目src\main\java\目录下的配置文件（.xml）和资源文件(.properties)搬运到target目录下，而intellij idea默认是不会帮我们做这件事的。
3.解法：在项目的pom.xml中添加如下配置
	<build>
		<!--解决Intellij构建项目时，target/classes目录下不存在mapper.xml文件-->
		<resources>
			<resource>
				<directory>${basedir}/src/main/java</directory>
				<includes>
					<include>**/*.xml</include>
				</includes>
			</resource>
		</resources>
	</build>
4.补充：这段代码的意思就是把src/main/java目录下所有的xml文件都包含进去，其中${basedir} 是MAVEN的内置变量，表示项目根目录。
	同样，想包含其他什么文件，比如.properties文件，再加一个<include>标签类似的写法即可。

Lombok
1.它会通过在编译阶段修改其结构来转换抽象语法树（AST）。
	AST 代表已解析源代码的树，它由编译器创建，与 XML 文件的 DOM 树模型类似。
	通过修改（或转换）AST，Lombok 可对源代码进行修剪，来避免膨胀，这与纯文本代码生成不同。
	Lombok 所生成的代码对于同一编译单元的类是可见的，这不同于带库的直接字符编码操作，比如 CGLib 或者 ASM。
2.对于想要了解内部情况的开发人员，Lombok delombok 工具将为您提供指导，可通过 Maven 或者 Ant 命令行访问。
	Delombok 获取通过 Lombok 转换的代码，并依据它来生成普通的 Java 源文件。
	“已被 delombok 处理过” 的代码将会包含之前由 Lombok 所完成的转换，格式为普通文本。
	例如，如果将 delombok 应用到 图 1 的代码中，您将能够看到，equals、hashCode、以及 toString 已被实施。
---------------------------------------------------------------------------------------------------------------
Java集合排序
return children.stream().sorted(Comparator.comparingInt(TreeEntity::getOrders)).collect(Collectors.toList());
children.sort(Comparator.comparing(TreeEntity::getOrders));
Collections.reverse(children);
---------------------------------------------------------------------------------------------------------------
在内网环境下，如果设置了私服Nexus仍然不能下载依赖，可以设置一个镜像
原profile
<profiles>
	<profile>
		<id>SunriseProfile</id>
		<repositories>
			<repository>
				<id>sunrise-nexus</id>
					<url>http://192.168.50.20:8081/repository/sp811Repository/</url>
				<releases>
				<enabled>true</enabled>
				</releases>
				<snapshots>
					<enabled>true</enabled>
				</snapshots>
				<!--<snapshotPolicy>always</snapshotPolicy>-->
			</repository>
		</repositories>
	</profile>
</profiles>

<mirrors>
    <!--<mirror>
			<id>nexus-aliyun</id>
			<mirrorOf>central</mirrorOf>
			<name>Nexus aliyun</name>
			<url>http://maven.aliyun.com/nexus/content/groups/public/</url>
	</mirror>-->
	镜像
    <mirror>  
        <id>central</id>
        <mirrorOf>*</mirrorOf>
        <name>Central Repository</name>
        <url>http://192.168.50.20:8081/repository/sp811Repository/</url>
    </mirror>  
</mirrors>
---------------------------------------------------------------------------------------------------------------
获取文件的元数据
public static String getContentType(String filename){  
String type = null;  
Path path = Paths.get(filename);  
try {  
	type = Files.probeContentType(path);  
} catch (IOException e) {
	//
}  
---------------------------------------------------------------------------------------------------------------
组装树结构
import java.util.*;

/**
 * Created by:YangJx
 *
 * @Description:
 * @Date:2018/5/9 16:53
 */
public class TreeBuildTest {

    public static void main(String[] args) {

        List<TreeNode> treeNodeList = new ArrayList<>();
        treeNodeList.add(new TreeNode("0", null, null));
        treeNodeList.add(new TreeNode("1", "0", null));
        treeNodeList.add(new TreeNode("2", "0", null));
        treeNodeList.add(new TreeNode("3", "3", null));
        treeNodeList.add(new TreeNode("4", "2", null));
        treeNodeList.add(new TreeNode("5", "1", null));
        treeNodeList.add(new TreeNode("6", "2", null));
        treeNodeList.add(new TreeNode("7", "3", null));

        treeNodeList.add(new TreeNode("8", "88", null));
        treeNodeList.add(new TreeNode("9", "8", null));

        treeNodeList.sort(Comparator.comparing(TreeNode::getId));

        Map<String, TreeNode> allTreeMap = getStructurizedTree(treeNodeList);
        System.out.println(allTreeMap.size());
    }

    public static Map<String, TreeNode> getStructurizedTree(List<TreeNode> treeNodeList) {

        //1.抽出所有点
        Map<String, TreeNode> allTreeNodeMap = new HashMap<>();
        for (TreeNode it : treeNodeList) {
            if (it.getId().equals(it.getParentId())) {
                System.out.println("数据错误：id和parentId相同！");
            }
            allTreeNodeMap.put(it.getId(), it);
        }

        //2.抽出所有父节点，并为父节点装入子节点
        Map<String, TreeNode> parentTreeNodeMap = new HashMap<>();
        for (TreeNode it : treeNodeList) {
            TreeNode parent = allTreeNodeMap.get(it.getParentId());
            if (parent != null) {
                String parentId = parent.getId();
                if (parent.getChildrenList() == null) {
                    parent.setChildrenList(new ArrayList<>());
                }
                if (!it.getId().equals(parentId)) {
                    parent.getChildrenList().add(it);
                }

                //只有独立节点才能作为根，才能被最终展示出来【包括父节点不在集合中、父节点id为空、父节点是本身三种情况】
                Set<String> allTreeNodeKeySet = allTreeNodeMap.keySet();
                String parentParentKey = parent.getParentId();
                if (parentParentKey == null || !allTreeNodeKeySet.contains(parentParentKey) || parentId.equals(parentParentKey)) {
                    parentTreeNodeMap.put(it.getParentId(), parent);
                }
            } else {
                continue;
            }
        }
        return parentTreeNodeMap;
    }

}

class TreeNode {

    private String id;

    private String parentId;

    private List<TreeNode> childrenList;

    public TreeNode(String id, String parentId, List<TreeNode> childrenList) {
        this.id = id;
        this.parentId = parentId;
        this.childrenList = childrenList;
    }
    public String getId() {
        return id;
    }
    public void setId(String id) {
        this.id = id;
    }
    public String getParentId() {
        return parentId;
    }
    public void setParentId(String parentId) {
        this.parentId = parentId;
    }
    public List<TreeNode> getChildrenList() {
        return childrenList;
    }
    public void setChildrenList(List<TreeNode> childrenList) {
        this.childrenList = childrenList;
    }
}
---------------------------------------------------------------------------------------------------------------
<!--解决Intellij Idea构建项目时，target/classes目录下不存在mapper.xml文件的问题-->
<build>
	<resources>
		<resource>
			<directory>${basedir}/src/main/java</directory>
			<includes>
				<include>**/*.xml</include>
			</includes>
		</resource>
	</resources>
</build>
---------------------------------------------------------------------------------------------------------------
Spring-Boot中使用自定义filter
一、在spring的应用中我们存在两种过滤的用法，一种是拦截器、另外一种当然是过滤器。这里介绍过滤器在springboot的用法，在springmvc中的用法基本上一样，只是配置上面有点区别。

二、filter功能，它使用户可以改变一个 request和修改一个response. Filter 不是一个servlet,它不能产生一个response,它能够在一个request到达servlet之前预处理request,也可以在离开 servlet时处理response.换种说法,filter其实是一个”servlet chaining”(servlet 链).
	一个Filter包括：
		1）、在servlet被调用之前截获;
		2）、在servlet被调用之前检查servlet request;
		3）、根据需要修改request头和request数据;
		4）、根据需要修改response头和response数据;
		5）、在servlet被调用之后截获.

三、应用
@Component
@ServletComponentScan2  //当遇到组件扫描不到的问题时可以使用这个注解并配合其属性来解决
@WebFilter(urlPatterns = "/login/*",filterName = "loginFilter")
public class LoginFilter implements Filter{

    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
    }

    @Override
    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException {
    }

    @Override
    public void destroy() {
    }
}

四、解释：
1、@Component 这个注解的目的是将LoginFilter交给容器来处理。也就是让LoginFilter起作用
2、@ServletComponentScan 这个使用来扫描@WebFilter 的让@WebFilter起作用。当然对于servlet相关注解也是可以的。这个@ServletComponentScan最好写在项目入口类Apllication.java这个上面，通用配置。我这里因为只有一个Filter所以没有写在Application上面。
3、@WebFilter 这个用处显而易见，针对于什么链接做过滤，filter的名称是为什么。

五、简单介绍一下springmvc中的Filter的用法
1、写的方法还是一样的都是继承Filter，来实现3个方法处理
2、丢入容器：这个需要配置在web.xml里面
    <filter>
        <filter-name>loginFilter</filter-name>
        <filter-class>com.troy.boot.filter.LoginFilter</filter-class>
    </filter>
    <filter-mapping>
        <filter-name>loginFilter</filter-name>
        <url-pattern>/*</url-pattern>
    </filter-mapping>
3、具体的用法可以自己研究。
---------------------------------------------------------------------------------------------------------------
Spring-Boot中interceptor(拦截器)的应用
一、SpringMVC 中的Interceptor 拦截器也是相当重要和相当有用的，它的主要作用是拦截用户的请求并进行相应的处理。在web开发中，拦截器是经常用到的功能。它可以帮我们验证是否登陆、预先设置数据以及统计方法的执行效率等等。spring中拦截器主要分两种，一个是HandlerInterceptor，一个是MethodInterceptor。这里我介绍HandlerInterceptor的一些用法

二、继承HandlerInterceptor接口
@Component
public class LoginInterceptor implements HandlerInterceptor {
    @Override
    public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception {
        return false;
    }

    @Override
    public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception {

    }

    @Override
    public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception {

    }
}
@Component 目的是加入容器管理

三、配置、在springboot中需要配置相关请求处理，来实现放行的目的
@Configuration
public class LoginConfigurtion extends WebMvcConfigurerAdapter{

    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(new LoginInterceptor())
                .addPathPatterns("/**")
                .excludePathPatterns("/user/login");
    }
}
这样可以进行相关的内容处理，在使用上面也可以根据自己的需求来实现。
preHandle方法的应用也很方便，通过就返回true。
/********************************************/
 /**
     * Java8版本的wordCount程序
     */
    @Test
    public void test() {
        Map<String, LongAdder> tempMap = new HashMap<>();
        Arrays.stream("1,2,3,4".split(",")).forEach(System.err::println);
        Stream.of("11,21,3", "4,5,6,11").flatMap(it -> Stream.of(it.split(","))).map(it -> {
            if (!tempMap.keySet().contains(it)) {
                LongAdder longAdder = new LongAdder();
                longAdder.add(1);
                tempMap.put(it, longAdder);
            } else {
                tempMap.get(it).increment();
            }
            return null;
        });

        Iterator<Map.Entry<String, LongAdder>> iterator = tempMap.entrySet().iterator();
        while (iterator.hasNext()) {
            Map.Entry<String, LongAdder> entry = iterator.next();
            System.out.println(entry.getKey() + "=====>" + entry.getValue().sum());
        }
    }

    /**
     * reduce测试
     */
    @Test
    public void test0() {
        IntStream.of(1,2,3,4).reduce((a, b) -> a + b).ifPresent(System.out::println);
        Arrays.asList("11", "22", "33").parallelStream().count();
        System.out.println(11);
    }

    /**
     * try-with-resouce语法测试
     * @throws IOException
     */
    @Test
    public void test1() throws IOException {
        try(FileInputStream input = new FileInputStream("E:\\test\\files0\\file_server_adaptor-master\\pom.xml")) {
            int data = input.read();
            while(data != -1){
                System.out.print((char) data);
                data = input.read();
            }
        }
    }
/********************************************/
Eureka是客户端服务发现机制，服务列表的获取，过滤，规则的执行，这些操作都是在客户端通过Ribbon。
/********************************************/
Ribbon功能：
服务发现：依据服务名称将所有服务实例找出来。
服务选择：根据一定规则策略从多个实例中挑选一个提供服务。
服务监听：检测失效服务，并快速剔除。

主要构件：
	ServerList：获取所有可用服务列表
	IServerListFilter：过滤掉一部分
	IRule：通过规则策略选择一个
/********************************************/
服务间通信：
	RestTemplate组件：内部是使用Ribbon作为负载均衡实现的。
	Feign组件：内部也是使用Ribbon作为负载均衡实现的。
/********************************************/
Feign
/********************************************/
Java 开发中，需要将一些易变的配置参数放置再 XML 配置文件或者 properties 配置文件中。
然而 XML 配置文件需要通过 DOM 或 SAX 方式解析，而读取 properties 配置文件就比较容易。
介绍几种读取方式：
1、基于ClassLoder读取配置文件
注意：该方式只能读取类路径下的配置文件，有局限但是如果配置文件在类路径下比较方便。
	Properties properties = new Properties();
	// 使用ClassLoader加载properties配置文件生成对应的输入流
	InputStream in = PropertiesMain.class.getClassLoader().getResourceAsStream("config/config.properties");
	// 使用properties对象加载输入流
	properties.load(in);
	//获取key对应的value值
	properties.getProperty(String key);

2、基于 InputStream 读取配置文件
	注意：该方式的优点在于可以读取任意路径下的配置文件
	Properties properties = new Properties();
	// 使用InPutStream流读取properties文件
	BufferedReader bufferedReader = new BufferedReader(new FileReader("E:/config.properties"));
	properties.load(bufferedReader);
	// 获取key对应的value值
	properties.getProperty(String key);
3、通过 java.util.ResourceBundle 类来读取，这种方式比使用 Properties 要方便一些
	通过 ResourceBundle.getBundle() 静态方法来获取（ResourceBundle是一个抽象类），这种方式来获取properties属性文件不需要加.properties后缀名，只需要文件名即可
	properties.getProperty(String key);
	//config为属性文件名，放在包com.test.config下，如果是放在src下，直接用config即可  
	ResourceBundle resource = ResourceBundle.getBundle("com/test/config/config");
	String key = resource.getString("keyWord"); 
	从 InputStream 中读取，获取 InputStream 的方法和上面一样，不再赘述
	ResourceBundle resource = new PropertyResourceBundle(inStream);
	注意：在使用中遇到的最大的问题可能是配置文件的路径问题，如果配置文件入在当前类所在的包下，那么需要使用包名限定，如：config.properties入在com.test.config包下，则要使用com/test/config/config.properties（通过Properties来获取）或com/test/config/config（通过ResourceBundle来获取）；属性文件在src根目录下，则直接使用config.properties或config即可。
ResourceBundle resource = ResourceBundle.getBundle("config");
String serverName = resource.getString("server.name");
System.out.printf("this is my serverName: %s", serverName);
/********************************************/
---------------------------------------------------------------------------------------------------------------
UPDATE t_yang_test t
SET t.parent_id = t.parent_id + 1
WHERE
	t.id = 1;
---------------------------------------------------------------------------------------------------------------
可以满足根节点不在列表中的需求
delimiter $$
DROP FUNCTION IF EXISTS getChildList;
CREATE FUNCTION `getChildList`(rootId varchar(100))   
RETURNS varchar(2000)  
BEGIN   
DECLARE str varchar(2000);
DECLARE cid varchar(100);   
SET str = '$';   
SET cid = rootId;   
WHILE cid is not null DO   
    SET str = concat(str, ',', cid);   
    SELECT group_concat(id) INTO cid FROM t_yang_test where FIND_IN_SET(parent_id, cid) > 0;   
END WHILE;
RETURN str;
END $$
delimiter ;

SELECT getChildList(115) FROM DUAL;
---------------------------------------------------------------------------------------------------------------
delimiter $$
DROP FUNCTION IF EXISTS getChildListWithoutRoot;
CREATE FUNCTION `getChildListWithoutRoot`(rootId varchar(100))   
RETURNS varchar(2000)  
BEGIN   
DECLARE str varchar(2000);
DECLARE resul varchar(2000);
DECLARE cid varchar(100);   
SET str = '$';   
SET cid = rootId;   
WHILE cid is not null DO   
    SET str = concat(str, ',', cid);   
    SELECT group_concat(id) INTO cid FROM t_yang_test where FIND_IN_SET(parent_id, cid) > 0;   
END WHILE;
SELECT SUBSTRING_INDEX(str, ",", -2) INTO resul FROM DUAL;  //去掉前两个元素（$和rootId）
RETURN resul;
END $$
delimiter ;

SELECT getChildListWithoutRoot(115) FROM DUAL;  //函数调用
---------------------------------------------------------------------------------------------------------------
SELECT SUBSTRING_INDEX("1,2,3,4", ",", -2) FROM DUAL;  //按照分隔符及其索引切分字符串，如果索引时正数表示从左往右数，负数择表示从右往左数
---------------------------------------------------------------------------------------------------------------
Axure RP 8 注册码
仅供个人学习交流使用（建议购买正版授权）
8.1.0.3366亲测可用
Licensee：University of Science and Technology of China (CLASSROOM)
Key：DTXRAnPn1P65Rt0xB4eTQ+4bF5IUF0gu0X9XBEUhM4QxY0DRFJxYEmgh4nyh7RtL
---------------------------------------------------------------------------------------------------------------
/*
 * 这是基于注解的映射方式，实现对数据的增删改查，将sql语句直接写在注解的括号中
 * 这是一个接口，其不需要类去实现它
 * 下边分别是插入，删除，修改，查询一个记录，查询所有的记录
 * */
public interface UserMapper {
	@Insert("insert into users(name,age) values(#{name},#{age})")
	public void insertT(User user);

	@Delete("delete from users where id=#{id}")
	public void deleteById(int id);

	@Update("update users set name=#{name},age=#{age} where id=#{id}")
	public void updateT(User user);

	@Select("select * from users where id=#{id}")
	public User getUser(int id);

	@Select("select * from users")
	public List<User> getAllUsers();
}
---------------------------------------------------------------------------------------------------------------
INSERT INTO t_yang_test VALUES (12, 1, "高新区", 2) ON DUPLICATE KEY UPDATE parent_id = 1, `name` = '高新区', type = 4;  //如果插入记录主键重复择直接更新
---------------------------------------------------------------------------------------------------------------
条件插入，单条记录
INSERT INTO t_yang_test (parent_id, `name`, type) SELECT 2, "nihao", 10
FROM
	DUAL
WHERE
	NOT EXISTS (
		SELECT
			id
		FROM
			t_yang_test
		WHERE
			parent_id = 2
		AND type = 10
	);
---------------------------------------------------------------------------------------------------------------
条件插入，多条记录
INSERT INTO clients   
(client_id, client_name, client_type)   
SELECT supplier_id, supplier_name, 'advertising'   
FROM suppliers   
WHERE not exists (select * from clients   
where clients.client_id = suppliers.supplier_id); 
---------------------------------------------------------------------------------------------------------------
/********************************************/
在MySql中如何查询每个用户的基本信息，同时还要查询这个用户拥有的角色个数
SELECT
	t.id AS `id`,
	t. NAME AS `name`,
	t.age AS `age`,
	(
		SELECT
			COUNT(*)
		FROM
			t_user u
		INNER JOIN t_user_role ur ON u.id = ur.user_id
		INNER JOIN t_role r ON ur.role_id = r.id
		WHERE
			u.id = t.id
	) AS `roleCount`
FROM
	t_user t
WHERE
	t.age = 30
OR t.age = 27;
/********************************************/
-- 在一个查询中比较两个子查询的结果的大小，返回值0：false，1：true。
SELECT
	foo.count <> bar.count AS `result`
FROM
	(
		SELECT
			COUNT(t.role_id) AS `count`
		FROM
			t_user_role t
		WHERE
			t.user_id IN (1, 2)
	) AS `foo`,
	(
		SELECT
			COUNT(DISTINCT t.role_id) AS `count`
		FROM
			t_user_role t
		WHERE
			t.user_id IN (1, 2)
	) AS `bar`;
/********************************************/
1.引入Spring-Security依赖
2.编写配置类
	SpringSecurityConfig extends WebSecurityConfigurerAdapter
/********************************************/
1.reactiveProgram：异步非阻塞
	1.异步数据流：
		1.具有集合的功能，看起来像Java的StreamAPI
		2.单向流动
		3.时间+序列
	2.数据流操作：		
/********************************************/
一、安装Docker
	1.yum list | grep docker  //由于yum仓库里的docker可能不是最新版的，所以不建议用yum安装，推荐使用curl执行远程脚本的方式从docker官网安装
	2.curl -fsSL https://get.docker.com/ | sh
	3.测试是否安装成功：docker version会显示各种版本信息
二、配置镜像加速
	鉴于国内网络问题，后续拉取Docker镜像十分缓慢，我们可以需要配置加速器来解决，我使用的是网易的镜像地址：http://hub-mirror.c.163.com。
	新版的Docker使用json配置文件/etc/docker/daemon.json（Linux）
	请在该配置文件中加入（没有该文件的话，请先建一个）：
	{
		"registry-mirrors": ["http://hub-mirror.c.163.com"]
	}
三、执行用户组
	创建用户组：groupadd docker
	将用户添加到组中：usermod -aG docker youn
四、启动服务
	1.systemctl start docker或者service docker start  //启动命令
五、Docker版Hello World测试
	1.拉取hello-world镜像：docker pull hello-world
	2.查看服务中的镜像列表：docker images
		docker history repository:tag  //查看镜像的变更历史，包括构建层级
	3.查看服务器中运行的容器：docker ps  //添加-l选项可以查看最近被创建的容器
	4.运行容器
		docker run hello-world  //如果打印出Hello from Docker!表示运行正常
		1.交互式：docker run -it centos:latest  //选项i表示启动一个交互式容器，用户退出容器后容器随即销毁，可以用exit或CTRL+D退出容器。
				  t选项表示启动容器后打开一个命令行terminal，可以在这个terminal执行容器内命令
		2.后台式：docker run -d centos:latest  //选项d表示启动一个后台式容器，用户不会登进容器，保持后台运行。
	5.根据容器唯一ID查看容器的运行日志：docker logs -f 2263e10d958b  //选项f表示实时显示，类似tail -400f命令
	6.根据容器唯一ID停止容器：docker stop 2263e10d958b
六、载入web应用：docker pull training/webapp  //拉取示例web应用镜像
				 docker run -d -P training/webapp python app.py  //开启容器，-P:将容器内部使用的网络端口映射宿主机的随机端口上，flask的是5000，会将主机的32768端口映射上去。
				 docker run -d -p 8888:500 training/webapp python app.py  //开启容器，-p:将容器内部使用的网络端口映射宿主机指定端口上，flask的是5000，会将主机的8888端口映射上去。
				 curl localhost:32768  //测试web应用，返回hello world表示正常
				 
				 docker run --name=containerName centos:imageName  //自定义名称启动容器
				 docker run --name "containerName" centos:imageName  //自定义名称启动容器
七、查看容器端口：
	1.docker ps  //查看运行中的容器
	2.docker ps -a  //查看所有容器，包括启动而未运行的
	2.docker port containerName  //可以跟容器名称或唯一ID作为参数
八、查看容器中的进程列表：
	docker top containerName  //也可以是唯一ID作为参数
九、查看容器度量参数：
	docker inspect 25b59eb18121  //以JSON格式返回容器的一系列参数
十、停/启一个容器：
	docker stop 25b59eb18121  //停止容器
	docker start 25b59eb18121  //容器停止后，还可以用上次的ID或者名称重启容器（并非新建容器）
	docker cp containerId:containerPath hostPath  //将容器内路径下的数据拷贝到宿主机目录下
十一、查看docker服务器整体信息：
	docker info
十二、根据镜像创建自定义名字的镜像：
	docker create --name="yang-centos" centos:latest
十三、删除已停止容器
	docker rm dreamy_roentgen
十四、在DockersHub中查找镜像
	docker search coreos
十五、更新镜像【更新容器内的yum库或者apt库】
	1.docker run -it centos:latest  //用旧镜像启动一个容器
	2.yum update  //在容器内执行yum库更新操作
	3.docker commit -m="yang's first entos image" -a="yang" 2ab20c209cd2 centos:centos-v7-yang  //基于已有容器，提交一个新的镜像到本地仓库
十六、删除镜像
	docker rmi centos:myCentos7
十八、启动和进入容器最佳实践：
	1.docker run -it --name myCentos centos:latest  //首先启动容器，并以交互是命令行形式进入容器
	2.待内部操作完成后使用CTRL+P+Q组合键静默退出，这样退出后容器任然会运行，不会停止
		还有一种不推荐的退出方式，直接用exit命令退出，此时容器会停止运行，然后再用restart myCentos命令重启容器
	3.docker attach myCentos  //再次进入容器命令行界面进行其他操作
	4.docker exec -it myCentos bash
十九、
	1.编写DockerFile构建文件
		FROM scratch  //所有镜像的基镜像，类似基类
		ADD centos-7-docker.tar.xz /

		LABEL org.label-schema.schema-version = "1.0" \
			org.label-schema.name="CentOS Base Image" \
			org.label-schema.vendor="CentOS" \
			org.label-schema.license="GPLv2" \
			org.label-schema.build-date="20180531"

		CMD ["/bin/bash"]  //启动容器后容器内默认执行的命令
	2.执行docker build -f fileName -t repsitoryName:tagName . 命令构建一个docker镜像
	  docker build -f DockerFile0 -t centos:ipcn .  //注意句点
	3.执行docker run命令用镜像启动一个容器
二十、数据卷
	1.概念：宿主机和容器间、容器与容器间共享存储介质，实现数据的持久化和数据共享。
	2.数据卷中的数据可以在容器间共享和重用。
	3.数据卷中数据的更改可以实时生效。
	4.数据卷中的数据不会包含到镜像中去。
	5.数据卷的生命周期会持续到没有容器引用它为止。
	
	6.为容器添加数据卷：
		1.容器内添加
		2.外部命令
			docker run -it imageName -v hostPath:containerPath:ro imageName /bin/bash  //将宿主机的指定目录映射到容器中的指定目录，两个目录任意一个不存在都会自动创建
			:or  //表示容器内只读，不可写
			docker inspect containerName  //可以查看容器数据卷映射情况即读写权限
			映射成功后，不论容器是否在运行，在任意一方对数据卷进行写操作都会在两方生效
		3.DockerFile
			VOLUME ["containerPath1", "containerPath2"]  //生成的镜像，会在容器启动时中生成多个目录
			可以用docker inspect命令查看容器内目录映射到了宿主机的默认位置
	7.数据卷容器：如果某个容器用来被继承，使其数据卷传递到子容器的容器就称为数据卷容器。
		docker run -it --name dataContainerParent centos:v7 /bin/bash  //启动一个父容器作为数据容器
		docker run -it --name son1 --volums-from dataContainerParent centos:v7 /bin/bash  //启动第一个自容器，会原封继承父容器的数据卷
		docker run -it --name son1 --volums-from dataContainerParent centos:v7 /bin/bash  //启动第一个自容器，会原封继承父容器的数据卷
		删除父容器，子容器也能访问数据卷，直到最后一个容器。
/********************************************/
DockerFile的构建过程
一、基础知识
	1.保留关键字指令必须为大写字母，且其后至少跟一个参数
	2.指令按照从上到下顺序执行
	3."#"用于注释
	4.每条指令都会创建一个新的镜像曾，并自动对镜像进行提交
二、DockerFile执行流程
	1.docker从基础镜像运行一个容器
	2.执行一条指令并对容器做出修改
	3.执行类似docker commit的操作提交一个新镜像层
	4.docker再基于刚提交的镜像运行一个新容器
	5.执行dockerfile的下一条指令，直到执行完所有指令
	6.有的centos基础镜像没有安装网络服务需要用yum安装
		yum -y install net-tools
三、保留关键字
	FROM  #基础镜像，可以是祖先镜像，也可以是其他自定义镜像
	MAINTAINER  #作者
	RUN  #镜像构建时需要运行的额外命令
	EXPOSE  #启动后容器对外暴露的端口号
	WORKDIR  #用户登录容器后的初始工作目录
	ENV  #在构建过程中设置环境变量，跟key-value型参数，可在其他地方应用
	COPY  #将构建上下文目录中（即当前执行build命令的宿主机目录）的文件或目录复制到新的一层镜像（目的路径）位置
	ADD  #包含COPY的功能，同时还会自动对url和压缩文件进行访问和解压处理
	VOLUME  #用于数据保存和持久化工作
	CMD  #指定容器启动时要执行的命令，一个dockerFile中可以有多个CMD命令，但容器启动时只有最后一个会生效，且当用docker run启动容器时会被其后跟有命令时，这些内置的命令都不会生效，会被覆盖
	ENTEYPOINT  #与cmd命令功能相同，但其后命令不会被docker run命令后所跟的命令覆盖，而是以追加的方式拼接到之前的命令后面
	ONBUILD  #指定一个被动触发指令，当此镜像被某子镜像继承（构建子镜像时），本命令就会被触发执行
/********************************************/
单点登录CAS
1.用户访问业务系统，业务系统中的CAS-Client检查请求中是否包含token，如果没有就带着业务系统的回路url将请求重定向到CAS-Server。
2.用户被引到CAS-Server的登录页面，填写用户名和密码进行认证登录，如果登录成功首先在CAS-Server侧初始化该用户的全局session，
	同时会返给用户浏览器Ticket-granting-cookie（TGC），其中包含认证token。
3.CAS-Client根据回路url将页面再次重定向回到业务系统，用户再次访问业务数据，这次带上了认证token，此token被业务系统中的CAS-Client发往CAS-Server进行验证，
	如果验证通过，业务系统会在本系统内初始化该用户的局部session。
/********************************************/
用JDK内置的SSL证书生成工具keytool制作自签名证书
1.生成服务器端证书
	keytool 
	-genkey 
	-alias tomcat(证书别名) 
	-keypass 123456(别名密码) 
	-keyalg RSA(算法) 
	-keysize 1024(密钥长度) 
	-validity 365(有效期，天单位) 
	-keystore D:/keys/tomcat.keystore(指定生成证书的位置和证书名称，一定要先建好目录) 
	-storepass 123456(获取keystore信息的密码)
	
	keytool -genkey -alias tomcat -keypass 123456 -keyalg RSA -keysize 1024 -validity 365 -keystore D:/keys/tomcat.keystore -storepass 123456
	命令执行过车中一路回车，IP也不用填写

2.生成浏览器端证书
	keytool
	-genkey
	-alias client
	-keypass 123456
	-keyalg RSA
	-storetype PKCS12
	-keypass 123456
	-storepass 123456
	-keystore D:/keys/client.p12
	
	keytool -genkey -alias client1 -keypass 123456 -keyalg RSA -keysize 1024 -validity 365 -storetype PKCS12 -keystore D:/keys/client1.p12 -storepass 123456
	命令执行过车中一路回车

3.让服务器信任客户端证书
	a.由于不能直接将PKCS12格式的证书库导入，必须先把客户端证书导出为一个单独的CER文件，使用如下命令：
		keytool -export -alias client -keystore D:/keys/client.p12 -storetype PKCS12 -keypass 123456 -file D:/keys/client.cer
		注意：Keypass：指定CER文件的密码，但会被忽略，而要求重新输入
	b.将该文件导入到服务器的证书库，添加为一个信任证书：
		keytool -import -v -file D:/keys/client.cer -keystore D:/keys/tomcat.keystore -storepass 123456
	c.完成之后通过list命令查看服务器的证书库，可以看到两个证书，一个是服务器证书，一个是受信任的客户端证书：
		keytool -list -v -keystore D:/keys/tomcat.keystore

4.让客户端信任服务器证书
	a.由于是双向SSL认证，客户端也要验证服务器证书，因此，必须把服务器证书添加到浏览器的“受信任的根证书颁发机构”。
		由于不能直接将keystore格式的证书库导入，必须先把服务器证书导出为一个单独的CER文件，使用如下命令：
		keytool -keystore D:/keys/tomcat.keystore -export -alias tomcat6 -file D:/keys/server.cer
	b.双击server.cer文件，按照提示安装证书，将证书填入到“受信任的根证书颁发机构”。
		填入方法：打开浏览器 -工具 - internet选项-内容- 证书-把中级证书颁发机构里的www.localhost.com
		(该名称即时你前面生成证书时填写的名字与姓氏)证书导出来-再把导出来的证书导入  受信任的根颁发机构  就OK了。

5.配置Tomcat服务器
	<Connector  port="8443" protocol="org.apache.coyote.http11.Http11NioProtocol" SSLEnabled="true"
	maxThreads="150"
	scheme="https"
	secure="true"
	clientAuth="true"
	sslProtocol="TLS"
	keystoreFile="D:/keys/tomcat.keystore"
	keystorePass="123456"
	truststoreFile="D:/keys/tomcat.keystore"
	truststorePass="123456" />
	 
	属性说明：
		clientAuth:设置是否双向验证，默认为false，设置为true代表双向验证
		keystoreFile:服务器证书文件路径
		keystorePass:服务器证书密码
		truststoreFile:用来验证客户端证书的根证书，此例中就是服务器证书
		truststorePass:根证书密码

	注意：
		①设置clientAuth属性为True时，需要手动导入客户端证书才能访问。
		②要访问https请求 需要访问8443端口，访问http请求则访问Tomcat默认端口（你自己设置的端口，默认8080）即可。

6.总结：
	经过以上五步，你使用HTTPS 端口为8443 进行访问的时候 就是经过SSL信息加密，不怕被截获了。
	通话的双方，必须是都拥有证书的端，才能进行会话，换句话说，就是只有安装了咱证书的客户端，才能与服务器通信。

7.小贴士
	强制 https 访问
	在 tomcat /conf/web.xml 中的 </welcome- file-list> 后面加上这
	<login-config>    
	<!-- Authorization setting for SSL -->    
	<auth-method>CLIENT-CERT</auth-method>    
	<realm-name>Client Cert Users-only Area</realm-name>    
	</login-config>    
	<security-constraint>    
	<!-- Authorization setting for SSL -->    
	<web-resource-collection >    
	<web-resource-name >SSL</web-resource-name>    
	<url-pattern>/*</url-pattern>    
	</web-resource-collection>    
	<user-data-constraint>    
	<transport-guarantee>CONFIDENTIAL</transport-guarantee>    
	</user-data-constraint>    
	</security-constraint> 
	完成以上步骤后，在浏览器中输入http的访问地址也会自动转换为https了。

附录1:
	keytool常用命令 
	-alias       产生别名 
	-keystore    指定密钥库的名称(就像数据库一样的证书库，可以有很多个证书，cacerts这个文件是jre自带的， 
				 你也可以使用其它文件名字，如果没有这个文件名字，它会创建这样一个) 
	-storepass   指定密钥库的密码 
	-keypass     指定别名条目的密码 
	-list        显示密钥库中的证书信息 
	-v           显示密钥库中的证书详细信息 
	-export      将别名指定的证书导出到文件 
	-file        参数指定导出到文件的文件名 
	-delete      删除密钥库中某条目 
	-import      将已签名数字证书导入密钥库 
	-keypasswd   修改密钥库中指定条目口令 
	-dname       指定证书拥有者信息 
	-keyalg      指定密钥的算法 
	-validity    指定创建的证书有效期多少天 
	-keysize     指定密钥长度 
 
	使用说明：
		导入一个证书命令可以如下： 
		keytool -import -keystore cacerts -storepass 666666 -keypass 888888 -alias alibabacert -file C:\alibabajava\cert\test_root.cer 
		其中-keystore cacerts中的cacerts是jre中默认的证书库名字，也可以使用其它名字 
		-storepass 666666中的666666是这个证书库的密码 
		-keypass 888888中的888888是这个特定证书的密码 
		-alias alibabacert中的alibabacert是你导入证书的别名，在其它操作命令中就可以使用它 
		-file C:\alibabajava\cert\test_root.cer中的文件路径就是要导入证书的路径 

		浏览证书库里面的证书信息，可以使用如下命令： 
		keytool -list -v -alias alibabacert -keystore cacerts -storepass 666666 

		要删除证书库里面的某个证书，可以使用如下命令： 
		keytool -delete -alias alibabacert -keystore cacerts -storepass 666666 

		要导出证书库里面的某个证书，可以使用如下命令： 
		keytool -export -keystore cacerts -storepass 666666 -alias alibabacert -file F:\alibabacert_root.cer 

		要修改某个证书的密码（注意：有些数字认证没有私有密码，只有公匙，这种情况此命令无效）

		这个是交互式的，在输入命令后，会要求你输入密码 
		keytool -keypasswd -alias alibabacert -keystore cacerts 

		这个不是交互式的，输入命令后直接更改
		Keytool -keypasswd -alias alibabacert -keypass 888888 -new 123456 -storepass 666666 -keystore cacerts
/********************************************/
Java之Fork-Join机制
	1.将大任务依据阈值分割为若干互不依赖的小任务
	2.根据cpu核心数等资源情况创建线程，并为每个线程构建一个对应的双端队列，将小任务按队列数分组，每组放到一个双端队列中
	3.各个线程并行从自己的队列头部取出并执行小任务
	4.并行执行一段时间后，有的线程已经执行完自己队列的任务，有的还未完成
	5.先执行完任务的线程就从未执行完的线程之队列尾部【窃取】任务并帮其执行
	6.所有工作线程都见子任务的执行结果统一放到一个结果队列中
	7.开启一个汇总线程对结果队列中的小结果进行汇总

	优点：可以并行的去处理大任务，能提高处理效率，任务分割并分配到特定线程的机制能有效地减少线程间竞争
	缺点：不能完全避免线程对同一队列的竞争，尤其是队列中只剩一个任务时的窃取行为
		 创建多个线程和队列会占中更多计算资源

	实现
		第一步：分割任务，首先我们需要有一个fork类来把大任务分割成子任务，有可能子任务还是很大，所以还需要不停的分割，直到分割出的子任务足够小。
		第二步：执行任务并合并结果，分割的子任务分别放在双端队列里，然后几个启动线程分别从双端队列里获取任务执行。
				子任务执行完的结果都统一放在一个队列里，启动一个线程从队列里拿数据，然后合并这些数据。

		Fork-Join使用两个类来完成以上两件事情：
		ForkJoinTask：我们要使用ForkJoin框架，必须首先创建一个ForkJoin任务。它提供在任务中执行fork()和join()操作的机制，
					  通常情况下我们不需要直接继承ForkJoinTask类，而只需要继承它的子类，Fork/Join框架提供了以下两个子类：
		RecursiveAction：用于没有返回结果的任务。
		RecursiveTask ：用于有返回结果的任务。
		ForkJoinPool ：ForkJoinTask需要通过ForkJoinPool来执行，任务分割出的子任务会添加到当前工作线程所维护的双端队列中，
					   进入队列的头部。当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务。
/********************************************/
1.在一个线程内如果遇到线程内部异常，并且没有捕获，本线程会立即终止。
2.在一个线程内如果遇到线程内部异常，做了捕获处理，本线程会继续执行。
3.外部线程（如main线程）是捕捉不到其子线程的内部异常的（由于run方法没有声明异常，即便在run方法中抛出了也捕获不到），所以子线程的内部异常要在其内部捕获并处理。
/********************************************/
1.Timer不支持多线程。全部挂在Timer下的任务都是单线程的，任务仅仅能串行运行。假设当中一个任务运行时间过长。
	会影响到其它任务的运行，然后就可能会有各种接踵而来的问题。
2.Timer的线程不会捕获异常。TimerTask假设抛出异常，那么Timer唯一的进程就会挂掉，这样挂在Timer下的全部任务都会无法继续运行。

第一个问题，随着业务数据的猛增，我们生产上有几个任务如今每次运行须要1-3个小时。在这段时间内，该timer下的其它任务仅仅能等待，
	这是让人无法忍受的。重开一个Timer？难道要为全部的耗时的Task都单开一个Timer。显然是不太可能。这样就太乱了。
第二个问题，是极其致命的，好多业务数据都是晚上的定时任务跑出来的。结果因为程序的问题或者内存资源不足，导致线程被kill了。
	该timer下的全部任务都未运行。结果第二天整整忙活了一天，主要任务就是——跑任务，调整数据。
/********************************************/
Java中符号引用和直接直接引用的区别
	符号引用：可以说是针对编译器和class文件而言的，符号引用使用类的全路径+字段名或方法名+相应描述符，来对字段或方法的精确定位。
	直接引用：可以说是针对jvm运行时而言的，是指字段或方法的内存地址，在运行时只能通过内存地址来进行定位寻址，所以类加载进内存后需要将符号引用转换为直接引用才可运行。
/********************************************/
线程池的shutdown和shutdownNow方法区别
	shutdown执行后池内正在执行的线程会继续执行直到结束，未执行的线程将不再执行。
	shutdownNow执行后池内正在执行的线程立即停止，未执行的也不再执行。
/********************************************/
LiuYudo
一、从学习方式上划分
	1、监督学习：训练数据即有特征又有标签、得分等结果，主要包括分类和回归。
	2、非监督学习：主要用于辅助监督学习。
		比如聚类分析，
		对数据降维：特征提取（去除无关特征），
					特征压缩与合并（PCA），尽量少的损失信息的前提下将高维特征压缩为低维特征，以降低处理复杂度，另一方面时方便可视化。
		异常检测：剔除离群值，因为离群值不利与算法发掘数据的普适性规律。
	3、半监督学习：针对部分数据有标签，部分没有标签的数据集，先用无监督手段处理然后再用监督学习方式处理数据。
	4、增强学习：模型根据环境的反馈逐渐调整内部参数来调整模型的过程。
二、
	1、在线学习：实时的将预测结果和正确结果返回给模型用来训练，一旦数据有误会导致模型失去准度，需要实时监测数据的异常。
	2、离线学习：在实验环境将模型训练好，然后部署到线上，优点时模型上线后就固定了，不用考虑生产数据的演变。
	问题：如何适应环境的变化
		方案：定时重新批量学习，但是存在运算量巨大的问题，不符合实时性要求比较高的场景。
三、
	1、参数学习：一旦学到了参数，训练集就没有用了。
	2、非参数学习：不对模型进行假设，但并不是说没有参数的参与。
/********************************************/
数据非常重要
目前仍是数据驱动的
	1.收集足够多的数据
	2.提高数据质量
	3.提高数据的代表性
	4.利用特征工程研究更重要的特征
奥卡姆的剃刀：简单的就是好的，即不要对模型有过多的假设。
没有免费的午餐：可以用严格的数学推导出任意两个性能的理论期望性能时相同的，但针对特定领域的特定问题不同算法表现有差异，
				脱离具体问题来比较算法的好坏时没有意义的，所以，在面临具体问题是尝试多种算法并进行对比实验是很有必要的。
/********************************************/

/********************************************/
修改缓存还是删除缓存：
	1.在多数形况下应该删除缓存，因为更新缓存前往往需要额外的多步计算或者查询，所以代价过高，但是在计算量非常小的情境下也是可以采用的。
	2.在并发写情况下可能造成缓存和DB的数据不一致【不论是先操作缓存还是DB都不能避免】，比如一个线程首先将缓存更新了，还没来得及更新DB，另一个线程在此间隙成功更新了缓存和DB，后来第一个线程再更新DB，
		这时，缓存中存的是第二个线程的数据，DB中存的是第一个线程的数据。
/********************************************/
缓存与DB数据同步更新方案：
	1.先删除缓存后更新DB：由于这个操作分为两个步骤，所以在多线程环境下第一个线程删掉了缓存还未来得及更细DB时如果其他线程来访问，
		由于缓存被删除，其他线程只能直接访问DB然后将数据重新存入缓存，但此时从DB获取的数据是旧的数据，所以导致缓存中被重新放入了旧数据，
		缓存被放入旧数据后读操作会被拦截在缓存层，即使DB更新成功了。后续读操作也会直接从缓存读而到不了DB，导致长时间读旧，即缓存脏读。
		同时如果更新DB的过程中有大量读取操作直接到达DB极易造成数据库压力过大，进而延长DB的读和更新操作，造成缓存击穿。
	2.先更新DB后删除缓存：这种操作会会造成概率性的短时间的缓存脏读，但是在DB更新过程中不会引起大量DB访问，即使第二部删除了缓存，有少量直接DB读，
		但是也是可以容忍的，不会造成缓存击穿问题，并且数据是正确的。
/********************************************/
先操作缓存还是先操作数据库：
	1.应该先操作数据库，因为如果先操作缓存，在读写并发的情境下可能导致读写数据不一致，比如第一个写线程让缓存失效了还没来得及更新DB，第二个读线程在此间隙来由于不能从
		缓存读取数据，所以直接从DB中读取旧数据，进而将旧数据存入缓存，此时第一个线程才来更新DB，此时缓存和DB中的数据就不一致了。
	2.但是也存在原子性被破坏的问题，即写线程先操作数据库成功了，但是缓存操作失败了，这时缓存和DB的数据就不一致了，后续读线程读到的都是旧的缓存值。
	3.原子性被破坏问题几率不大，所以还是要先操作DB后操作缓存。
/********************************************/
一、前言
	设计一个缓存系统，不得不要考虑的问题就是：缓存穿透、缓存击穿与失效时的雪崩效应。
/********************************************/
一、缓存穿透
	缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，
	这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，
	这就是bug。
二、解决方案
	有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，
	一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。另外也有一个更为简单粗暴的方法（我们采用的就是这种），
	如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。
/********************************************/
一、缓存雪崩
	缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。
二、解决方案
	缓存失效时的雪崩效应对底层系统的冲击非常可怕。大多数系统设计者考虑用加锁或者队列的方式保证缓存的单线 程（进程）写，
	从而避免失效时大量的并发请求落到底层存储系统上。这里分享一个简单方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间
	基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。
/********************************************/
一、缓存击穿
	对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：
	缓存被“击穿”的问题，这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。缓存在某个时间点过期的时候，恰好在这个时间点
	对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。
二、解决方案
	我们的目标是：尽量少的线程构建缓存(甚至是一个) + 数据一致性 + 较少的潜在危险，下面会介绍四种方法来解决这个问题：
	1.使用互斥锁，即
/********************************************/
Java值类型
	1.java对8个原始类型，在实现层面上是用的值类型，即赋值操作是通过直接拷贝变量值来实现的。
	2.在语义层面上是没有的，也就是说从开发人员的角度来说，在代码中对原始类型变量的各种赋值操作，最终表现的结果和引用类型是一致的。
/********************************************/
Gradle本地化配置，建立好Gradle项目后往往需要修改一些参数才能正常运行
1.配置Gradle程序包地址：项目根目/gradle/wrapper/gradle-wrapper.properties，编辑distributionUrl=file:///D:ProgramFilse/gradle-4.9/gradle-4.9-bin.zip
2.配置远程仓库（阿里云）地址：项目根目录/gradle.build，编辑repositories下的maven仓库镜像路径：
	maven {url 'http://maven.aliyun.com/nexus/content/groups/public/'}
	maven {url 'http://maven.aliyun.com/nexus/content/repositories/jcenter'}
/********************************************/
CAS无锁机制如何保证非阻塞和原子性
在JDK1.5之前。Java主要靠synchronized这个关键字保证同步，已解决多线程下的线程不安全问题，但是这会导致锁的发生，会引发一些个性能问题。
锁主要存在一下问题
（1）在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，引起性能问题。
（2）一个线程持有锁会导致其它所有需要此锁的线程挂起。
（3）如果一个优先级高的线程等待一个优先级低的线程释放锁会导致优先级倒置，引起性能风险。
Volatile是一个不错的选择，但是前面我们已经说了，volatile不能保证原子性，因此同步还是需要用到锁。也许大家已经听说过，
锁分两种，一个叫悲观锁，一种称之为乐观锁。Synchronized就是悲观锁的一种，也称之为独占锁，加了synchronized关键字的代码
基本上就只能以单线程的形式去执行了，它会导致其他需要该资源的线程【挂起】，直到前面的线程执行完毕释放所资源。而另外一种乐
观锁是一种更高效的机制，它的原理就是每次不加锁去执行某项操作，如果发生冲突则失败并重试，直到成功为止，其实本质上不算锁，
所以很多地方也称之为自旋。使用这种机制编写的算法也叫非阻塞算法，标准定义为一个线程的失败或者挂起不影响其他线程的失败或者挂起的算法。
现代CPU提供了特殊的指令来自动更新共享数据，而且能检测到其他数据的干扰，因此可以通过compareAndSet来替代锁定。
尽管CAS机制使得我们可以不依赖同步，不影响和挂起其他线程实现原子性操作，能大大提升运行时的性能，但是会导致一个ABA的问题。
如线程一和线程二都取出了主存中的数据为A，这时候线程二将数据修改为B，然后又修改为A，此时线程一再次去进行compareAndSet的时候仍然能够匹配成功，
而实际的数据已经发生了变更了，只不过发生了2次变化将对应的值修改为原始的数据了，并不代表实际数据没有发生变化。
这时候前面提到的原子操作AtomicStampedReference/AtomicMarkableReference就很有用了。这允许一对变化的元素进行原子操作。

缺点及解决方案：
	1.CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做
		从Java1.5开始JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，
		并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值
	2.循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，
		pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，
		在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），
		从而提高CPU的执行效率。
	3.只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，
		循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，
		合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。
/********************************************/
CopyOnWrite机制
	1.CopyOnWrite机制利用【读写分离】的思路解决并发读写共享对象的问题。
	2.原理就是在执行写操作时将原对象拷贝一份副本，写过程在副本上执行，其他的读操作在原对象上执行，写操作完成后将元对象的指针指向修改后的副本。
	3.其中读操作是未加锁的，写操作的方法是互斥的。
	4.由于读的是原对象，写的是副本，所以并发读写时读的内容并不是最新的，不能保证强一致性，只能保证最终一致性。
	5.这种机制适合读多写少且只需要满足最终一致性的并发场景。
/********************************************/
JavaLambda表达式与内部类反编译测试，测试代码功能如下: new Thread(() -> System.out.printlin("55555")).start();
	1.反编译结果显示，只有显式的匿名内部类，编译后才会生成额外的带$.class文件。
	2.不论是明式Lambda还是匿名式Lambda表达式，反编译后都不会生成额外的带$.class文件。
/********************************************/
JavaNIO之ZeroCopy工作机制及演进，以服务器读取本地磁盘文件，经过逻辑处理再通过Socket发送到网络为例：
1.传统IO方式，当程序向系统发起读取磁盘文件调用后数据流向是这样的，DMA将数据从磁盘读取到内核空间，CPU将内核空间的数据读取到用户空间，
	经过处理后，CPU将数据从用户空间写到内核空间，CPU再将数据写入到网卡，整个过程经历了四次上下文的切换，四次数据的Copy，其中二次都用到了CPU。
	有内核空间作为缓冲区在小数据量的读写中是可以提高效率的，数据量大了，这一块反而成为了性能瓶颈。
2.改进版的实现是，DMA将数据从磁盘读取到内核空间的ReadBuffer，然后CPU在内核中将数据从ReadBuffer写入到内核空间的SocketBuffer，
	DMA再将数据从SocketBuffer写入网卡。整个过程发生了二次上下文切换和三次Copy，但只有一次用到了CPU。
3.更进版实现是，在上一版的基础上，将内核空间中省略掉SocketBuffer，用一个描述符来在内存中标记以替代其作用，这样又省去了一次Copy，
	至此CPU参与的Copy就为0了，上下文切换二次。
/********************************************/
1.可见性问题的根本原因是不同线程在操作共享数据时，每个线程各自拥有独立于主存的共享数据拷贝，
	线程内部的数据操作不能及时的与主存数据同步造成的。
2.同步锁synchronized(data)关键字可以解决可见性问题，但是会引起效率问题。
3.volatile关键字可以保证共享数据在不同线程间是可见的，由于它禁止了jvm的重排序优化，也会导致一些性能上的损耗，但是其性能要高过同步锁。
4.相较于synchronized的适用场景是不同的：
	volatile不能保证读写共享数据的多个线程的互斥性。
	不能保证对数据操作的原子性。
/********************************************/
Atomic
1.i++操作在底层分为三个步骤才能完成。
2.并发包中的原子类型通过volatile关键子保证可见性，依托硬件用CAS保证原子性。
3.比阻塞式更高效，因为在一个线程更新失败后并不会阻塞，避免了线程切换等开销，而是继续持有cpu资源进行尝试，直到满足条件而更新成功。
/********************************************/
分布式锁
1.目的：在分布式场景中，要保证某个方法调用、资源占用的互斥性，就需要用到分布式锁。
2.实现方式：基于数据库实现、基于缓存实现、基于Zookeeper实现分布式锁
3.需满足要求：
	可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。
	这把锁要是一把可重入锁（避免死锁）
	这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条）
	有高可用的获取锁和释放锁功能
	获取锁和释放锁的性能要好
4.基于数据库的实现：
	1.在数据库中建立一张分布式锁表，每一行记录代表一个方法或者资源正在被占用，将方法名字段设置为唯一索引。
	2.调用方法前先将被调用方法名作为唯一索引插入到锁表，由于方法名被设置了唯一索引，如果已经存在则会插入失败，表示正在被调用还未释放，方法就不能顺利被调用。
	3.如果插入成功，则表示未被占用，可以顺利调用。
	4.调用完成后，在finally语句块中删除锁记录，达到释放锁的目的。
	
	1、这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。
	2、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。
	3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。
	4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。
	
	当然，我们也可以有其他方式解决上面的问题。
	数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。
	没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。
	非阻塞的？搞一个while循环，直到insert成功再返回成功。
	非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，
	如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。
我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。 基于MySql的InnoDB引擎，可以使用以下方法来实现加锁操作：
public boolean lock() {
	connection.setAutoCommit(false);
	while(true) {
		try{
			result = select t.id from t_method_lock t where t.method_name = "target_name";
			if(result == null) {
				return true;
			}else{
				TimeUnit.sleep(3).SECONDS;
			}
		}catch(e){
			TimeUnit.sleep(3).SECONDS;
		}
	}
	return false;
}
在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁（这里再多提一句，InnoDB引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，
否则会使用表级锁。这里我们希望使用行级锁，就要给method_name添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。
重载方法的话建议把参数类型也加上。）。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。
我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁：
public void unlock() {
	connection.commit();
}
通过connection.commit()操作来释放锁。
这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。
阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。
锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。
但是还是无法直接解决数据库单点和可重入问题。
这里还可能存在另外一个问题，虽然我们对method_name 使用了唯一索引，并且显示使用for update来使用行级锁。但是，MySql会对查询进行优化，即便在条件中使用了索引字段，
但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，
这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。
还有一个问题，就是我们要使用排他锁来进行分布式锁的lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆

总结
总结一下使用数据库来实现分布式锁的方式，这两种方式都是依赖数据库的一张表，一种是通过表中的记录的存在情况确定当前是否有锁存在，另外一种是通过数据库的排他锁来实现分布式锁。
数据库实现分布式锁的优点
直接借助数据库，容易理解。
数据库实现分布式锁的缺点
会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。
操作数据库需要一定的开销，性能问题需要考虑。
使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。
/********************************************/
1.在security-parent的pom中引入spring-io-platform是为了管理整个项目中跟spring相关依赖的版本，能自动保证依赖间的兼容性。
2.如果spring-boot应用中引入了Spring-security，默认会打开http-basic认证方式，对应用进行基本的保护，但不能满足多变的实际环境。
/********************************************/
自定义认证业务逻辑：
3.实现UserDetailsService类在loadUserByUsername方法中实现自定义校验逻辑，
	可以在这里实现从数据库获取用户信息，然后包装为UserDetails的实现类对象，也可以用内置的实现类User，
	它拥有两个重载的构造方法，除了可以封装用户名、密码外还提供了账号是否过期、用户密码是否过期、用户账号是否锁定、用户是否被禁用等附加属性。
4.新版本的Spring-security强制要求配置密码加密器，框架里PasswordEncoder接口定义了两个方法，
	encode()供用户调用，需要在密码入库前调用。
	matches()供Spring-security框架调用，主要发生在认证过程。
/********************************************/
自定义认证流程：
1.spring-boot项目中的根路径页面需要放在resources/static目录下才能访问到。
2.根据客户端类型，如果页面性请求就返回登录页面，如果是数据性就返回登录提示的json串。
	解决思路：首先当客户端请求的地址需要认证时不重定向到页面，而是转发到一个专门的controller，在这个Cotroller方法中判断请求的性质，
	如果是页面性请求就返回登录页面，如果是数据性请求就返回401状态码及提示信息。
/********************************************/
spring-security认证流程源码：
一、到达认证过滤器UsernamePasswordAuthenticationFilter，它负责处理最传统的用户名+密码类型的认证过程。
	1.调用attemptAuthentication()方法拿到请求中的用户名和密码构造了一个UsernamePasswordAuthenticationToken对象，
		这个类是Authentication接口的一个实现，但这个对象的被暂时设为未认证。
	2.将构造的这个token对象传递给AuthenticationManager接口的实现ProviderManager的authenticate()方法。这个方法会获取token的类型，
		然后便利spring-security所支持的认证类型处理器，用匹配上的认证器对传进来的token进行认证。调用的是AbstractUserDetailsAuthenticationProvider
		这个抽象类实现类的authenticate()方法。
	5.在这个方法中会引入自定义实现的UserDetailsService并调用loadUserByUsername()方法从数据库获取用户数据，
		然后对从DB中获取的用户数据进行各个属性的匹配，这其中包括凭证加密匹配和四个布尔值的预检查和后检查，遇到不匹配的项就抛出相应的异常。
	6.如果前面的各项检查都通过了，就重新构造一个成功认证的认证信息。
	7.至于登录成功与否都会调用父类AbstractAuthenticationProcessingFilter中的成功和失败相应的处理方法。
二、认证结果如何在多个请求间共享
	1.AbstractAuthenticationProcessingFilter抽象类中successfulAuthentication()方法中会访问SecurityContextHolder.getContext()对认证信息进行存储。
		这个SecurityContextHolder相当于一个ThreadLocal的变量，在本次请求的的各处都可以访问。
三、如何获取用户的认证信息
	
/********************************************/
/********************************************/
/********************************************/
/********************************************/
/********************************************/
/********************************************/
/********************************************/




















